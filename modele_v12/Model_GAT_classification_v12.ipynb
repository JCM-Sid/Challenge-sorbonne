{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3523,
     "status": "ok",
     "timestamp": 1743022078234,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "59fbdd14-a668-45e3-a568-95abf69ed525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    }
   ],
   "source": [
    "# Challenge Sorbonne - DST\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3356,
     "status": "ok",
     "timestamp": 1743022081591,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "21408444-d6e8-4d4b-9845-5f96c313f686"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init Inputs files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1743022082826,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "129bed9b-247d-42e9-f5ed-786a494b04c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des donnÃ©es\n",
    "if current_directory == '/content': # Google Colab\n",
    "    training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "\n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/split_training-colab-v12.csv'\n",
    "    filename_trained =      '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/list_hash_trained.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    #test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_set_to_predict.csv'\n",
    "    #filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"..\\Pred_test_v12_500_2803\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v12_500_2803\\split_training-pc-v12.csv'\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v12_500_2803\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "    test_path_dir =   r\"D:\\ChallengeDST\\folder_test_set\\folder_test_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\"\n",
    "    test_init_file = r\"..\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\"..\\Pred_test_v12_500_2803\\test_prediction.csv\"\n",
    "    filename_test_res_partiels = r'..\\Pred_test_v12_500_2803\\test_prediction_part.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v12\": #PC2\n",
    "    split_char = '\\\\'\n",
    "    training_path_dir =   r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\\folder_training_set\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"..\\Pred_test_v12_500_2803\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v12_500_2803\\split_training-pc-v12.csv'\n",
    "    model_save_file =  r'..\\Pred_test_v12_500_2803\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "    test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\\folder_test_set\"\n",
    "    test_init_file = r\"..\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\"..\\Pred_test_v12_500_2803\\test_prediction.csv\"\n",
    "    filename_test_res_partiels = r'..\\Pred_test_v12_500_2803\\test_prediction_part.csv'\n",
    "    #filename_test_res_partiels = r'C:\\Users\\jch_m\\OneDrive\\test_prediction_part.csv'\n",
    "\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "# read meta data CSV input and save in df\n",
    "random_seed = 42\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "df_meta_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1743022082873,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "6a0d2500-7f21-447d-ba57-3dc495d196bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Pred_test_v12_500_2803\\list_hash_trained.csv\n",
      "taille : 560\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f993af4662324130ca817cf3c7b04d5701a85422cb773e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "1  bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...\n",
       "2  bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...\n",
       "3  f993af4662324130ca817cf3c7b04d5701a85422cb773e...\n",
       "4  bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1743022082888,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "ec6a1c40-769e-461e-8442-c9e8cba7be7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0' 'rep_500' 'rep_1000']\n",
      "['G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\61c0810a23580cf492a6ba4f7654566108331e7a4134c968c2d6a05261b2d8a1.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\de76670dc04153703f7d346fdcd0b13af2dd1a7eaae3e494f7b0777cdf97533d.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\44edcf113309783b1af298ce908c7546c0e48b7699a23ef5e9fd6228fc973b59.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\652b78411eeb580f14d4f6ce5c22b5c931b18464ed3f8d5fdd03112ae2c04d59.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\0303959c78d21b55fc3421ef30a80da761a5a5e2a6f8f844fdadfbd41e7f1c50.json']\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_500']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique()[:3])\n",
    "print(full_file_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1743022082938,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0', max_file=0):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    count_files = 0\n",
    "\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "            #print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 100_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743022082946,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "84w6HNzrOKjz",
    "outputId": "618e03b9-f421-4306-eb80-ebecfa7c12e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Number of files with error: 1\n",
      "2 9 1\n"
     ]
    }
   ],
   "source": [
    "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch='rep_500',max_file=10)\n",
    "print(len(list_val), len(list_train), len(list_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init modele et fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1743022083004,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "class HierarchicalAssemblyTokenizer:\n",
    "    def __init__(self):\n",
    "        # Tokenizer implementation from previous code\n",
    "        # Abbreviated for brevity but would be the full implementation\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "    def fit(self, graph_l, min_freq=1):\n",
    "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        self.unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        # Extract features from node labels\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                # Extract node type (JCC, INST)\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                # Extract operation (xor, push, mov)\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                # Extract registers\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                # Extract memory reference patterns\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                # Extract immediate values\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            self.unique_hashes.append(item['name'])\n",
    "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "            for i, hash_val in enumerate(self.unique_hashes):\n",
    "                self.hash_to_id[hash_val] = i\n",
    "                self.id_to_hash[i] = hash_val\n",
    "\n",
    "        # Build vocabularies with frequency filtering\n",
    "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
    "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
    "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
    "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_from_counts(self, all_counts, min_freq=3):\n",
    "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
    "        self.node_type_vocab.clear()\n",
    "        self.operation_vocab.clear()\n",
    "        self.register_vocab.clear()\n",
    "        self.memory_pattern_vocab.clear()\n",
    "        self.immediate_vocab.clear()\n",
    "\n",
    "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
    "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
    "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
    "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
    "\n",
    "        self.unique_hashes = all_counts['unique_hashes']\n",
    "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "        for i, hash_val in enumerate(self.unique_hashes):\n",
    "            self.hash_to_id[hash_val] = i\n",
    "            self.id_to_hash[i] = hash_val\n",
    "\n",
    "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
    "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
    "        counter = Counter(tokens)\n",
    "        # Add special UNK token\n",
    "        vocab_dict[self.UNK_TOKEN] = 0\n",
    "\n",
    "        # Add tokens that meet minimum frequency\n",
    "        idx = 1\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                vocab_dict[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def _memory_to_pattern(self, mem_ref):\n",
    "        \"\"\"Convert memory reference to pattern\"\"\"\n",
    "        # Replace registers with REG placeholder\n",
    "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
    "        # Replace immediate values with IMM placeholder\n",
    "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
    "        return pattern.strip()\n",
    "\n",
    "    def tokenize(self, node_label):\n",
    "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
    "        features = {\n",
    "            'node_type': self.UNK_TOKEN,\n",
    "            'operation': self.UNK_TOKEN,\n",
    "            'registers': [],\n",
    "            'memory_pattern': self.UNK_TOKEN,\n",
    "            'immediate': self.UNK_TOKEN\n",
    "        }\n",
    "\n",
    "        # Extract node type\n",
    "        node_type_match = self.patterns['node_type'].search(node_label)\n",
    "        if node_type_match:\n",
    "            node_type = node_type_match.group(1)\n",
    "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract operation\n",
    "        op_match = self.patterns['operation'].search(node_label)\n",
    "        if op_match:\n",
    "            operation = op_match.group(1)\n",
    "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract registers\n",
    "        reg_matches = self.patterns['registers'].findall(node_label)\n",
    "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
    "\n",
    "        # Extract memory reference\n",
    "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
    "        if mem_matches:\n",
    "            pattern = self._memory_to_pattern(mem_matches[0])\n",
    "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract immediate values\n",
    "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
    "        if imm_matches:\n",
    "            imm = imm_matches[0]\n",
    "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
    "\n",
    "        return features\n",
    "\n",
    "    def encode_nodelabel(self, node_label):\n",
    "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
    "        features = self.tokenize(node_label)\n",
    "\n",
    "        # Encode each feature\n",
    "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
    "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
    "\n",
    "        # Encode registers (take up to 3, pad if fewer)\n",
    "        register_indices = []\n",
    "        for i in range(min(3, len(features['registers']))):\n",
    "            reg = features['registers'][i]\n",
    "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
    "            register_indices.append(reg_idx)\n",
    "\n",
    "        # Pad register indices if needed\n",
    "        while len(register_indices) < 3:\n",
    "            register_indices.append(0)  # 0 for padding\n",
    "\n",
    "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
    "            features['memory_pattern'],\n",
    "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        immediate_idx = self.immediate_vocab.get(\n",
    "            features['immediate'],\n",
    "            self.immediate_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Combine all indices into a feature vector\n",
    "        encoded = np.array([\n",
    "            node_type_idx,\n",
    "            operation_idx,\n",
    "            register_indices[0],\n",
    "            register_indices[1],\n",
    "            register_indices[2],\n",
    "            memory_pattern_idx,\n",
    "            immediate_idx\n",
    "        ], dtype=np.int64)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_graph(self, digraph):\n",
    "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
    "        node_features = {}\n",
    "\n",
    "        for node_id in digraph.nodes():\n",
    "            label = digraph.nodes[node_id].get('label', '')\n",
    "            node_features[node_id] = self.encode_nodelabel(label)\n",
    "\n",
    "        return node_features\n",
    "\n",
    "    def encode_hash(self, hash_file):\n",
    "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return the size of each vocabulary\"\"\"\n",
    "        return {\n",
    "            'node_type': len(self.node_type_vocab),\n",
    "            'operation': len(self.operation_vocab),\n",
    "            'register': len(self.register_vocab),\n",
    "            'memory_pattern': len(self.memory_pattern_vocab),\n",
    "            'immediate': len(self.immediate_vocab)\n",
    "        }\n",
    "\n",
    "    def get_tokens_and_counts(self, graph_l):\n",
    "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            unique_hashes.append(item['name'])\n",
    "\n",
    "        return {\n",
    "            'node_types': node_types,\n",
    "            'operations': operations,\n",
    "            'registers': registers,\n",
    "            'memory_patterns': memory_patterns,\n",
    "            'immediates': immediates,\n",
    "            'unique_hashes': unique_hashes,\n",
    "        }\n",
    "\n",
    "class AssemblyGraphDataset:\n",
    "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
    "        \"\"\"\n",
    "        Prepares a dataset of assembly graphs for GAT model training\n",
    "        Args:\n",
    "            graph_list: List of NetworkX digraphs representing assembly code\n",
    "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
    "            node_feature_dim: Dimension of node embeddings\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        # Initialize embedding layers for each feature type\n",
    "        num_feature_type = 5\n",
    "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
    "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
    "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
    "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
    "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
    "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
    "\n",
    "        # Process graphs into PyTorch Geometric Data objects\n",
    "        self.data_list = []\n",
    "        for graph in self.graph_list:\n",
    "            # process graph info\n",
    "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
    "\n",
    "    def _process_graph(self, graph, hash_name):\n",
    "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
    "        # Create a node ID mapping for consecutive IDs\n",
    "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "        # Get node features\n",
    "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
    "\n",
    "        # Convert to tensor-friendly format\n",
    "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
    "        for node_id, features in node_features_dict.items():\n",
    "            x[node_mapping[node_id]] = torch.tensor(features)\n",
    "\n",
    "        # Create edge index\n",
    "        edge_list = list(graph.edges())\n",
    "        if not edge_list:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
    "\n",
    "        # Endcode Hash name\n",
    "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
    "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
    "\n",
    "        # Get embeddings before creating Data object\n",
    "        x = self.get_embeddings(x)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
    "        return data\n",
    "\n",
    "    def optimize_edge_index(self, edge_list, node_mapping):\n",
    "        \"\"\"\n",
    "        Optimizes the creation of edge_index tensor for graph representation.\n",
    "        Args:\n",
    "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
    "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Optimized edge_index tensor.\n",
    "        \"\"\"\n",
    "        if not edge_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "        num_edges = len(edge_list)\n",
    "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "\n",
    "        for i, (src, tgt) in enumerate(edge_list):\n",
    "            src_indices[i] = node_mapping[src]\n",
    "            tgt_indices[i] = node_mapping[tgt]\n",
    "\n",
    "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
    "        # Split features into their components\n",
    "        node_type_idx = x[:, 0]\n",
    "        operation_idx = x[:, 1]\n",
    "        register_idx1 = x[:, 2]\n",
    "        register_idx2 = x[:, 3]\n",
    "        register_idx3 = x[:, 4]\n",
    "        memory_pattern_idx = x[:, 5]\n",
    "        immediate_idx = x[:, 6]\n",
    "\n",
    "        # Get embeddings for each component\n",
    "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
    "        operation_emb = self.operation_embedding(operation_idx)\n",
    "\n",
    "        # Combine register embeddings (average them)\n",
    "        register_emb = (self.register_embedding(register_idx1) +\n",
    "                        self.register_embedding(register_idx2) +\n",
    "                        self.register_embedding(register_idx3)) / 3\n",
    "\n",
    "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
    "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        return torch.cat([\n",
    "            node_type_emb,\n",
    "            operation_emb,\n",
    "            register_emb,\n",
    "            memory_pattern_emb,\n",
    "            immediate_emb\n",
    "        ], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Graph Attention Network for assembly code analysis\n",
    "Args:\n",
    "    dataset: AssemblyGraphDataset instance\n",
    "    hidden_dim: Hidden dimension of GAT layers\n",
    "    output_dim: Output dimension of node embeddings\n",
    "    heads: Number of attention heads\n",
    "    dropout: Dropout rate\n",
    "\"\"\"\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, x, edge_index, hash_encoded, batch):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G\n",
    "\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                #f = open(full_path_file, 'r')\n",
    "                digraph_str = f.read()\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append({\n",
    "                    'name': hash_file,\n",
    "                    'graph_input' : G\n",
    "                })\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
    "    if os.path.exists(model_save_file):\n",
    "      if torch.cuda.is_available():\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "      else:\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        count_batch = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # get the different components of the dataset\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
    "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # save model trained\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
    "    if os.path.exists(model_save_file):\n",
    "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "\n",
    "    model.eval() #set the model to evaluation mode.\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "    #hash_v = []\n",
    "\n",
    "    count_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_label.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #hash_v.append(list_hash_batch)\n",
    "\n",
    "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "\n",
    "    return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "    \"\"\"\n",
    "    Processes files in batches for tokenization and model training/evaluation in a single loop.\n",
    "    Args:\n",
    "        full_file_paths (list): List of full file paths to process.\n",
    "        batch_files_size (int): Number of files to process in each batch for tokenization.\n",
    "        batch_data_size (int): Batch size for the DataLoader during training/evaluation.\n",
    "        num_feature_dim (int, optional): Dimension of node features. Defaults to None.\n",
    "        mode (str): 'train' or other mode (e.g., 'eval'). Defaults to 'train'.\n",
    "        epochs (int): Number of training epochs per batch (if mode is 'train'). Defaults to 3.\n",
    "        tokenizer: Tokenizer object with methods like get_tokens_and_counts and fit_from_counts.\n",
    "        process_batch (callable): Function to process a batch of file paths and return a list of graph-like objects.\n",
    "        train_model (callable): Function to train the model with a DataLoader and other info.\n",
    "    \"\"\"\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) + 1\n",
    "\n",
    "    hash_v = []\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    all_counts = {\n",
    "        'node_types': [],\n",
    "        'operations': [],\n",
    "        'registers': [],\n",
    "        'memory_patterns': [],\n",
    "        'immediates': [],\n",
    "        'unique_hashes': [],\n",
    "    }\n",
    "\n",
    "    print(\"**** Tokenization and Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        batch_num = i // batch_files_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if not batch_graph_list:\n",
    "            print(\"graph list is empty for this batch\")\n",
    "            continue\n",
    "\n",
    "        # Tokenization for the current batch\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list)\n",
    "        for key in all_counts:\n",
    "            if key in tokens:\n",
    "                all_counts[key].extend(tokens[key])\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "        # Create Dataset and DataLoader for the current batch\n",
    "        if tokenizer and num_feature_dim is not None:\n",
    "            dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
    "            curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
    "            list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
    "\n",
    "            # Training or other processing for the current batch\n",
    "            if mode == 'train' and train_model:\n",
    "                #print(f\"**** Training on batch {batch_num}/{nb_batch}\")\n",
    "                train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=epochs)\n",
    "                hash_t.extend(list_of_hash)\n",
    "                \n",
    "                # Sauver le hash entrainÃ©\n",
    "                with open(filename_trained, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    for item in list_of_hash:\n",
    "                        writer.writerow([item])\n",
    "                    csvfile.close()\n",
    "            else:\n",
    "                pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
    "                all_true_label.append(true_labels.cpu())\n",
    "                all_pred_label.append(pred_labels.cpu())\n",
    "                hash_v.append(list_of_hash)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Final fitting of the tokenizer after processing all batches\n",
    "    if all_counts:\n",
    "        print(\"**** Finalizing Tokenizer Vocabulary\")\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "    #print(\"**** Processing Complete\")\n",
    "    # save all labels as tensor to return if mode= 'validation'\n",
    "    if mode == 'validation':\n",
    "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "\n",
    "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "    elif mode == 'train':\n",
    "        return hash_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Train for Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avU1v3vVtSu-",
    "outputId": "1c290e1b-31d3-4874-b0af-921df5e29187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch: rep_1000 ========== \n",
      "Number of files with error: 4\n",
      "There are 450 files for train\n",
      "**** Tokenization and Dataset Processing & Training\n",
      "Processing batch 1/451: 1 files\n",
      "Processing batch 2/451: 1 files\n",
      "Processing batch 3/451: 1 files\n",
      "Processing batch 4/451: 1 files\n",
      "Processing batch 5/451: 1 files\n",
      "Processing batch 6/451: 1 files\n",
      "Processing batch 7/451: 1 files\n",
      "Processing batch 8/451: 1 files\n",
      "Processing batch 9/451: 1 files\n",
      "Processing batch 10/451: 1 files\n",
      "Processing batch 11/451: 1 files\n",
      "Processing batch 12/451: 1 files\n",
      "Processing batch 13/451: 1 files\n",
      "Processing batch 14/451: 1 files\n",
      "Processing batch 15/451: 1 files\n",
      "Processing batch 16/451: 1 files\n",
      "Processing batch 17/451: 1 files\n",
      "Processing batch 18/451: 1 files\n",
      "Processing batch 19/451: 1 files\n",
      "Processing batch 20/451: 1 files\n",
      "Processing batch 21/451: 1 files\n",
      "Processing batch 22/451: 1 files\n",
      "Processing batch 23/451: 1 files\n",
      "Processing batch 24/451: 1 files\n",
      "Processing batch 25/451: 1 files\n",
      "Processing batch 26/451: 1 files\n",
      "Processing batch 27/451: 1 files\n",
      "Processing batch 28/451: 1 files\n",
      "Processing batch 29/451: 1 files\n",
      "Processing batch 30/451: 1 files\n",
      "Processing batch 31/451: 1 files\n",
      "Processing batch 32/451: 1 files\n",
      "Processing batch 33/451: 1 files\n",
      "Processing batch 34/451: 1 files\n",
      "Processing batch 35/451: 1 files\n",
      "Processing batch 36/451: 1 files\n",
      "Processing batch 37/451: 1 files\n",
      "Processing batch 38/451: 1 files\n",
      "Processing batch 39/451: 1 files\n",
      "Processing batch 40/451: 1 files\n",
      "Processing batch 41/451: 1 files\n",
      "Processing batch 42/451: 1 files\n",
      "Processing batch 43/451: 1 files\n",
      "Processing batch 44/451: 1 files\n",
      "Processing batch 45/451: 1 files\n",
      "Processing batch 46/451: 1 files\n",
      "Processing batch 47/451: 1 files\n",
      "Processing batch 48/451: 1 files\n",
      "Processing batch 49/451: 1 files\n",
      "Processing batch 50/451: 1 files\n",
      "Processing batch 51/451: 1 files\n",
      "Processing batch 52/451: 1 files\n",
      "Processing batch 53/451: 1 files\n",
      "Processing batch 54/451: 1 files\n",
      "Processing batch 55/451: 1 files\n",
      "Processing batch 56/451: 1 files\n",
      "Processing batch 57/451: 1 files\n",
      "Processing batch 58/451: 1 files\n",
      "Processing batch 59/451: 1 files\n",
      "Processing batch 60/451: 1 files\n",
      "Processing batch 61/451: 1 files\n",
      "Processing batch 62/451: 1 files\n",
      "Processing batch 63/451: 1 files\n",
      "Processing batch 64/451: 1 files\n",
      "Processing batch 65/451: 1 files\n",
      "Processing batch 66/451: 1 files\n",
      "Processing batch 67/451: 1 files\n",
      "Processing batch 68/451: 1 files\n",
      "Processing batch 69/451: 1 files\n",
      "Processing batch 70/451: 1 files\n",
      "Processing batch 71/451: 1 files\n",
      "Processing batch 72/451: 1 files\n",
      "Processing batch 73/451: 1 files\n",
      "Processing batch 74/451: 1 files\n",
      "Processing batch 75/451: 1 files\n",
      "Processing batch 76/451: 1 files\n",
      "Processing batch 77/451: 1 files\n",
      "Processing batch 78/451: 1 files\n",
      "Processing batch 79/451: 1 files\n",
      "Processing batch 80/451: 1 files\n",
      "Processing batch 81/451: 1 files\n",
      "Processing batch 82/451: 1 files\n",
      "Processing batch 83/451: 1 files\n",
      "Processing batch 84/451: 1 files\n",
      "Processing batch 85/451: 1 files\n",
      "Processing batch 86/451: 1 files\n",
      "Processing batch 87/451: 1 files\n",
      "Processing batch 88/451: 1 files\n",
      "Processing batch 89/451: 1 files\n",
      "Processing batch 90/451: 1 files\n",
      "Processing batch 91/451: 1 files\n",
      "Processing batch 92/451: 1 files\n",
      "Processing batch 93/451: 1 files\n",
      "Processing batch 94/451: 1 files\n",
      "Processing batch 95/451: 1 files\n",
      "Processing batch 96/451: 1 files\n",
      "Processing batch 97/451: 1 files\n",
      "Processing batch 98/451: 1 files\n",
      "Processing batch 99/451: 1 files\n",
      "Processing batch 100/451: 1 files\n",
      "Processing batch 101/451: 1 files\n",
      "Processing batch 102/451: 1 files\n",
      "Processing batch 103/451: 1 files\n",
      "Processing batch 104/451: 1 files\n",
      "Processing batch 105/451: 1 files\n",
      "Processing batch 106/451: 1 files\n",
      "Processing batch 107/451: 1 files\n",
      "Processing batch 108/451: 1 files\n",
      "Processing batch 109/451: 1 files\n",
      "Processing batch 110/451: 1 files\n",
      "Processing batch 111/451: 1 files\n",
      "Processing batch 112/451: 1 files\n",
      "Processing batch 113/451: 1 files\n",
      "Processing batch 114/451: 1 files\n",
      "Processing batch 115/451: 1 files\n",
      "Processing batch 116/451: 1 files\n",
      "Processing batch 117/451: 1 files\n",
      "Processing batch 118/451: 1 files\n",
      "Processing batch 119/451: 1 files\n",
      "Processing batch 120/451: 1 files\n",
      "Processing batch 121/451: 1 files\n",
      "Processing batch 122/451: 1 files\n",
      "Processing batch 123/451: 1 files\n",
      "Processing batch 124/451: 1 files\n",
      "Processing batch 125/451: 1 files\n",
      "Processing batch 126/451: 1 files\n",
      "Processing batch 127/451: 1 files\n",
      "Processing batch 128/451: 1 files\n",
      "Processing batch 129/451: 1 files\n",
      "Processing batch 130/451: 1 files\n",
      "Processing batch 131/451: 1 files\n",
      "Processing batch 132/451: 1 files\n",
      "Processing batch 133/451: 1 files\n",
      "Processing batch 134/451: 1 files\n",
      "Processing batch 135/451: 1 files\n",
      "Processing batch 136/451: 1 files\n",
      "Processing batch 137/451: 1 files\n",
      "Processing batch 138/451: 1 files\n",
      "Processing batch 139/451: 1 files\n",
      "Processing batch 140/451: 1 files\n",
      "Processing batch 141/451: 1 files\n",
      "Processing batch 142/451: 1 files\n",
      "Processing batch 143/451: 1 files\n",
      "Processing batch 144/451: 1 files\n",
      "Processing batch 145/451: 1 files\n",
      "Processing batch 146/451: 1 files\n",
      "Processing batch 147/451: 1 files\n",
      "Processing batch 148/451: 1 files\n",
      "Processing batch 149/451: 1 files\n",
      "Processing batch 150/451: 1 files\n",
      "Processing batch 151/451: 1 files\n",
      "Processing batch 152/451: 1 files\n",
      "Processing batch 153/451: 1 files\n",
      "Processing batch 154/451: 1 files\n",
      "Processing batch 155/451: 1 files\n",
      "Processing batch 156/451: 1 files\n",
      "Processing batch 157/451: 1 files\n",
      "Processing batch 158/451: 1 files\n",
      "Processing batch 159/451: 1 files\n",
      "Processing batch 160/451: 1 files\n",
      "Processing batch 161/451: 1 files\n",
      "Processing batch 162/451: 1 files\n",
      "Processing batch 163/451: 1 files\n",
      "Processing batch 164/451: 1 files\n",
      "Processing batch 165/451: 1 files\n",
      "Processing batch 166/451: 1 files\n",
      "Processing batch 167/451: 1 files\n",
      "Processing batch 168/451: 1 files\n",
      "Processing batch 169/451: 1 files\n",
      "Processing batch 170/451: 1 files\n",
      "Processing batch 171/451: 1 files\n",
      "Processing batch 172/451: 1 files\n",
      "Processing batch 173/451: 1 files\n",
      "Processing batch 174/451: 1 files\n",
      "Processing batch 175/451: 1 files\n",
      "Processing batch 176/451: 1 files\n",
      "Processing batch 177/451: 1 files\n",
      "Processing batch 178/451: 1 files\n",
      "Processing batch 179/451: 1 files\n",
      "Processing batch 180/451: 1 files\n",
      "Processing batch 181/451: 1 files\n",
      "Processing batch 182/451: 1 files\n",
      "Processing batch 183/451: 1 files\n",
      "Processing batch 184/451: 1 files\n",
      "Processing batch 185/451: 1 files\n",
      "Processing batch 186/451: 1 files\n",
      "Processing batch 187/451: 1 files\n",
      "Processing batch 188/451: 1 files\n",
      "Processing batch 189/451: 1 files\n",
      "Processing batch 190/451: 1 files\n",
      "Processing batch 191/451: 1 files\n",
      "Processing batch 192/451: 1 files\n",
      "Processing batch 193/451: 1 files\n",
      "Processing batch 194/451: 1 files\n",
      "Processing batch 195/451: 1 files\n",
      "Processing batch 196/451: 1 files\n",
      "Processing batch 197/451: 1 files\n",
      "Processing batch 198/451: 1 files\n",
      "Processing batch 199/451: 1 files\n",
      "Processing batch 200/451: 1 files\n",
      "Processing batch 201/451: 1 files\n",
      "Processing batch 202/451: 1 files\n",
      "Processing batch 203/451: 1 files\n",
      "Processing batch 204/451: 1 files\n",
      "Processing batch 205/451: 1 files\n",
      "Processing batch 206/451: 1 files\n",
      "Processing batch 207/451: 1 files\n",
      "Processing batch 208/451: 1 files\n",
      "Processing batch 209/451: 1 files\n",
      "Processing batch 210/451: 1 files\n",
      "Processing batch 211/451: 1 files\n",
      "Processing batch 212/451: 1 files\n",
      "Processing batch 213/451: 1 files\n",
      "Processing batch 214/451: 1 files\n",
      "Processing batch 215/451: 1 files\n",
      "Processing batch 216/451: 1 files\n",
      "Processing batch 217/451: 1 files\n",
      "Processing batch 218/451: 1 files\n",
      "Processing batch 219/451: 1 files\n",
      "Processing batch 220/451: 1 files\n",
      "Processing batch 221/451: 1 files\n",
      "Processing batch 222/451: 1 files\n",
      "Processing batch 223/451: 1 files\n",
      "Processing batch 224/451: 1 files\n",
      "Processing batch 225/451: 1 files\n",
      "Processing batch 226/451: 1 files\n",
      "Processing batch 227/451: 1 files\n",
      "Processing batch 228/451: 1 files\n",
      "Processing batch 229/451: 1 files\n",
      "Processing batch 230/451: 1 files\n",
      "Processing batch 231/451: 1 files\n",
      "Processing batch 232/451: 1 files\n",
      "Processing batch 233/451: 1 files\n",
      "Processing batch 234/451: 1 files\n",
      "Processing batch 235/451: 1 files\n",
      "Processing batch 236/451: 1 files\n",
      "Processing batch 237/451: 1 files\n",
      "Processing batch 238/451: 1 files\n",
      "Processing batch 239/451: 1 files\n",
      "Processing batch 240/451: 1 files\n",
      "Processing batch 241/451: 1 files\n",
      "Processing batch 242/451: 1 files\n",
      "Processing batch 243/451: 1 files\n",
      "Processing batch 244/451: 1 files\n",
      "Processing batch 245/451: 1 files\n",
      "Processing batch 246/451: 1 files\n",
      "Processing batch 247/451: 1 files\n",
      "Processing batch 248/451: 1 files\n",
      "Processing batch 249/451: 1 files\n",
      "Processing batch 250/451: 1 files\n",
      "Processing batch 251/451: 1 files\n",
      "Processing batch 252/451: 1 files\n",
      "Processing batch 253/451: 1 files\n",
      "Processing batch 254/451: 1 files\n",
      "Processing batch 255/451: 1 files\n",
      "Processing batch 256/451: 1 files\n",
      "Processing batch 257/451: 1 files\n",
      "Processing batch 258/451: 1 files\n",
      "Processing batch 259/451: 1 files\n",
      "Processing batch 260/451: 1 files\n",
      "Processing batch 261/451: 1 files\n",
      "Processing batch 262/451: 1 files\n",
      "Processing batch 263/451: 1 files\n",
      "Processing batch 264/451: 1 files\n",
      "Processing batch 265/451: 1 files\n",
      "Processing batch 266/451: 1 files\n",
      "Processing batch 267/451: 1 files\n",
      "Processing batch 268/451: 1 files\n",
      "Processing batch 269/451: 1 files\n",
      "Processing batch 270/451: 1 files\n",
      "Processing batch 271/451: 1 files\n",
      "Processing batch 272/451: 1 files\n",
      "Processing batch 273/451: 1 files\n",
      "Processing batch 274/451: 1 files\n",
      "Processing batch 275/451: 1 files\n",
      "Processing batch 276/451: 1 files\n",
      "Processing batch 277/451: 1 files\n",
      "Processing batch 278/451: 1 files\n",
      "Processing batch 279/451: 1 files\n",
      "Processing batch 280/451: 1 files\n",
      "Processing batch 281/451: 1 files\n",
      "Processing batch 282/451: 1 files\n",
      "Processing batch 283/451: 1 files\n",
      "Processing batch 284/451: 1 files\n",
      "Processing batch 285/451: 1 files\n",
      "Processing batch 286/451: 1 files\n",
      "Processing batch 287/451: 1 files\n",
      "Processing batch 288/451: 1 files\n",
      "Processing batch 289/451: 1 files\n",
      "Processing batch 290/451: 1 files\n",
      "Processing batch 291/451: 1 files\n",
      "Processing batch 292/451: 1 files\n",
      "Processing batch 293/451: 1 files\n",
      "Processing batch 294/451: 1 files\n",
      "Processing batch 295/451: 1 files\n",
      "Processing batch 296/451: 1 files\n",
      "Processing batch 297/451: 1 files\n",
      "Processing batch 298/451: 1 files\n",
      "Processing batch 299/451: 1 files\n",
      "Processing batch 300/451: 1 files\n",
      "Processing batch 301/451: 1 files\n",
      "Processing batch 302/451: 1 files\n",
      "Processing batch 303/451: 1 files\n",
      "Processing batch 304/451: 1 files\n",
      "Processing batch 305/451: 1 files\n",
      "Processing batch 306/451: 1 files\n",
      "Processing batch 307/451: 1 files\n",
      "Processing batch 308/451: 1 files\n",
      "Processing batch 309/451: 1 files\n",
      "Processing batch 310/451: 1 files\n",
      "Processing batch 311/451: 1 files\n",
      "Processing batch 312/451: 1 files\n",
      "Processing batch 313/451: 1 files\n",
      "Processing batch 314/451: 1 files\n",
      "Processing batch 315/451: 1 files\n",
      "Processing batch 316/451: 1 files\n",
      "Processing batch 317/451: 1 files\n",
      "Processing batch 318/451: 1 files\n",
      "Processing batch 319/451: 1 files\n",
      "Processing batch 320/451: 1 files\n",
      "Processing batch 321/451: 1 files\n",
      "Processing batch 322/451: 1 files\n",
      "Processing batch 323/451: 1 files\n",
      "Processing batch 324/451: 1 files\n",
      "Processing batch 325/451: 1 files\n",
      "Processing batch 326/451: 1 files\n",
      "Processing batch 327/451: 1 files\n",
      "Processing batch 328/451: 1 files\n",
      "Processing batch 329/451: 1 files\n",
      "Processing batch 330/451: 1 files\n",
      "Processing batch 331/451: 1 files\n",
      "Processing batch 332/451: 1 files\n",
      "Processing batch 333/451: 1 files\n",
      "Processing batch 334/451: 1 files\n",
      "Processing batch 335/451: 1 files\n",
      "Processing batch 336/451: 1 files\n",
      "Processing batch 337/451: 1 files\n",
      "Processing batch 338/451: 1 files\n",
      "Processing batch 339/451: 1 files\n",
      "Processing batch 340/451: 1 files\n",
      "Processing batch 341/451: 1 files\n",
      "Processing batch 342/451: 1 files\n",
      "Processing batch 343/451: 1 files\n",
      "Processing batch 344/451: 1 files\n",
      "Processing batch 345/451: 1 files\n",
      "Processing batch 346/451: 1 files\n",
      "Processing batch 347/451: 1 files\n",
      "Processing batch 348/451: 1 files\n",
      "Processing batch 349/451: 1 files\n",
      "Processing batch 350/451: 1 files\n",
      "Processing batch 351/451: 1 files\n",
      "Processing batch 352/451: 1 files\n",
      "Processing batch 353/451: 1 files\n",
      "Processing batch 354/451: 1 files\n",
      "Processing batch 355/451: 1 files\n",
      "Processing batch 356/451: 1 files\n",
      "Processing batch 357/451: 1 files\n",
      "Processing batch 358/451: 1 files\n",
      "Processing batch 359/451: 1 files\n",
      "Processing batch 360/451: 1 files\n",
      "Processing batch 361/451: 1 files\n",
      "Processing batch 362/451: 1 files\n",
      "Processing batch 363/451: 1 files\n",
      "Processing batch 364/451: 1 files\n",
      "Processing batch 365/451: 1 files\n",
      "Processing batch 366/451: 1 files\n",
      "Processing batch 367/451: 1 files\n",
      "Processing batch 368/451: 1 files\n",
      "Processing batch 369/451: 1 files\n",
      "Processing batch 370/451: 1 files\n",
      "Processing batch 371/451: 1 files\n",
      "Processing batch 372/451: 1 files\n",
      "Processing batch 373/451: 1 files\n",
      "Processing batch 374/451: 1 files\n",
      "Processing batch 375/451: 1 files\n",
      "Processing batch 376/451: 1 files\n",
      "Processing batch 377/451: 1 files\n",
      "Processing batch 378/451: 1 files\n",
      "Processing batch 379/451: 1 files\n",
      "Processing batch 380/451: 1 files\n",
      "Processing batch 381/451: 1 files\n",
      "Processing batch 382/451: 1 files\n",
      "Processing batch 383/451: 1 files\n",
      "Processing batch 384/451: 1 files\n",
      "Processing batch 385/451: 1 files\n",
      "Processing batch 386/451: 1 files\n",
      "Processing batch 387/451: 1 files\n",
      "Processing batch 388/451: 1 files\n",
      "Processing batch 389/451: 1 files\n",
      "Processing batch 390/451: 1 files\n",
      "Processing batch 391/451: 1 files\n",
      "Processing batch 392/451: 1 files\n",
      "Processing batch 393/451: 1 files\n",
      "Processing batch 394/451: 1 files\n",
      "Processing batch 395/451: 1 files\n",
      "Processing batch 396/451: 1 files\n",
      "Processing batch 397/451: 1 files\n",
      "Processing batch 398/451: 1 files\n",
      "Processing batch 399/451: 1 files\n",
      "Processing batch 400/451: 1 files\n",
      "Processing batch 401/451: 1 files\n",
      "Processing batch 402/451: 1 files\n",
      "Processing batch 403/451: 1 files\n",
      "Processing batch 404/451: 1 files\n",
      "Processing batch 405/451: 1 files\n",
      "Processing batch 406/451: 1 files\n",
      "Processing batch 407/451: 1 files\n",
      "Processing batch 408/451: 1 files\n",
      "Processing batch 409/451: 1 files\n",
      "Processing batch 410/451: 1 files\n",
      "Processing batch 411/451: 1 files\n",
      "Processing batch 412/451: 1 files\n",
      "Processing batch 413/451: 1 files\n",
      "Processing batch 414/451: 1 files\n",
      "Processing batch 415/451: 1 files\n",
      "Processing batch 416/451: 1 files\n",
      "Processing batch 417/451: 1 files\n",
      "Processing batch 418/451: 1 files\n",
      "Processing batch 419/451: 1 files\n",
      "Processing batch 420/451: 1 files\n",
      "Processing batch 421/451: 1 files\n",
      "Processing batch 422/451: 1 files\n",
      "Processing batch 423/451: 1 files\n",
      "Processing batch 424/451: 1 files\n",
      "Processing batch 425/451: 1 files\n",
      "Processing batch 426/451: 1 files\n",
      "Processing batch 427/451: 1 files\n",
      "Processing batch 428/451: 1 files\n",
      "Processing batch 429/451: 1 files\n",
      "Processing batch 430/451: 1 files\n",
      "Processing batch 431/451: 1 files\n",
      "Processing batch 432/451: 1 files\n",
      "Processing batch 433/451: 1 files\n",
      "Processing batch 434/451: 1 files\n",
      "Processing batch 435/451: 1 files\n",
      "Processing batch 436/451: 1 files\n",
      "Processing batch 437/451: 1 files\n",
      "Processing batch 438/451: 1 files\n",
      "Processing batch 439/451: 1 files\n",
      "Processing batch 440/451: 1 files\n",
      "Processing batch 441/451: 1 files\n",
      "Processing batch 442/451: 1 files\n",
      "Processing batch 443/451: 1 files\n",
      "Processing batch 444/451: 1 files\n",
      "Processing batch 445/451: 1 files\n",
      "Processing batch 446/451: 1 files\n",
      "Processing batch 447/451: 1 files\n",
      "Processing batch 448/451: 1 files\n",
      "Processing batch 449/451: 1 files\n",
      "Processing batch 450/451: 1 files\n",
      "**** Finalizing Tokenizer Vocabulary\n",
      "************ End of training ***********************\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Main init and training sequence\n",
    "###############################################################\n",
    "# Batch params\n",
    "batch_data_size = 1\n",
    "batch_files_size = 1\n",
    "num_feature_dim = 20\n",
    "epochs =  5\n",
    "\n",
    "# Initialize model\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "for i in range(1000, 1500, 500):\n",
    "  batch_curr = f'rep_{i}'\n",
    "  print(f\"========== batch: {batch_curr} ========== \")\n",
    "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=5, rep_batch=batch_curr, max_file=500)\n",
    "\n",
    "  # train\n",
    "  hash_t = run_files_in_batches(list_train, batch_files_size=batch_files_size, mode='train')\n",
    "  print(\"************ End of training ***********************\")\n",
    "\n",
    "  \"\"\"\n",
    "  # validate\n",
    "  all_pred_label_tensor, all_true_label_tensor, hash_v = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode='validation')\n",
    "  f1_macro = f1_score(all_true_label_tensor, all_pred_label_tensor, average='macro', zero_division=1)\n",
    "  print(f\"**** F1 score: {f1_macro*100:.2f} \\n\")\"\n",
    "  \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
