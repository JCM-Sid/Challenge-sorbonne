{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16312,
     "status": "ok",
     "timestamp": 1742994966616,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "9fc125ff-3caf-4a58-9271-536d388255fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (3.11.14)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\jch_m\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\jch_m\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    }
   ],
   "source": [
    "# Challenge Sorbonne - DST\n",
    "#\n",
    "#!pip install torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import AdamW\n",
    "!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18759,
     "status": "ok",
     "timestamp": 1742994988870,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "3972c2f6-e1a6-4945-ab3e-8c82cf0ec802"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1742995143971,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "3d29a151-3bad-42c9-aee5-94cc5d8faa90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/content': # Google Colab\n",
    "    training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "\n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/split_val-colab-v12.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_set_to_predict.csv'\n",
    "    filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "    filename_trained =      '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/list_hash_trained.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\": #PC1\n",
    "    split_char = '\\\\'\n",
    "\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\Pred_test_v12_500_2803\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "    file_split_training = r'C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\Pred_test_v13\\split_val-pc-v13.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\"\n",
    "    #test_init_file = r\".\\test_set_to_predict.csv\"\n",
    "    #filename_test_results = r\".\\test_prediction.csv\"\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v11_15K_2503\\model_sorbonne_weights.pth'\n",
    "    #filename_test_results = r\".\\test_prediction.csv\"\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\": #PC2\n",
    "    split_char = '\\\\'\n",
    "\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\".\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  'training_set_metadata.csv'\n",
    "    file_split_training = r'.\\split_training-pc.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_path_dir = r\"G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\"\n",
    "    test_init_file = \"test_set_to_predict.csv\"\n",
    "    filename_test_results = \"test_prediction.csv\"\n",
    "    model_save_file =  'model_sorbonne_weights.pth'\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "\n",
    "# read CSV input and save in df\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "df_meta_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1742995083637,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "wCU7s5ZbtSu9",
    "outputId": "f90a395e-1598-4430-ec12-023d86f1c231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23102 entries, 0 to 23101\n",
      "Columns: 454 entries, name to yoda\n",
      "dtypes: int64(453), object(1)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meta_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1742995088880,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "3e8a7303-8f99-4b35-a6de-ec04819f534e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\Pred_test_v12_500_2803\\list_hash_trained.csv\n",
      "taille : 497\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f993af4662324130ca817cf3c7b04d5701a85422cb773e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>370a7ee4dd61e0951a080e4d9d5a29b7b2bddfc832d838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5fb5b616dc35dc898c5b823f3393838b25b57bfd92d078...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "1  bd880010492d29b7ea382eeb76405e6251cf9f51aa8d17...\n",
       "2  f993af4662324130ca817cf3c7b04d5701a85422cb773e...\n",
       "3  370a7ee4dd61e0951a080e4d9d5a29b7b2bddfc832d838...\n",
       "4  5fb5b616dc35dc898c5b823f3393838b25b57bfd92d078..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1742995476399,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "864f8ce9-421f-4a0c-cd1f-b51c6c97c2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0']\n",
      "['G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\40936a67037f8fd8e215b045f9cdf9c55840411316a62c85c8b54f75c6b0a5c8.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\368d2b5ccfd49f01942f462037710146ec3ca5ca8a5318a092bc49bfebfe8bad.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\75010f6d21b0b4451b5465ab8f46b385bb1edc15ed50634f4120352edc49cf3a.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\95e944577e686782e9d725e2e79ef9657a3f3b4f3b9cb265487bcd8e55cba95d.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\72356f26be98c580e23ca7baebe665c781fb2484575b9b05ad38676ad74ffb9e.json']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84 entries, 0 to 83\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   orign path  84 non-null     object\n",
      " 1   batch       84 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_0']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique())\n",
    "print(full_file_list[:5])\n",
    "print(df_training_split.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1742995606261,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0', max_file=0):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    file_with_err = 0\n",
    "    count_files = 0\n",
    "    #max_file = 0\n",
    "\n",
    "    #file_split_training = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\split_training-colab.csv\"\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "        #    print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 200_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                print(\"size\", file_size/1000, full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        list_val_hash.append(full_path_file)\n",
    "        \"\"\"\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "        \"\"\"\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1742995610025,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "84w6HNzrOKjz",
    "outputId": "bca9b4b0-03c1-45ba-d968-2873979f2cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with error: 0\n",
      "81 0 0\n"
     ]
    }
   ],
   "source": [
    "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.01, rep_batch='rep_0')\n",
    "print(len(list_val), len(list_train), len(list_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742995613755,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "class HierarchicalAssemblyTokenizer:\n",
    "    def __init__(self):\n",
    "        # Tokenizer implementation from previous code\n",
    "        # Abbreviated for brevity but would be the full implementation\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "    def fit(self, graph_l, min_freq=1):\n",
    "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        self.unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        # Extract features from node labels\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                # Extract node type (JCC, INST)\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                # Extract operation (xor, push, mov)\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                # Extract registers\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                # Extract memory reference patterns\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                # Extract immediate values\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            self.unique_hashes.append(item['name'])\n",
    "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "            for i, hash_val in enumerate(self.unique_hashes):\n",
    "                self.hash_to_id[hash_val] = i\n",
    "                self.id_to_hash[i] = hash_val\n",
    "\n",
    "        # Build vocabularies with frequency filtering\n",
    "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
    "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
    "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
    "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_from_counts(self, all_counts, min_freq=3):\n",
    "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
    "        self.node_type_vocab.clear()\n",
    "        self.operation_vocab.clear()\n",
    "        self.register_vocab.clear()\n",
    "        self.memory_pattern_vocab.clear()\n",
    "        self.immediate_vocab.clear()\n",
    "\n",
    "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
    "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
    "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
    "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
    "\n",
    "        self.unique_hashes = all_counts['unique_hashes']\n",
    "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "        for i, hash_val in enumerate(self.unique_hashes):\n",
    "            self.hash_to_id[hash_val] = i\n",
    "            self.id_to_hash[i] = hash_val\n",
    "\n",
    "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
    "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
    "        counter = Counter(tokens)\n",
    "        # Add special UNK token\n",
    "        vocab_dict[self.UNK_TOKEN] = 0\n",
    "\n",
    "        # Add tokens that meet minimum frequency\n",
    "        idx = 1\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                vocab_dict[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def _memory_to_pattern(self, mem_ref):\n",
    "        \"\"\"Convert memory reference to pattern\"\"\"\n",
    "        # Replace registers with REG placeholder\n",
    "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
    "        # Replace immediate values with IMM placeholder\n",
    "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
    "        return pattern.strip()\n",
    "\n",
    "    def tokenize(self, node_label):\n",
    "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
    "        features = {\n",
    "            'node_type': self.UNK_TOKEN,\n",
    "            'operation': self.UNK_TOKEN,\n",
    "            'registers': [],\n",
    "            'memory_pattern': self.UNK_TOKEN,\n",
    "            'immediate': self.UNK_TOKEN\n",
    "        }\n",
    "\n",
    "        # Extract node type\n",
    "        node_type_match = self.patterns['node_type'].search(node_label)\n",
    "        if node_type_match:\n",
    "            node_type = node_type_match.group(1)\n",
    "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract operation\n",
    "        op_match = self.patterns['operation'].search(node_label)\n",
    "        if op_match:\n",
    "            operation = op_match.group(1)\n",
    "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract registers\n",
    "        reg_matches = self.patterns['registers'].findall(node_label)\n",
    "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
    "\n",
    "        # Extract memory reference\n",
    "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
    "        if mem_matches:\n",
    "            pattern = self._memory_to_pattern(mem_matches[0])\n",
    "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract immediate values\n",
    "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
    "        if imm_matches:\n",
    "            imm = imm_matches[0]\n",
    "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
    "\n",
    "        return features\n",
    "\n",
    "    def encode_nodelabel(self, node_label):\n",
    "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
    "        features = self.tokenize(node_label)\n",
    "\n",
    "        # Encode each feature\n",
    "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
    "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
    "\n",
    "        # Encode registers (take up to 3, pad if fewer)\n",
    "        register_indices = []\n",
    "        for i in range(min(3, len(features['registers']))):\n",
    "            reg = features['registers'][i]\n",
    "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
    "            register_indices.append(reg_idx)\n",
    "\n",
    "        # Pad register indices if needed\n",
    "        while len(register_indices) < 3:\n",
    "            register_indices.append(0)  # 0 for padding\n",
    "\n",
    "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
    "            features['memory_pattern'],\n",
    "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        immediate_idx = self.immediate_vocab.get(\n",
    "            features['immediate'],\n",
    "            self.immediate_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Combine all indices into a feature vector\n",
    "        encoded = np.array([\n",
    "            node_type_idx,\n",
    "            operation_idx,\n",
    "            register_indices[0],\n",
    "            register_indices[1],\n",
    "            register_indices[2],\n",
    "            memory_pattern_idx,\n",
    "            immediate_idx\n",
    "        ], dtype=np.int64)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_graph(self, digraph):\n",
    "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
    "        node_features = {}\n",
    "\n",
    "        for node_id in digraph.nodes():\n",
    "            label = digraph.nodes[node_id].get('label', '')\n",
    "            node_features[node_id] = self.encode_nodelabel(label)\n",
    "\n",
    "        return node_features\n",
    "\n",
    "    def encode_hash(self, hash_file):\n",
    "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return the size of each vocabulary\"\"\"\n",
    "        return {\n",
    "            'node_type': len(self.node_type_vocab),\n",
    "            'operation': len(self.operation_vocab),\n",
    "            'register': len(self.register_vocab),\n",
    "            'memory_pattern': len(self.memory_pattern_vocab),\n",
    "            'immediate': len(self.immediate_vocab)\n",
    "        }\n",
    "\n",
    "    def get_tokens_and_counts(self, graph_l):\n",
    "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            unique_hashes.append(item['name'])\n",
    "\n",
    "        return {\n",
    "            'node_types': node_types,\n",
    "            'operations': operations,\n",
    "            'registers': registers,\n",
    "            'memory_patterns': memory_patterns,\n",
    "            'immediates': immediates,\n",
    "            'unique_hashes': unique_hashes,\n",
    "        }\n",
    "\n",
    "class AssemblyGraphDataset:\n",
    "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
    "        \"\"\"\n",
    "        Prepares a dataset of assembly graphs for GAT model training\n",
    "        Args:\n",
    "            graph_list: List of NetworkX digraphs representing assembly code\n",
    "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
    "            node_feature_dim: Dimension of node embeddings\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        # Initialize embedding layers for each feature type\n",
    "        num_feature_type = 5\n",
    "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
    "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
    "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
    "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
    "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
    "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
    "\n",
    "        # Process graphs into PyTorch Geometric Data objects\n",
    "        self.data_list = []\n",
    "        for graph in self.graph_list:\n",
    "            # process graph info\n",
    "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
    "\n",
    "    def _process_graph(self, graph, hash_name):\n",
    "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
    "        # Create a node ID mapping for consecutive IDs\n",
    "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "        # Get node features\n",
    "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
    "\n",
    "        # Convert to tensor-friendly format\n",
    "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
    "        for node_id, features in node_features_dict.items():\n",
    "            x[node_mapping[node_id]] = torch.tensor(features)\n",
    "\n",
    "        # Create edge index\n",
    "        edge_list = list(graph.edges())\n",
    "        if not edge_list:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
    "\n",
    "        # Endcode Hash name\n",
    "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
    "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
    "\n",
    "        # Get embeddings before creating Data object\n",
    "        x = self.get_embeddings(x)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
    "        return data\n",
    "\n",
    "    def optimize_edge_index(self, edge_list, node_mapping):\n",
    "        \"\"\"\n",
    "        Optimizes the creation of edge_index tensor for graph representation.\n",
    "        Args:\n",
    "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
    "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Optimized edge_index tensor.\n",
    "        \"\"\"\n",
    "        if not edge_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "        num_edges = len(edge_list)\n",
    "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "\n",
    "        for i, (src, tgt) in enumerate(edge_list):\n",
    "            src_indices[i] = node_mapping[src]\n",
    "            tgt_indices[i] = node_mapping[tgt]\n",
    "\n",
    "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
    "        # Split features into their components\n",
    "        node_type_idx = x[:, 0]\n",
    "        operation_idx = x[:, 1]\n",
    "        register_idx1 = x[:, 2]\n",
    "        register_idx2 = x[:, 3]\n",
    "        register_idx3 = x[:, 4]\n",
    "        memory_pattern_idx = x[:, 5]\n",
    "        immediate_idx = x[:, 6]\n",
    "\n",
    "        # Get embeddings for each component\n",
    "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
    "        operation_emb = self.operation_embedding(operation_idx)\n",
    "\n",
    "        # Combine register embeddings (average them)\n",
    "        register_emb = (self.register_embedding(register_idx1) +\n",
    "                        self.register_embedding(register_idx2) +\n",
    "                        self.register_embedding(register_idx3)) / 3\n",
    "\n",
    "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
    "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        return torch.cat([\n",
    "            node_type_emb,\n",
    "            operation_emb,\n",
    "            register_emb,\n",
    "            memory_pattern_emb,\n",
    "            immediate_emb\n",
    "        ], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Graph Attention Network for assembly code analysis\n",
    "Args:\n",
    "    dataset: AssemblyGraphDataset instance\n",
    "    hidden_dim: Hidden dimension of GAT layers\n",
    "    output_dim: Output dimension of node embeddings\n",
    "    heads: Number of attention heads\n",
    "    dropout: Dropout rate\n",
    "\"\"\"\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, x, edge_index, hash_encoded, batch):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1742995627385,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "INiZZqldtSu-"
   },
   "outputs": [],
   "source": [
    "# Create empty list to store dataframes\n",
    "# init parameters to loop the file loading by batch\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "random_seed = 42\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                #f = open(full_path_file, 'r')\n",
    "                digraph_str = f.read()\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append({\n",
    "                    'name': hash_file,\n",
    "                    'graph_input' : G\n",
    "                })\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
    "    if os.path.exists(model_save_file):\n",
    "      if torch.cuda.is_available():\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "      else:\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        count_batch = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # get the different components of the dataset\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
    "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        #avg_train_loss = train_loss / len(train_dataloader)\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # save model trained\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
    "    if os.path.exists(model_save_file):\n",
    "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "\n",
    "    model.eval() #set the model to evaluation mode.\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "    #hash_v = []\n",
    "\n",
    "    count_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_label.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #hash_v.append(list_hash_batch)\n",
    "\n",
    "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "\n",
    "    return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "    \"\"\"\n",
    "    Processes files in batches for tokenization and model training/evaluation in a single loop.\n",
    "    Args:\n",
    "        full_file_paths (list): List of full file paths to process.\n",
    "        batch_files_size (int): Number of files to process in each batch for tokenization.\n",
    "        batch_data_size (int): Batch size for the DataLoader during training/evaluation.\n",
    "        num_feature_dim (int, optional): Dimension of node features. Defaults to None.\n",
    "        mode (str): 'train' or other mode (e.g., 'eval'). Defaults to 'train'.\n",
    "        epochs (int): Number of training epochs per batch (if mode is 'train'). Defaults to 3.\n",
    "        tokenizer: Tokenizer object with methods like get_tokens_and_counts and fit_from_counts.\n",
    "        process_batch (callable): Function to process a batch of file paths and return a list of graph-like objects.\n",
    "        train_model (callable): Function to train the model with a DataLoader and other info.\n",
    "    \"\"\"\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) + 1\n",
    "\n",
    "    hash_v = []\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    all_counts = {\n",
    "        'node_types': [],\n",
    "        'operations': [],\n",
    "        'registers': [],\n",
    "        'memory_patterns': [],\n",
    "        'immediates': [],\n",
    "        'unique_hashes': [],\n",
    "    }\n",
    "\n",
    "    print(\"**** Tokenization and Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        batch_num = i // batch_files_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if not batch_graph_list:\n",
    "            print(\"graph list is empty for this batch\")\n",
    "            continue\n",
    "\n",
    "        # Tokenization for the current batch\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list)\n",
    "        for key in all_counts:\n",
    "            if key in tokens:\n",
    "                all_counts[key].extend(tokens[key])\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "        # Create Dataset and DataLoader for the current batch\n",
    "        if tokenizer and num_feature_dim is not None:\n",
    "            dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
    "            curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
    "            list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
    "\n",
    "            # Training or other processing for the current batch\n",
    "            if mode == 'train' and train_model:\n",
    "                print(f\"**** Training on batch {batch_num}/{nb_batch}\")\n",
    "                train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=epochs)\n",
    "                hash_t.extend(list_of_hash)\n",
    "            # Add logic for other modes (e.g., 'eval') here if needed\n",
    "            else:\n",
    "                pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
    "                all_true_label.append(true_labels.cpu())\n",
    "                all_pred_label.append(pred_labels.cpu())\n",
    "                hash_v.append(list_of_hash)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Final fitting of the tokenizer after processing all batches\n",
    "    if all_counts:\n",
    "        print(\"**** Finalizing Tokenizer Vocabulary\")\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "    #print(\"**** Processing Complete\")\n",
    "    # save all labels as tensor to return if mode= 'validation'\n",
    "    if mode == 'validation':\n",
    "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "\n",
    "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "    elif mode == 'train':\n",
    "        return hash_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1012311,
     "status": "ok",
     "timestamp": 1742996849138,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "avU1v3vVtSu-",
    "outputId": "6997934b-0488-425c-8002-82e6e68dcf47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch: rep_0 ========== \n",
      "Number of files with error: 0\n",
      "There are 81 files for validation\n",
      "**** Tokenization and Dataset Processing & Training\n",
      "Processing batch 1/82: 1 files\n",
      "Processing batch 2/82: 1 files\n",
      "Processing batch 3/82: 1 files\n",
      "Processing batch 4/82: 1 files\n",
      "Processing batch 5/82: 1 files\n",
      "Processing batch 6/82: 1 files\n",
      "Processing batch 7/82: 1 files\n",
      "Processing batch 8/82: 1 files\n",
      "Processing batch 9/82: 1 files\n",
      "Processing batch 10/82: 1 files\n",
      "Processing batch 11/82: 1 files\n",
      "Processing batch 12/82: 1 files\n",
      "Processing batch 13/82: 1 files\n",
      "Processing batch 14/82: 1 files\n",
      "Processing batch 15/82: 1 files\n",
      "Processing batch 16/82: 1 files\n",
      "graph list is empty for this batch\n",
      "Processing batch 17/82: 1 files\n",
      "Processing batch 18/82: 1 files\n",
      "Processing batch 19/82: 1 files\n",
      "Processing batch 20/82: 1 files\n",
      "Processing batch 21/82: 1 files\n",
      "Processing batch 22/82: 1 files\n",
      "Processing batch 23/82: 1 files\n",
      "Processing batch 24/82: 1 files\n",
      "Processing batch 25/82: 1 files\n",
      "Processing batch 26/82: 1 files\n",
      "Processing batch 27/82: 1 files\n",
      "Processing batch 28/82: 1 files\n",
      "Processing batch 29/82: 1 files\n",
      "Processing batch 30/82: 1 files\n",
      "Processing batch 31/82: 1 files\n",
      "Processing batch 32/82: 1 files\n",
      "Processing batch 33/82: 1 files\n",
      "Processing batch 34/82: 1 files\n",
      "Processing batch 35/82: 1 files\n",
      "Processing batch 36/82: 1 files\n",
      "Processing batch 37/82: 1 files\n",
      "Processing batch 38/82: 1 files\n",
      "Processing batch 39/82: 1 files\n",
      "Processing batch 40/82: 1 files\n",
      "Processing batch 41/82: 1 files\n",
      "Processing batch 42/82: 1 files\n",
      "Processing batch 43/82: 1 files\n",
      "Processing batch 44/82: 1 files\n",
      "Processing batch 45/82: 1 files\n",
      "Processing batch 46/82: 1 files\n",
      "Processing batch 47/82: 1 files\n",
      "Processing batch 48/82: 1 files\n",
      "Processing batch 49/82: 1 files\n",
      "Processing batch 50/82: 1 files\n",
      "Processing batch 51/82: 1 files\n",
      "Processing batch 52/82: 1 files\n",
      "Processing batch 53/82: 1 files\n",
      "Processing batch 54/82: 1 files\n",
      "Processing batch 55/82: 1 files\n",
      "Processing batch 56/82: 1 files\n",
      "Processing batch 57/82: 1 files\n",
      "Processing batch 58/82: 1 files\n",
      "Processing batch 59/82: 1 files\n",
      "Processing batch 60/82: 1 files\n",
      "Processing batch 61/82: 1 files\n",
      "Processing batch 62/82: 1 files\n",
      "Processing batch 63/82: 1 files\n",
      "Processing batch 64/82: 1 files\n",
      "Processing batch 65/82: 1 files\n",
      "Processing batch 66/82: 1 files\n",
      "Processing batch 67/82: 1 files\n",
      "Processing batch 68/82: 1 files\n",
      "Processing batch 69/82: 1 files\n",
      "Processing batch 70/82: 1 files\n",
      "Processing batch 71/82: 1 files\n",
      "Processing batch 72/82: 1 files\n",
      "Processing batch 73/82: 1 files\n",
      "Processing batch 74/82: 1 files\n",
      "Processing batch 75/82: 1 files\n",
      "Processing batch 76/82: 1 files\n",
      "Processing batch 77/82: 1 files\n",
      "Processing batch 78/82: 1 files\n",
      "Processing batch 79/82: 1 files\n",
      "Processing batch 80/82: 1 files\n",
      "Processing batch 81/82: 1 files\n",
      "**** Finalizing Tokenizer Vocabulary\n",
      "**** F1 score: 37.69 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Main init and training sequence\n",
    "###############################################################\n",
    "# Batch params\n",
    "batch_data_size = 1\n",
    "batch_files_size = 1\n",
    "num_feature_dim = 15\n",
    "epochs =  5\n",
    "\n",
    "# Initialize model\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "batch_curr = 'rep_0'\n",
    "for i in range(0, 100, 500):\n",
    "  batch_curr = f'rep_{i}'\n",
    "  print(f\"========== batch: {batch_curr} ========== \")\n",
    "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.01, rep_batch=batch_curr, max_file=0)\n",
    "\n",
    "  # validate\n",
    "  all_pred_label_tensor, all_true_label_tensor, hash_v = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode='validation')\n",
    "  f1_macro = f1_score(all_true_label_tensor, all_pred_label_tensor, average='macro', zero_division=1)\n",
    "  print(f\"**** F1 score: {f1_macro*100:.2f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 455 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, type, 64-bit execution via heavens gate, 64bits, PEB access, accept command line arguments, access the Windows event log, act as TCP client, allocate RW memory, allocate RWX memory, allocate memory, allocate or change RW memory, allocate or change RWX memory, allocate thread local storage, android, anorganix, apatch, apk, arm, armadillo, aspack, asprotect, assembly, attach user process memory, attachment, authenticate HMAC, block operations on executable memory pages using Arbitrary Code Guard, bobsoft, bypass Windows File Protection, calculate modulo 256 via x86 assembly, calls-wmi, capture screenshot, capture screenshot in Go, cexe, change file permission on Linux, change memory protection, change the wallpaper, check HTTP status code, check OS version, check ProcessDebugFlags, check ProcessDebugPort, check SystemKernelDebuggerInformation, check for OutputDebugString error, check for PEB BeingDebugged flag, check for PEB NtGlobalFlag flag, check for VM using instruction VPCEXT, check for Windows sandbox via genuine state, check for Windows sandbox via process name, check for debugger via API, check for foreground window switch, check for hardware breakpoints, check for process debug object, check for protected handle exception, check for sandbox and av modules, check for sandbox username or hostname, check for software breakpoints, check for time delay via GetTickCount, check for time delay via QueryPerformanceCounter, check for trap flag exception, check for unexpected memory writes, check for unmoving mouse cursor, check if file exists, check if process is running under wine, check mutex, check mutex and exit, check process job object, checks-bios, checks-cpu-name, checks-disk-space, checks-gps, checks-hostname, checks-memory-available, checks-network-adapters, checks-usb-bus, checks-user-input, clipboard, compress data using GZip in .NET, compress data using LZO, compress data via WinAPI, compress data via ZLIB inflate or deflate, compute adler32 checksum, connect TCP socket, connect pipe, connect to HTTP server, connect to URL, contain an embedded PE file, contain anti-disasm techniques, contain obfuscated stackstrings, contains-elf, contains-pe, copy file, corrupt, create HTTP request, create TCP socket, create UDP socket, create a process with modified I/O handles and window, create directory, create mailslot, create mutex, create new application domain in .NET, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 455 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.read_csv(r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\Pred_test_v12_500_2803\\test_prediction_val.csv\")\n",
    "df_res_pred = df_result\n",
    "df_res_true = df_result\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\40936a67037f8fd8e215b045f9cdf9c55840411316a62c85c8b54f75c6b0a5c8.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\368d2b5ccfd49f01942f462037710146ec3ca5ca8a5318a092bc49bfebfe8bad.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\75010f6d21b0b4451b5465ab8f46b385bb1edc15ed50634f4120352edc49cf3a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\95e944577e686782e9d725e2e79ef9657a3f3b4f3b9cb265487bcd8e55cba95d.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\72356f26be98c580e23ca7baebe665c781fb2484575b9b05ad38676ad74ffb9e.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\76b1b2485d06f974b85c7418a9b21736273be7ac13ee5f1270d6c8bc4926b020.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\84c3c971abcc07fcfce7269735a837f63d68d184af2cc0c28a949ab850c61b91.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d4caac06ff94ed782fdd980f4ef2fa18203ddfd2d84a5dd89aed63fcf428fdec.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\66ba064105a10960b3fc13d982c69390e8d44c8452edea47e8aa8ef8df943153.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d0fd084659d6ebbaa6267dd7876104028f701e618e20a259c83fc43a990f6e4a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\657e2497e96656ebdf912ec6432c6ed62ebcc9ab519c5da4ffaeef6a056f98d2.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d51e2d782f86a6765ca532fb42df4d7ec5b86a9d7190525b8fb7a8468d3d1cc7.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\8adf81619e3b1262278ab7333849d1faf92be25668501f4ecc60536ab2a9cc76.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d2a812d6a05f75f2169a4f26634b316e689b0bb037b63e46268069292805439d.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\b769dcf82ddffe68fa80718026e3f915b6f58688133dbe09334e61af55e9b0d1.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\8cbe77aaed72dff9f87eafd87ade3aff2059b537b3903bd25b47d7fdaf2c898a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\ee0b5131e57d84cf8f2c00d5b87f0704c2f1b40663cd091cce46ecc070a322c5.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\7734053a0ecafa4efa1e45fdd8afad1303a021aaaddb22b15fd3c289e8f86f36.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\5e136ae069ca10ede20f636a953f78769379feefc4da218dc3682e510734616a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\efecb17d9d73082bf28a6e7c6bb87a81c65a59b2d4d14251678da3cffa6a12a1.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\cc687fe3741bbde1dd142eac0ef59fd1d4457daee43cdde23bb162ef28d04e64.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\e11434558518a2b9a43ce0857e1149c927916c208931f6c3a03a921a307ad628.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\777593c76570c49cdfa8682e6db2f76e6cb91fcac1d6319627fc0bb86ac8e2b5.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\1d7dfa0e92935d231c651e83c81e9c96d17c35d15da133df97434d5c77f176c8.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f819c070084c5ab608d6a1d58e7a42bfe8a304aa0003e70a7de72c9fec835f27.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\101f5cfb89dc5c5764d204c78d8c337ac930d11e596d86dfc5063058f34d11f8.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\022e68b193b8ea272925b9f355db03f95728200ebea6c7aa251a037cdc2eca05.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\2f59bb2973217cbf658dd0b0ac584672b0a012233680a5910f1e4f92806be705.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\3074bd6e3d7e283e9eda2e4d7525ca7da95c6f266c8d20892984173490f87c73.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\998c69179f6cb7d4ec4f2692c04ba56fa5ed7debc0b61fe3cbb757a58652989a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\b8a39b79d61a70ff8675db18f759cae82aefc9f0c3dc48087a44f5805c6da7e7.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\1bd54cd37218b10aca07d2d0267c30c7064d50cd1f4fbbbde2571d84d4d3d6bd.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\9a95b31753368994644c602a4eb8378c40400080594a591173385a309fbf9259.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\7c4c6612d976fc3f55d83592242228880c151a440c394426c0007f71a319877a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\249fe2142afc7df36da8e719865c55cdacb7175f733fe77741212e1e991a0abc.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\1126540dd257bc0017eb6d23808a3bb05e140b972021e44055084e6eaa7c4d32.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\5f232cf3e5cbe9ec01400625c3e8fd75c150d2b9dbe7e0453937de552a1c83ba.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d93704a289f0489b2d9719a0dfa006b0155eb424ea1f8913972af817ab8f1875.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\8a7b7528db30ab123b060d8e41954d95913c07bb40cdae32e97f9edb0baf79c7.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\94de1cacf61223285ee8fef5f9e8b76c9c3edd769099a34c923360c5f605bc0a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\077ee50ed27cbc48f300e39d738658a78d96f1858fc512d35c5285dcd8c3ee9d.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\59494dfac199e7e5be5629c13106824f9507dc6bbe85111905d2cd3470f53843.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f194de4ae308f99bee66e368ee721cdfc14f29e801bc813a4198e1da8d64f00f.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\546c7504a27e1181744715bd43b530375bacc4625bd1df23cccd9a19f2be9a04.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\5239e9d7c4442fff121c20196cff5d1ffea3425e9f0ee8249e90a25d39b8bfbf.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\59c0038e358be524c0d00c824bedd06d5203a08b2373118b862fbcf9b34ee496.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\ff7614513da3c8a99c131a96f0e47b690d2bade72a1a7b5e6d1bc0f3295fa081.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\1c5b4e004cdb4b45be7e2bd10e8e777457e9d4cb6a0f7eda8b79dd9c54615816.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\96af747c1cbeff2c35c1a8223b43cff11f877759a40082e17abcd189cc188fbd.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\34c5b57fd289856d4d57b03b23aa7e0b5ad65a856a5a7c70a387fbe52b72934a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\a47fce16b1a026ff310ef9c72065bd4bf83497b6398c94bb503c9b34a342d684.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\146fd7e915d18fd2cced440b481f536df2aebfe42c65647d587c51795c8a9f17.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\ac41f34b6373014fbf4afb4eb69d311fd863b45b38b16d48f40399b3346a72ca.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d79f1ead5bd9ad461cb677e33c5fa3dd20f780d94faeb270d2ae5f1d074fe91f.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\baa049267e73b97695d472ef4025f625d68714ad6391d123a860ae787a621668.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d9a4500412cd005acab7002a04bfaf92915fb177303173fb64004e9537f3c0fd.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\d28890e9fba96039bd2755be68fd005a3ca003d240e295b523816038ccb6be36.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\e066e03f5da8f44ddb3d8272898b1df654c3d3725a4ae5f7af40f063d177ef64.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\5a7549d6055688d9178bcd5e1586568c727589ce1a17c13252182a3529bf582f.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\39d903375920c257a54127eab0ca96dbcf969720d3db6b0f747e7e283f9b7d2c.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\36c85c0ba7046433c48af6d2492dfdcd0f5c7ee26b137249acc7de4ab48ea6e2.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\cef5b60321f17991400a19072052535638c0a5c02d338234686552deadeea82e.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\a235a96f1965bc01cc332ab8bad8591ec670f5b21d78fa66c1fe26f14513111b.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\b07dd9b408842d5e52fb51f1beca137e1199cc410aa0313e90e11842c02f28d2.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\20592b59e922f374395d606e66b20b13c942706f0bf0ce60a06682fa168e14e2.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\08be9be2e66bdc6ef0f0997df89ecf77ae7c88e0100254e519bf40cf3a1f3bd9.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\567ca88b9680494ae248d8899195fb5894ae79e60c4a3b9f47153fdf6379d6e6.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\952e5a3751be46366114fd83e47bb347ac0c0db8e146aac6bac43bb1bc1ea40b.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\ebf9db114edebbb286bf0cae49983bf3d69825df56574ee80fe52ab0d7095737.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f71a640478c1d2e3ffe3249fa067cbeaf5b705b901d7ab9fdbe683b0b1e41d81.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\cbb63bab7cd984f54e158e362d97c00d559bd9531617128bd7382c8a961ff20f.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\4387bcd4d53afd03049d33d97a28b3e6b6b8e8e95173c5185d869a3ded43b080.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\bc45a8c9f9000742eabeb8cf95b15f12dd414f1dc2760671536282c53021b98a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\200ad283a72005b069d0025999780e6fac7821626fbffb46a9096ad24c8d6962.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f8ce012a0003172daab87cf4c5eb1edb3f986ae105d5f7e10d081f25a404780b.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\9b4d066e939d875c65a7c196b519b715c36546092e5a73e942955e1ac55a5431.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\c456b91c2cfe4edcd78237bf94046e7256f95153faea0b54a034f709e3aa192e.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\5bfde8d8e5ab7cb5174bacc5c47ad832c86fd39a4c4bce59d77efb955f39898a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\812e9898afa4f02decbb277dd635e7c61efa1605fdbc36e34d0de6468fc3480a.json\n",
      "G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\da9f07c7013154689326d9f19d48fbdfa8f7fa3d57099b9b307a168a55903def.json\n"
     ]
    }
   ],
   "source": [
    "  #all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "\n",
    "\n",
    "for row_index in range(len(all_pred_label_tensor)):\n",
    "  print(list_val[row_index])\n",
    "  pred_list = all_pred_label_tensor[row_index].tolist()  # Convert tensor row to list\n",
    "  int_pred_list = [int(val) for val in pred_list]  # Convert to int\n",
    "  df_res_pred.loc[row_index,0]=list_val[row_index]\n",
    "\n",
    "  true_list = all_true_label_tensor[row_index].tolist()\n",
    "  int_true_list = [int(val) for val in true_list]  # Convert to int\n",
    "  df_res_true.loc[row_index, 0]=list_val[row_index]\n",
    "  # Update the DataFrame\n",
    "  for j in range(len(int_true_list)):\n",
    "      df_res_pred.iloc[row_index, j+2] = int_pred_list[j]\n",
    "      df_res_true.iloc[row_index, j+2] = int_true_list[j]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>...</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  name type 64-bit execution via heavens gate 64bits PEB access  \\\n",
       "0  NaN  NaN                                 0      1          0   \n",
       "1  NaN  NaN                                 0      0          1   \n",
       "2  NaN  NaN                                 0      1          0   \n",
       "3  NaN  NaN                                 0      0          0   \n",
       "4  NaN  NaN                                 0      0          0   \n",
       "\n",
       "  accept command line arguments access the Windows event log  \\\n",
       "0                             0                            0   \n",
       "1                             0                            0   \n",
       "2                             1                            0   \n",
       "3                             0                            1   \n",
       "4                             0                            0   \n",
       "\n",
       "  act as TCP client allocate RW memory allocate RWX memory  ... wise worm  \\\n",
       "0                 0                  0                   0  ...    0    0   \n",
       "1                 0                  0                   0  ...    0    0   \n",
       "2                 0                  0                   0  ...    0    0   \n",
       "3                 0                  0                   0  ...    0    0   \n",
       "4                 1                  0                   0  ...    0    0   \n",
       "\n",
       "  write and execute a file write clipboard data write file on Linux  \\\n",
       "0                        0                    0                   0   \n",
       "1                        0                    1                   0   \n",
       "2                        0                    0                   0   \n",
       "3                        0                    0                   0   \n",
       "4                        1                    1                   0   \n",
       "\n",
       "  write file on Windows write pipe xorcrypt yoda  \\\n",
       "0                     1          0        0    0   \n",
       "1                     1          0        0    0   \n",
       "2                     1          0        0    0   \n",
       "3                     1          0        0    0   \n",
       "4                     1          0        0    0   \n",
       "\n",
       "                                                   0  \n",
       "0  G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...  \n",
       "1  G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...  \n",
       "2  G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...  \n",
       "3  G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...  \n",
       "4  G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1...  \n",
       "\n",
       "[5 rows x 456 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sauve les results partiels\n",
    "df_res_true.to_csv(f\"val_true.csv\", header=True ,index=False )\n",
    "df_res_pred.to_csv(f\"val_pred.csv\", header=True ,index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- SHAP Analysis ---\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Explainer pour le modèle linéaire\u001b[39;00m\n\u001b[32m      7\u001b[39m explainer = shap.LinearExplainer(model, X_train)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "# --- SHAP Analysis ---\n",
    "\n",
    "# Explainer pour le modèle linéaire\n",
    "explainer = shap.LinearExplainer(model, X_train)\n",
    "\n",
    "# Calculer les valeurs SHAP pour l'ensemble de test\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualisation du résumé des valeurs SHAP\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.title(\"Résumé de l'Importance des Caractéristiques (via SHAP)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.title(\"Impact des Caractéristiques sur la Prédiction (via SHAP)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse de la dépendance SHAP pour l'éducation\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.dependence_plot(\"Education\", shap_values, X_test)\n",
    "plt.title(\"Dépendance SHAP pour l'Éducation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP Analysis avec le facteur de confusion inclus ---\n",
    "\n",
    "X_with_S = synthetic_data[['Education', 'Experience', 'City_Size', 'Languages_Spoken', 'Socioeconomic_Status']]\n",
    "Y_salary_with_S = synthetic_data['Salary']\n",
    "X_train_S, X_test_S, Y_train_S, Y_test_S = train_test_split(X_with_S, Y_salary_with_S, test_size=0.2, random_state=42)\n",
    "\n",
    "model_with_S = LinearRegression()\n",
    "model_with_S.fit(X_train_S, Y_train_S)\n",
    "\n",
    "explainer_with_S = shap.LinearExplainer(model_with_S, X_train_S)\n",
    "shap_values_with_S = explainer_with_S.shap_values(X_test_S)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "shap.summary_plot(shap_values_with_S, X_test_S, plot_type=\"bar\")\n",
    "plt.title(\"Résumé de l'Importance des Caractéristiques (avec Statut Socioéconomique)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "shap.summary_plot(shap_values_with_S, X_test_S)\n",
    "plt.title(\"Impact des Caractéristiques sur la Prédiction (avec Statut Socioéconomique)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.dependence_plot(\"Education\", shap_values_with_S, X_test_S)\n",
    "plt.title(\"Dépendance SHAP pour l'Éducation (avec Statut Socioéconomique)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.dependence_plot(\"Socioeconomic_Status\", shap_values_with_S, X_test_S)\n",
    "plt.title(\"Dépendance SHAP pour le Statut Socioéconomique\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nExplication avec SHAP (SHapley Additive exPlanations):\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"SHAP values provide insights into how each feature contributes to the prediction of each individual instance.\")\n",
    "print(\"They decompose the prediction by quantifying the additive contribution of each feature.\")\n",
    "\n",
    "print(\"\\n**Analyse du premier modèle (sans Statut Socioéconomique):**\")\n",
    "print(\"- Le **Summary Plot (bar)** montre l'importance globale de chaque caractéristique pour le modèle. Les barres plus longues indiquent une plus grande importance.\")\n",
    "print(\"- Le **Summary Plot (points)** donne plus d'informations :\")\n",
    "print(\"  - L'axe horizontal représente la valeur SHAP (l'impact de la caractéristique sur la prédiction).\")\n",
    "print(\"  - L'axe vertical liste les caractéristiques, ordonnées par importance.\")\n",
    "print(\"  - Chaque point représente une instance du jeu de données.\")\n",
    "print(\"  - La couleur du point indique la valeur de la caractéristique (rouge = haute, bleu = basse).\")\n",
    "print(\"  - On peut observer si une valeur élevée d'une caractéristique tend à augmenter ou diminuer la prédiction.\")\n",
    "print(\"- Le **Dependence Plot** pour l'Éducation montre comment la valeur SHAP de l'Éducation varie en fonction de sa valeur réelle. Il peut révéler des relations non linéaires et les interactions avec d'autres caractéristiques (la couleur des points peut représenter une autre caractéristique).\")\n",
    "\n",
    "print(\"\\n**Analyse du deuxième modèle (avec Statut Socioéconomique):**\")\n",
    "print(\"- En incluant le 'Socioeconomic_Status' dans le modèle, les valeurs SHAP pour les autres caractéristiques (comme l'Éducation) peuvent changer.\")\n",
    "print(\"- Le nouveau **Summary Plot** montrera l'importance relative de toutes les caractéristiques, y compris le 'Socioeconomic_Status'. On s'attend à ce que le 'Socioeconomic_Status' ait une importance significative étant donné son rôle de confondeur dans la génération des données.\")\n",
    "print(\"- Le **Dependence Plot** pour l'Éducation dans ce deuxième modèle devrait montrer l'impact de l'Éducation sur le salaire *après avoir tenu compte* du 'Socioeconomic_Status'. La relation observée pourrait être différente de celle du premier modèle.\")\n",
    "print(\"- Le **Dependence Plot** pour le 'Socioeconomic_Status' montrera son impact direct sur la prédiction du salaire dans ce modèle.\")\n",
    "\n",
    "print(\"\\n**Interprétation concernant le biais de confusion avec SHAP:**\")\n",
    "print(\"- En comparant les valeurs SHAP et les summary plots des deux modèles, on peut observer comment l'importance et l'impact des autres caractéristiques (notamment l'Éducation et potentiellement l'Expérience) changent lorsque le facteur de confusion est inclus dans le modèle.\")\n",
    "print(\"- Si le biais de confusion est présent, le premier modèle pourrait surestimer ou sous-estimer l'impact direct de certaines caractéristiques sur le salaire, car il capture également l'effet indirect via le confondeur.\")\n",
    "print(\"- Le deuxième modèle, en incluant le confondeur, fournit des estimations SHAP qui reflètent l'impact direct de chaque caractéristique *conditionnellement* aux autres caractéristiques, y compris le confondeur, aidant ainsi à démêler les relations causales potentielles.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
