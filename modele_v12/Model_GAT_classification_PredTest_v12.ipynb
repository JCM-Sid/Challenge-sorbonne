{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14994,
     "status": "ok",
     "timestamp": 1743175095724,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "2faf6144-bc3b-4331-fbcb-df07c56e7f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    }
   ],
   "source": [
    "# Challenge Sorbonne - DST\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31453,
     "status": "ok",
     "timestamp": 1743175127338,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "2774ab57-d75b-42f6-e629-9b2f32c7d279"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1743175128813,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "2e329095-9a8c-4121-bef3-2107aee72915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/content': # Google Colab\n",
    "    #training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "    #file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/split_training-colab.csv'\n",
    "\n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/model_sorbonne_weights.pth'\n",
    "    filename_trained =    '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge_v12/list_hash_trained.csv'\n",
    "\n",
    "    split_char = '/'\n",
    "    test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/test_set_to_predict.csv'\n",
    "    filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/test_prediction.csv'\n",
    "    filename_test_res_partiels = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/test_prediction_part.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v12\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir = r'G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set'\n",
    "    filename_trained = r\"..\\Pred_test_v12_500_2803\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v12_500_2803\\split_training-pc-v12.csv'\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v12_500_2803\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "    test_path_dir = r'G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set'\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_init_file = r\"..\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\"..\\Pred_test_v12_500_2803\\test_prediction.csv\"\n",
    "    filename_test_res_partiels = r'..\\Pred_test_v12_500_2803\\test_prediction_part.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v12\": #PC2\n",
    "    split_char = '\\\\'\n",
    "\n",
    "    training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"..\\modele_v11\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\split_training-pc.csv'\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v11_23K_2603\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "    test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\"\n",
    "    test_init_file = r\"..\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\"..\\Pred_test_v11_23K_2603\\test_prediction.csv\"\n",
    "    filename_test_res_partiels = r'..\\Pred_test_v11_23K_2603\\test_prediction_part.csv'\n",
    "    filename_test_res_partiels = r'C:\\Users\\jch_m\\OneDrive\\test_prediction_part.csv'\n",
    "\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1743175724802,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "113faf73-8475-4f1c-f403-57697203de82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Pred_test_v12_500_2803\\test_prediction_part.csv\n",
      "taille : 3000\n",
      "Nombre de graphs predits: 2654\n",
      "Nombre de graphs A PREDIRE : 346\n"
     ]
    }
   ],
   "source": [
    "print(filename_test_res_partiels)\n",
    "if os.path.exists(filename_test_res_partiels):\n",
    "    df_pred_partiel = pd.read_csv(filename_test_res_partiels, sep=\",\")\n",
    "    print(\"taille :\",len(df_pred_partiel))\n",
    "    if len(df_pred_partiel)>0 :\n",
    "      nan_count = df_pred_partiel['64bits'].isna().sum()\n",
    "      print(f\"Nombre de graphs predits: {len(df_pred_partiel) - nan_count}\")\n",
    "      print(f\"Nombre de graphs A PREDIRE : {nan_count}\")\n",
    "      df_pred_partiel.tail()\n",
    "    else:\n",
    "      nan_count = df_pred_partiel['64bits'].isna().sum()\n",
    "      print(f\"Nombre de graphs predits: 0\")\n",
    "      print(f\"Nombre de graphs A PREDIRE : tous\")\n",
    "else:\n",
    "    print(\"pas predictions partielles trouvées, pas de fichier\", filename_test_res_partiels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9WRoMQnVDJE"
   },
   "source": [
    "# Init MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1743175733643,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "class HierarchicalAssemblyTokenizer:\n",
    "    def __init__(self):\n",
    "        # Tokenizer implementation from previous code\n",
    "        # Abbreviated for brevity but would be the full implementation\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "    def fit(self, graph_l, min_freq=1):\n",
    "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        self.unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        # Extract features from node labels\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                # Extract node type (JCC, INST)\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                # Extract operation (xor, push, mov)\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                # Extract registers\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                # Extract memory reference patterns\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                # Extract immediate values\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            self.unique_hashes.append(item['name'])\n",
    "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "            for i, hash_val in enumerate(self.unique_hashes):\n",
    "                self.hash_to_id[hash_val] = i\n",
    "                self.id_to_hash[i] = hash_val\n",
    "\n",
    "        # Build vocabularies with frequency filtering\n",
    "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
    "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
    "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
    "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_from_counts(self, all_counts, min_freq=3):\n",
    "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
    "        self.node_type_vocab.clear()\n",
    "        self.operation_vocab.clear()\n",
    "        self.register_vocab.clear()\n",
    "        self.memory_pattern_vocab.clear()\n",
    "        self.immediate_vocab.clear()\n",
    "\n",
    "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
    "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
    "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
    "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
    "\n",
    "        self.unique_hashes = all_counts['unique_hashes']\n",
    "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "        for i, hash_val in enumerate(self.unique_hashes):\n",
    "            self.hash_to_id[hash_val] = i\n",
    "            self.id_to_hash[i] = hash_val\n",
    "\n",
    "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
    "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
    "        counter = Counter(tokens)\n",
    "        # Add special UNK token\n",
    "        vocab_dict[self.UNK_TOKEN] = 0\n",
    "\n",
    "        # Add tokens that meet minimum frequency\n",
    "        idx = 1\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                vocab_dict[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def _memory_to_pattern(self, mem_ref):\n",
    "        \"\"\"Convert memory reference to pattern\"\"\"\n",
    "        # Replace registers with REG placeholder\n",
    "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
    "        # Replace immediate values with IMM placeholder\n",
    "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
    "        return pattern.strip()\n",
    "\n",
    "    def tokenize(self, node_label):\n",
    "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
    "        features = {\n",
    "            'node_type': self.UNK_TOKEN,\n",
    "            'operation': self.UNK_TOKEN,\n",
    "            'registers': [],\n",
    "            'memory_pattern': self.UNK_TOKEN,\n",
    "            'immediate': self.UNK_TOKEN\n",
    "        }\n",
    "\n",
    "        # Extract node type\n",
    "        node_type_match = self.patterns['node_type'].search(node_label)\n",
    "        if node_type_match:\n",
    "            node_type = node_type_match.group(1)\n",
    "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract operation\n",
    "        op_match = self.patterns['operation'].search(node_label)\n",
    "        if op_match:\n",
    "            operation = op_match.group(1)\n",
    "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract registers\n",
    "        reg_matches = self.patterns['registers'].findall(node_label)\n",
    "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
    "\n",
    "        # Extract memory reference\n",
    "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
    "        if mem_matches:\n",
    "            pattern = self._memory_to_pattern(mem_matches[0])\n",
    "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract immediate values\n",
    "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
    "        if imm_matches:\n",
    "            imm = imm_matches[0]\n",
    "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
    "\n",
    "        return features\n",
    "\n",
    "    def encode_nodelabel(self, node_label):\n",
    "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
    "        features = self.tokenize(node_label)\n",
    "\n",
    "        # Encode each feature\n",
    "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
    "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
    "\n",
    "        # Encode registers (take up to 3, pad if fewer)\n",
    "        register_indices = []\n",
    "        for i in range(min(3, len(features['registers']))):\n",
    "            reg = features['registers'][i]\n",
    "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
    "            register_indices.append(reg_idx)\n",
    "\n",
    "        # Pad register indices if needed\n",
    "        while len(register_indices) < 3:\n",
    "            register_indices.append(0)  # 0 for padding\n",
    "\n",
    "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
    "            features['memory_pattern'],\n",
    "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        immediate_idx = self.immediate_vocab.get(\n",
    "            features['immediate'],\n",
    "            self.immediate_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Combine all indices into a feature vector\n",
    "        encoded = np.array([\n",
    "            node_type_idx,\n",
    "            operation_idx,\n",
    "            register_indices[0],\n",
    "            register_indices[1],\n",
    "            register_indices[2],\n",
    "            memory_pattern_idx,\n",
    "            immediate_idx\n",
    "        ], dtype=np.int64)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_graph(self, digraph):\n",
    "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
    "        node_features = {}\n",
    "\n",
    "        for node_id in digraph.nodes():\n",
    "            label = digraph.nodes[node_id].get('label', '')\n",
    "            node_features[node_id] = self.encode_nodelabel(label)\n",
    "\n",
    "        return node_features\n",
    "\n",
    "    def encode_hash(self, hash_file):\n",
    "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return the size of each vocabulary\"\"\"\n",
    "        return {\n",
    "            'node_type': len(self.node_type_vocab),\n",
    "            'operation': len(self.operation_vocab),\n",
    "            'register': len(self.register_vocab),\n",
    "            'memory_pattern': len(self.memory_pattern_vocab),\n",
    "            'immediate': len(self.immediate_vocab)\n",
    "        }\n",
    "\n",
    "    def get_tokens_and_counts(self, graph_l):\n",
    "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            unique_hashes.append(item['name'])\n",
    "\n",
    "        return {\n",
    "            'node_types': node_types,\n",
    "            'operations': operations,\n",
    "            'registers': registers,\n",
    "            'memory_patterns': memory_patterns,\n",
    "            'immediates': immediates,\n",
    "            'unique_hashes': unique_hashes,\n",
    "        }\n",
    "\n",
    "class AssemblyGraphDataset:\n",
    "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
    "        \"\"\"\n",
    "        Prepares a dataset of assembly graphs for GAT model training\n",
    "        Args:\n",
    "            graph_list: List of NetworkX digraphs representing assembly code\n",
    "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
    "            node_feature_dim: Dimension of node embeddings\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        # Initialize embedding layers for each feature type\n",
    "        num_feature_type = 5\n",
    "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
    "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
    "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
    "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
    "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
    "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
    "\n",
    "        # Process graphs into PyTorch Geometric Data objects\n",
    "        self.data_list = []\n",
    "        for graph in self.graph_list:\n",
    "            # process graph info\n",
    "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
    "\n",
    "    def _process_graph(self, graph, hash_name):\n",
    "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
    "        # Create a node ID mapping for consecutive IDs\n",
    "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "        # Get node features\n",
    "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
    "\n",
    "        # Convert to tensor-friendly format\n",
    "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
    "        for node_id, features in node_features_dict.items():\n",
    "            x[node_mapping[node_id]] = torch.tensor(features)\n",
    "\n",
    "        # Create edge index\n",
    "        edge_list = list(graph.edges())\n",
    "        if not edge_list:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
    "\n",
    "        # Endcode Hash name\n",
    "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
    "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
    "\n",
    "        # Get embeddings before creating Data object\n",
    "        x = self.get_embeddings(x)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
    "        return data\n",
    "\n",
    "    def optimize_edge_index(self, edge_list, node_mapping):\n",
    "        \"\"\"\n",
    "        Optimizes the creation of edge_index tensor for graph representation.\n",
    "        Args:\n",
    "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
    "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Optimized edge_index tensor.\n",
    "        \"\"\"\n",
    "        if not edge_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "        num_edges = len(edge_list)\n",
    "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "\n",
    "        for i, (src, tgt) in enumerate(edge_list):\n",
    "            src_indices[i] = node_mapping[src]\n",
    "            tgt_indices[i] = node_mapping[tgt]\n",
    "\n",
    "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
    "        # Split features into their components\n",
    "        node_type_idx = x[:, 0]\n",
    "        operation_idx = x[:, 1]\n",
    "        register_idx1 = x[:, 2]\n",
    "        register_idx2 = x[:, 3]\n",
    "        register_idx3 = x[:, 4]\n",
    "        memory_pattern_idx = x[:, 5]\n",
    "        immediate_idx = x[:, 6]\n",
    "\n",
    "        # Get embeddings for each component\n",
    "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
    "        operation_emb = self.operation_embedding(operation_idx)\n",
    "\n",
    "        # Combine register embeddings (average them)\n",
    "        register_emb = (self.register_embedding(register_idx1) +\n",
    "                        self.register_embedding(register_idx2) +\n",
    "                        self.register_embedding(register_idx3)) / 3\n",
    "\n",
    "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
    "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        return torch.cat([\n",
    "            node_type_emb,\n",
    "            operation_emb,\n",
    "            register_emb,\n",
    "            memory_pattern_emb,\n",
    "            immediate_emb\n",
    "        ], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Graph Attention Network for assembly code analysis\n",
    "Args:\n",
    "    dataset: AssemblyGraphDataset instance\n",
    "    hidden_dim: Hidden dimension of GAT layers\n",
    "    output_dim: Output dimension of node embeddings\n",
    "    heads: Number of attention heads\n",
    "    dropout: Dropout rate\n",
    "\"\"\"\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, x, edge_index, hash_encoded, batch):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                if os.path.exists(full_path_file):\n",
    "                    file_size = os.path.getsize(full_path_file)\n",
    "                    if file_size/1000 > 200_000: #file too big\n",
    "                        print(\"size issue : skip this file test\", file_size/1000, full_path_file)\n",
    "                        continue\n",
    "\n",
    "                digraph_str = f.read()\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append({\n",
    "                    'name': hash_file,\n",
    "                    'graph_input' : G\n",
    "                })\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
    "    if os.path.exists(model_save_file):\n",
    "      if torch.cuda.is_available():\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "      else:\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        count_batch = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # get the different components of the dataset\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
    "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # save model trained\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
    "    if os.path.exists(model_save_file):\n",
    "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "\n",
    "    model.eval() #set the model to evaluation mode.\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "    hash_v = []\n",
    "\n",
    "    count_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_label.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #hash_v.append(list_hash_batch)\n",
    "\n",
    "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "\n",
    "    return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) +1\n",
    "\n",
    "    hash_v = []\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    all_counts = {\n",
    "        'node_types': [],\n",
    "        'operations': [],\n",
    "        'registers': [],\n",
    "        'memory_patterns': [],\n",
    "        'immediates': [],\n",
    "        'unique_hashes': [],\n",
    "    }\n",
    "    print(\"**** Tokenization and Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        batch_num = i // batch_files_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if not batch_graph_list:\n",
    "            print(\"graph list is empty for this batch\")\n",
    "            continue\n",
    "\n",
    "        # Tokenization for the current batch\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list)\n",
    "        for key in all_counts:\n",
    "            if key in tokens:\n",
    "                all_counts[key].extend(tokens[key])\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "        # Create Dataset and DataLoader for the current batch\n",
    "        if tokenizer and num_feature_dim is not None:\n",
    "            dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
    "            curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
    "            list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
    "\n",
    "            # Training or other processing for the current batch\n",
    "            if mode == 'train' and train_model:\n",
    "                print(f\"**** Training on batch {batch_num}/{nb_batch}\")\n",
    "                train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=epochs)\n",
    "                hash_t.extend(list_of_hash)\n",
    "            # Add logic for other modes (e.g., 'eval') here if needed\n",
    "            else:\n",
    "                pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
    "                all_true_label.append(true_labels.cpu())\n",
    "                all_pred_label.append(pred_labels.cpu())\n",
    "                hash_v.append(list_of_hash)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Final fitting of the tokenizer after processing all batches\n",
    "    if all_counts:\n",
    "        print(\"**** Finalizing Tokenizer Vocabulary\")\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "    #print(\"**** Processing Complete\")\n",
    "    # save all labels as tensor to return if mode= 'validation'\n",
    "    if mode == 'validation':\n",
    "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "\n",
    "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "    elif mode == 'train':\n",
    "        return hash_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OgAOmbMvs7W"
   },
   "source": [
    "# MODE TEST ONLY\n",
    "\n",
    "Generate prediction based on the requested list of hash (no true labels available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1743175184600,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "mvoa8Jcyvs7X",
    "outputId": "20459e54-5df0-47f8-de26-9667a2943c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test init: 453 labels to predict for 3000 graphs \n",
      "from files located: G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_init = pd.read_csv(test_init_file, sep=\",\")\n",
    "col_to_predict = df_test_init.columns\n",
    "df_test_init.head()\n",
    "print(f\"Test init: {df_test_init.shape[1]-1} labels to predict for {df_test_init.shape[0]} graphs \\nfrom files located: {test_path_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mts_g2jUtSu-",
    "outputId": "105fc894-9623-449f-9586-c00f939ada0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions partielles disponibles\n",
<<<<<<< HEAD
      "Nombre de graphs predits: 2654\n",
      "Nombre de graphs A PREDIRE : 346\n",
      "**** Dataset Processing for Test\n",
      "Batch 0/346.0 : files 2654 and next 1\n",
      "Batch 1/346.0 : files 2655 and next 1\n",
      "Batch 2/346.0 : files 2656 and next 1\n",
      "Batch 3/346.0 : files 2657 and next 1\n",
      "Batch 4/346.0 : files 2658 and next 1\n",
      "Batch 5/346.0 : files 2659 and next 1\n",
      "Batch 6/346.0 : files 2660 and next 1\n",
      "Batch 7/346.0 : files 2661 and next 1\n",
      "Batch 8/346.0 : files 2662 and next 1\n",
      "Batch 9/346.0 : files 2663 and next 1\n",
      "Batch 10/346.0 : files 2664 and next 1\n",
      "Batch 11/346.0 : files 2665 and next 1\n",
      "Batch 12/346.0 : files 2666 and next 1\n",
      "Batch 13/346.0 : files 2667 and next 1\n",
      "Batch 14/346.0 : files 2668 and next 1\n",
      "Batch 15/346.0 : files 2669 and next 1\n",
      "Batch 16/346.0 : files 2670 and next 1\n",
      "Batch 17/346.0 : files 2671 and next 1\n",
      "Batch 18/346.0 : files 2672 and next 1\n",
      "Batch 19/346.0 : files 2673 and next 1\n",
      "Batch 20/346.0 : files 2674 and next 1\n",
      "Batch 21/346.0 : files 2675 and next 1\n",
      "Batch 22/346.0 : files 2676 and next 1\n",
      "Batch 23/346.0 : files 2677 and next 1\n",
      "Batch 24/346.0 : files 2678 and next 1\n",
      "Batch 25/346.0 : files 2679 and next 1\n",
      "Batch 26/346.0 : files 2680 and next 1\n",
      "Batch 27/346.0 : files 2681 and next 1\n",
      "Batch 28/346.0 : files 2682 and next 1\n",
      "Batch 29/346.0 : files 2683 and next 1\n",
      "Batch 30/346.0 : files 2684 and next 1\n",
      "Batch 31/346.0 : files 2685 and next 1\n",
      "Batch 32/346.0 : files 2686 and next 1\n",
      "Batch 33/346.0 : files 2687 and next 1\n",
      "Batch 34/346.0 : files 2688 and next 1\n",
      "Batch 35/346.0 : files 2689 and next 1\n",
      "Batch 36/346.0 : files 2690 and next 1\n",
      "Batch 37/346.0 : files 2691 and next 1\n",
      "Batch 38/346.0 : files 2692 and next 1\n",
      "Batch 39/346.0 : files 2693 and next 1\n",
      "Batch 40/346.0 : files 2694 and next 1\n",
      "Batch 41/346.0 : files 2695 and next 1\n",
      "Batch 42/346.0 : files 2696 and next 1\n",
      "Batch 43/346.0 : files 2697 and next 1\n",
      "Batch 44/346.0 : files 2698 and next 1\n",
      "Batch 45/346.0 : files 2699 and next 1\n",
      "Batch 46/346.0 : files 2700 and next 1\n",
      "Batch 47/346.0 : files 2701 and next 1\n",
      "Batch 48/346.0 : files 2702 and next 1\n",
      "Batch 49/346.0 : files 2703 and next 1\n",
      "Batch 50/346.0 : files 2704 and next 1\n",
      "Batch 51/346.0 : files 2705 and next 1\n",
      "Batch 52/346.0 : files 2706 and next 1\n",
      "Batch 53/346.0 : files 2707 and next 1\n",
      "Batch 54/346.0 : files 2708 and next 1\n",
      "Batch 55/346.0 : files 2709 and next 1\n",
      "Batch 56/346.0 : files 2710 and next 1\n",
      "Batch 57/346.0 : files 2711 and next 1\n",
      "Batch 58/346.0 : files 2712 and next 1\n",
      "Batch 59/346.0 : files 2713 and next 1\n",
      "Batch 60/346.0 : files 2714 and next 1\n",
      "Batch 61/346.0 : files 2715 and next 1\n",
      "Batch 62/346.0 : files 2716 and next 1\n",
      "Batch 63/346.0 : files 2717 and next 1\n",
      "Batch 64/346.0 : files 2718 and next 1\n",
      "Batch 65/346.0 : files 2719 and next 1\n",
      "Batch 66/346.0 : files 2720 and next 1\n",
      "Batch 67/346.0 : files 2721 and next 1\n",
      "Batch 68/346.0 : files 2722 and next 1\n",
      "Batch 69/346.0 : files 2723 and next 1\n",
      "Batch 70/346.0 : files 2724 and next 1\n",
      "Batch 71/346.0 : files 2725 and next 1\n",
      "Batch 72/346.0 : files 2726 and next 1\n",
      "Batch 73/346.0 : files 2727 and next 1\n",
      "Batch 74/346.0 : files 2728 and next 1\n",
      "Batch 75/346.0 : files 2729 and next 1\n",
      "Batch 76/346.0 : files 2730 and next 1\n",
      "Batch 77/346.0 : files 2731 and next 1\n",
      "Batch 78/346.0 : files 2732 and next 1\n",
      "Batch 79/346.0 : files 2733 and next 1\n",
      "Batch 80/346.0 : files 2734 and next 1\n",
      "Batch 81/346.0 : files 2735 and next 1\n",
      "Batch 82/346.0 : files 2736 and next 1\n",
      "Batch 83/346.0 : files 2737 and next 1\n",
      "Batch 84/346.0 : files 2738 and next 1\n",
      "Batch 85/346.0 : files 2739 and next 1\n",
      "Batch 86/346.0 : files 2740 and next 1\n",
      "Batch 87/346.0 : files 2741 and next 1\n",
      "Batch 88/346.0 : files 2742 and next 1\n",
      "Batch 89/346.0 : files 2743 and next 1\n",
      "Batch 90/346.0 : files 2744 and next 1\n",
      "Batch 91/346.0 : files 2745 and next 1\n",
      "Batch 92/346.0 : files 2746 and next 1\n",
      "Batch 93/346.0 : files 2747 and next 1\n",
      "Batch 94/346.0 : files 2748 and next 1\n",
      "Batch 95/346.0 : files 2749 and next 1\n",
      "Batch 96/346.0 : files 2750 and next 1\n",
      "Batch 97/346.0 : files 2751 and next 1\n",
      "Batch 98/346.0 : files 2752 and next 1\n",
      "Batch 99/346.0 : files 2753 and next 1\n",
      "Batch 100/346.0 : files 2754 and next 1\n",
      "Batch 101/346.0 : files 2755 and next 1\n",
      "Batch 102/346.0 : files 2756 and next 1\n",
      "Batch 103/346.0 : files 2757 and next 1\n",
      "Batch 104/346.0 : files 2758 and next 1\n",
      "Batch 105/346.0 : files 2759 and next 1\n",
      "Batch 106/346.0 : files 2760 and next 1\n",
      "Batch 107/346.0 : files 2761 and next 1\n",
      "Batch 108/346.0 : files 2762 and next 1\n",
      "Batch 109/346.0 : files 2763 and next 1\n",
      "Batch 110/346.0 : files 2764 and next 1\n",
      "Batch 111/346.0 : files 2765 and next 1\n",
      "Batch 112/346.0 : files 2766 and next 1\n",
      "Batch 113/346.0 : files 2767 and next 1\n",
      "Batch 114/346.0 : files 2768 and next 1\n",
      "Batch 115/346.0 : files 2769 and next 1\n",
      "Batch 116/346.0 : files 2770 and next 1\n",
      "Batch 117/346.0 : files 2771 and next 1\n",
      "Batch 118/346.0 : files 2772 and next 1\n",
      "Batch 119/346.0 : files 2773 and next 1\n",
      "Batch 120/346.0 : files 2774 and next 1\n",
      "Batch 121/346.0 : files 2775 and next 1\n",
      "Batch 122/346.0 : files 2776 and next 1\n",
      "Batch 123/346.0 : files 2777 and next 1\n",
      "Batch 124/346.0 : files 2778 and next 1\n",
      "Batch 125/346.0 : files 2779 and next 1\n",
      "Batch 126/346.0 : files 2780 and next 1\n",
      "Batch 127/346.0 : files 2781 and next 1\n",
      "Batch 128/346.0 : files 2782 and next 1\n",
      "Batch 129/346.0 : files 2783 and next 1\n",
      "Batch 130/346.0 : files 2784 and next 1\n",
      "Batch 131/346.0 : files 2785 and next 1\n",
      "Batch 132/346.0 : files 2786 and next 1\n",
      "Batch 133/346.0 : files 2787 and next 1\n",
      "Batch 134/346.0 : files 2788 and next 1\n",
      "Batch 135/346.0 : files 2789 and next 1\n",
      "Batch 136/346.0 : files 2790 and next 1\n",
      "Batch 137/346.0 : files 2791 and next 1\n",
      "Batch 138/346.0 : files 2792 and next 1\n",
      "Batch 139/346.0 : files 2793 and next 1\n",
      "Batch 140/346.0 : files 2794 and next 1\n",
      "Batch 141/346.0 : files 2795 and next 1\n",
      "Batch 142/346.0 : files 2796 and next 1\n",
      "Batch 143/346.0 : files 2797 and next 1\n",
      "Batch 144/346.0 : files 2798 and next 1\n",
      "Batch 145/346.0 : files 2799 and next 1\n",
      "Batch 146/346.0 : files 2800 and next 1\n",
      "Batch 147/346.0 : files 2801 and next 1\n",
      "Batch 148/346.0 : files 2802 and next 1\n",
      "Batch 149/346.0 : files 2803 and next 1\n",
      "Batch 150/346.0 : files 2804 and next 1\n",
      "Batch 151/346.0 : files 2805 and next 1\n",
      "Batch 152/346.0 : files 2806 and next 1\n",
      "Batch 153/346.0 : files 2807 and next 1\n",
      "Batch 154/346.0 : files 2808 and next 1\n",
      "Batch 155/346.0 : files 2809 and next 1\n",
      "Batch 156/346.0 : files 2810 and next 1\n",
      "Batch 157/346.0 : files 2811 and next 1\n",
      "Batch 158/346.0 : files 2812 and next 1\n",
      "Batch 159/346.0 : files 2813 and next 1\n",
      "Batch 160/346.0 : files 2814 and next 1\n",
      "Batch 161/346.0 : files 2815 and next 1\n",
      "Batch 162/346.0 : files 2816 and next 1\n",
      "Batch 163/346.0 : files 2817 and next 1\n",
      "Batch 164/346.0 : files 2818 and next 1\n",
      "Batch 165/346.0 : files 2819 and next 1\n",
      "Batch 166/346.0 : files 2820 and next 1\n",
      "Batch 167/346.0 : files 2821 and next 1\n",
      "Batch 168/346.0 : files 2822 and next 1\n",
      "Batch 169/346.0 : files 2823 and next 1\n",
      "Batch 170/346.0 : files 2824 and next 1\n",
      "Batch 171/346.0 : files 2825 and next 1\n",
      "Batch 172/346.0 : files 2826 and next 1\n",
      "Batch 173/346.0 : files 2827 and next 1\n",
      "Batch 174/346.0 : files 2828 and next 1\n",
      "Batch 175/346.0 : files 2829 and next 1\n",
      "Batch 176/346.0 : files 2830 and next 1\n",
      "Batch 177/346.0 : files 2831 and next 1\n",
      "Batch 178/346.0 : files 2832 and next 1\n",
      "Batch 179/346.0 : files 2833 and next 1\n",
      "Batch 180/346.0 : files 2834 and next 1\n",
      "Batch 181/346.0 : files 2835 and next 1\n",
      "Batch 182/346.0 : files 2836 and next 1\n",
      "Batch 183/346.0 : files 2837 and next 1\n",
      "Batch 184/346.0 : files 2838 and next 1\n",
      "Batch 185/346.0 : files 2839 and next 1\n",
      "Batch 186/346.0 : files 2840 and next 1\n",
      "Batch 187/346.0 : files 2841 and next 1\n",
      "Batch 188/346.0 : files 2842 and next 1\n",
      "Batch 189/346.0 : files 2843 and next 1\n",
      "Batch 190/346.0 : files 2844 and next 1\n",
      "Batch 191/346.0 : files 2845 and next 1\n",
      "Batch 192/346.0 : files 2846 and next 1\n",
      "Batch 193/346.0 : files 2847 and next 1\n",
      "Batch 194/346.0 : files 2848 and next 1\n",
      "Batch 195/346.0 : files 2849 and next 1\n",
      "Batch 196/346.0 : files 2850 and next 1\n",
      "Batch 197/346.0 : files 2851 and next 1\n",
      "Batch 198/346.0 : files 2852 and next 1\n",
      "Batch 199/346.0 : files 2853 and next 1\n",
      "Batch 200/346.0 : files 2854 and next 1\n",
      "Batch 201/346.0 : files 2855 and next 1\n",
      "Batch 202/346.0 : files 2856 and next 1\n",
      "Batch 203/346.0 : files 2857 and next 1\n",
      "Batch 204/346.0 : files 2858 and next 1\n",
      "Batch 205/346.0 : files 2859 and next 1\n",
      "Batch 206/346.0 : files 2860 and next 1\n",
      "Batch 207/346.0 : files 2861 and next 1\n",
      "Batch 208/346.0 : files 2862 and next 1\n",
      "Batch 209/346.0 : files 2863 and next 1\n",
      "Batch 210/346.0 : files 2864 and next 1\n",
      "Batch 211/346.0 : files 2865 and next 1\n",
      "Batch 212/346.0 : files 2866 and next 1\n",
      "Batch 213/346.0 : files 2867 and next 1\n",
      "Batch 214/346.0 : files 2868 and next 1\n",
      "Batch 215/346.0 : files 2869 and next 1\n",
      "Batch 216/346.0 : files 2870 and next 1\n",
      "Batch 217/346.0 : files 2871 and next 1\n",
      "Batch 218/346.0 : files 2872 and next 1\n",
      "Batch 219/346.0 : files 2873 and next 1\n",
      "Batch 220/346.0 : files 2874 and next 1\n",
      "Batch 221/346.0 : files 2875 and next 1\n",
      "Batch 222/346.0 : files 2876 and next 1\n",
      "Batch 223/346.0 : files 2877 and next 1\n",
      "Batch 224/346.0 : files 2878 and next 1\n",
      "Batch 225/346.0 : files 2879 and next 1\n",
      "Batch 226/346.0 : files 2880 and next 1\n",
      "Batch 227/346.0 : files 2881 and next 1\n",
      "Batch 228/346.0 : files 2882 and next 1\n",
      "Batch 229/346.0 : files 2883 and next 1\n",
      "Batch 230/346.0 : files 2884 and next 1\n",
      "Batch 231/346.0 : files 2885 and next 1\n",
      "Batch 232/346.0 : files 2886 and next 1\n",
      "Batch 233/346.0 : files 2887 and next 1\n",
      "Batch 234/346.0 : files 2888 and next 1\n",
      "Batch 235/346.0 : files 2889 and next 1\n",
      "Batch 236/346.0 : files 2890 and next 1\n",
      "Batch 237/346.0 : files 2891 and next 1\n",
      "Batch 238/346.0 : files 2892 and next 1\n",
      "Batch 239/346.0 : files 2893 and next 1\n",
      "Batch 240/346.0 : files 2894 and next 1\n",
      "Batch 241/346.0 : files 2895 and next 1\n",
      "Batch 242/346.0 : files 2896 and next 1\n",
      "Batch 243/346.0 : files 2897 and next 1\n",
      "Batch 244/346.0 : files 2898 and next 1\n",
      "Batch 245/346.0 : files 2899 and next 1\n",
      "Batch 246/346.0 : files 2900 and next 1\n",
      "Batch 247/346.0 : files 2901 and next 1\n",
      "Batch 248/346.0 : files 2902 and next 1\n",
      "Batch 249/346.0 : files 2903 and next 1\n",
      "Batch 250/346.0 : files 2904 and next 1\n",
      "Batch 251/346.0 : files 2905 and next 1\n",
      "Batch 252/346.0 : files 2906 and next 1\n",
      "Batch 253/346.0 : files 2907 and next 1\n",
      "Batch 254/346.0 : files 2908 and next 1\n",
      "Batch 255/346.0 : files 2909 and next 1\n",
      "Batch 256/346.0 : files 2910 and next 1\n",
      "Batch 257/346.0 : files 2911 and next 1\n",
      "Batch 258/346.0 : files 2912 and next 1\n",
      "Batch 259/346.0 : files 2913 and next 1\n",
      "Batch 260/346.0 : files 2914 and next 1\n",
      "Batch 261/346.0 : files 2915 and next 1\n",
      "Batch 262/346.0 : files 2916 and next 1\n",
      "Batch 263/346.0 : files 2917 and next 1\n",
      "Batch 264/346.0 : files 2918 and next 1\n",
      "Batch 265/346.0 : files 2919 and next 1\n",
      "Batch 266/346.0 : files 2920 and next 1\n",
      "Batch 267/346.0 : files 2921 and next 1\n",
      "Batch 268/346.0 : files 2922 and next 1\n",
      "Batch 269/346.0 : files 2923 and next 1\n",
      "Batch 270/346.0 : files 2924 and next 1\n",
      "Batch 271/346.0 : files 2925 and next 1\n",
      "Batch 272/346.0 : files 2926 and next 1\n",
      "Batch 273/346.0 : files 2927 and next 1\n",
      "Batch 274/346.0 : files 2928 and next 1\n",
      "Batch 275/346.0 : files 2929 and next 1\n",
      "Batch 276/346.0 : files 2930 and next 1\n",
      "Batch 277/346.0 : files 2931 and next 1\n",
      "Batch 278/346.0 : files 2932 and next 1\n",
      "Batch 279/346.0 : files 2933 and next 1\n",
      "Batch 280/346.0 : files 2934 and next 1\n",
      "Batch 281/346.0 : files 2935 and next 1\n",
      "Batch 282/346.0 : files 2936 and next 1\n",
      "Batch 283/346.0 : files 2937 and next 1\n",
      "Batch 284/346.0 : files 2938 and next 1\n",
      "Batch 285/346.0 : files 2939 and next 1\n",
      "Batch 286/346.0 : files 2940 and next 1\n",
      "Batch 287/346.0 : files 2941 and next 1\n",
      "Batch 288/346.0 : files 2942 and next 1\n",
      "Batch 289/346.0 : files 2943 and next 1\n",
      "Batch 290/346.0 : files 2944 and next 1\n",
      "Batch 291/346.0 : files 2945 and next 1\n",
      "Batch 292/346.0 : files 2946 and next 1\n",
      "Batch 293/346.0 : files 2947 and next 1\n",
      "Batch 294/346.0 : files 2948 and next 1\n",
      "Batch 295/346.0 : files 2949 and next 1\n",
      "Batch 296/346.0 : files 2950 and next 1\n",
      "Batch 297/346.0 : files 2951 and next 1\n",
      "Batch 298/346.0 : files 2952 and next 1\n",
      "Batch 299/346.0 : files 2953 and next 1\n",
      "Batch 300/346.0 : files 2954 and next 1\n",
      "Batch 301/346.0 : files 2955 and next 1\n",
      "Batch 302/346.0 : files 2956 and next 1\n",
      "Batch 303/346.0 : files 2957 and next 1\n",
      "Batch 304/346.0 : files 2958 and next 1\n",
      "Batch 305/346.0 : files 2959 and next 1\n",
      "Batch 306/346.0 : files 2960 and next 1\n",
      "Batch 307/346.0 : files 2961 and next 1\n",
      "Batch 308/346.0 : files 2962 and next 1\n",
      "Batch 309/346.0 : files 2963 and next 1\n",
      "Batch 310/346.0 : files 2964 and next 1\n",
      "Batch 311/346.0 : files 2965 and next 1\n",
      "Batch 312/346.0 : files 2966 and next 1\n",
      "Batch 313/346.0 : files 2967 and next 1\n",
      "Batch 314/346.0 : files 2968 and next 1\n",
      "Batch 315/346.0 : files 2969 and next 1\n",
      "Batch 316/346.0 : files 2970 and next 1\n",
      "Batch 317/346.0 : files 2971 and next 1\n",
      "Batch 318/346.0 : files 2972 and next 1\n",
      "Batch 319/346.0 : files 2973 and next 1\n",
      "Batch 320/346.0 : files 2974 and next 1\n",
      "Batch 321/346.0 : files 2975 and next 1\n",
      "Batch 322/346.0 : files 2976 and next 1\n",
      "Batch 323/346.0 : files 2977 and next 1\n",
      "Batch 324/346.0 : files 2978 and next 1\n",
      "Batch 325/346.0 : files 2979 and next 1\n",
      "Batch 326/346.0 : files 2980 and next 1\n",
      "Batch 327/346.0 : files 2981 and next 1\n",
      "Batch 328/346.0 : files 2982 and next 1\n",
      "Batch 329/346.0 : files 2983 and next 1\n",
      "Batch 330/346.0 : files 2984 and next 1\n",
      "Batch 331/346.0 : files 2985 and next 1\n",
      "Batch 332/346.0 : files 2986 and next 1\n",
      "Batch 333/346.0 : files 2987 and next 1\n",
      "Batch 334/346.0 : files 2988 and next 1\n",
      "Batch 335/346.0 : files 2989 and next 1\n",
      "Batch 336/346.0 : files 2990 and next 1\n",
      "Batch 337/346.0 : files 2991 and next 1\n",
      "Batch 338/346.0 : files 2992 and next 1\n",
      "Batch 339/346.0 : files 2993 and next 1\n",
      "Batch 340/346.0 : files 2994 and next 1\n",
      "Batch 341/346.0 : files 2995 and next 1\n",
      "Batch 342/346.0 : files 2996 and next 1\n",
      "Batch 343/346.0 : files 2997 and next 1\n",
      "Batch 344/346.0 : files 2998 and next 1\n",
      "Batch 345/346.0 : files 2999 and next 1\n",
      "Test end: nb pred labels: 3000 \n"
=======
      "Nombre de graphs predits: 367\n",
      "Nombre de graphs A PREDIRE : 2633\n",
      "**** Dataset Processing for Test\n",
      "Batch 0 : files 367 and next 2\n",
      "Batch 1 : files 369 and next 2\n",
      "Batch 2 : files 371 and next 2\n",
      "Batch 3 : files 373 and next 2\n",
      "Batch 4 : files 375 and next 2\n",
      "Batch 5 : files 377 and next 2\n",
      "Batch 6 : files 379 and next 2\n",
      "Batch 7 : files 381 and next 2\n",
      "Batch 8 : files 383 and next 2\n",
      "Batch 9 : files 385 and next 2\n",
      "Batch 10 : files 387 and next 2\n",
      "Batch 11 : files 389 and next 2\n",
      "Batch 12 : files 391 and next 2\n",
      "Batch 13 : files 393 and next 2\n",
      "Batch 14 : files 395 and next 2\n",
      "Batch 15 : files 397 and next 2\n",
      "Batch 16 : files 399 and next 2\n",
      "Batch 17 : files 401 and next 2\n",
      "Batch 18 : files 403 and next 2\n",
      "Batch 19 : files 405 and next 2\n",
      "Batch 20 : files 407 and next 2\n",
      "Batch 21 : files 409 and next 2\n",
      "Batch 22 : files 411 and next 2\n",
      "Batch 23 : files 413 and next 2\n",
      "Batch 24 : files 415 and next 2\n",
      "Batch 25 : files 417 and next 2\n",
      "Batch 26 : files 419 and next 2\n",
      "Batch 27 : files 421 and next 2\n",
      "Batch 28 : files 423 and next 2\n",
      "Batch 29 : files 425 and next 2\n",
      "Batch 30 : files 427 and next 2\n",
      "Batch 31 : files 429 and next 2\n",
      "Batch 32 : files 431 and next 2\n",
      "Batch 33 : files 433 and next 2\n",
      "Batch 34 : files 435 and next 2\n",
      "Batch 35 : files 437 and next 2\n",
      "Batch 36 : files 439 and next 2\n",
      "Batch 37 : files 441 and next 2\n",
      "Batch 38 : files 443 and next 2\n",
      "Batch 39 : files 445 and next 2\n",
      "Batch 40 : files 447 and next 2\n",
      "Batch 41 : files 449 and next 2\n",
      "Batch 42 : files 451 and next 2\n",
      "Batch 43 : files 453 and next 2\n",
      "Batch 44 : files 455 and next 2\n",
      "Batch 45 : files 457 and next 2\n",
      "Batch 46 : files 459 and next 2\n",
      "Batch 47 : files 461 and next 2\n",
      "Batch 48 : files 463 and next 2\n",
      "Batch 49 : files 465 and next 2\n",
      "Batch 50 : files 467 and next 2\n",
      "Batch 51 : files 469 and next 2\n",
      "Batch 52 : files 471 and next 2\n",
      "Batch 53 : files 473 and next 2\n",
      "Batch 54 : files 475 and next 2\n",
      "Batch 55 : files 477 and next 2\n",
      "Batch 56 : files 479 and next 2\n",
      "Batch 57 : files 481 and next 2\n",
      "Batch 58 : files 483 and next 2\n",
      "Batch 59 : files 485 and next 2\n",
      "Batch 60 : files 487 and next 2\n",
      "Batch 61 : files 489 and next 2\n",
      "Batch 62 : files 491 and next 2\n",
      "Batch 63 : files 493 and next 2\n",
      "Batch 64 : files 495 and next 2\n",
      "Batch 65 : files 497 and next 2\n",
      "Batch 66 : files 499 and next 2\n",
      "Batch 67 : files 501 and next 2\n",
      "Batch 68 : files 503 and next 2\n",
      "Batch 69 : files 505 and next 2\n",
      "Batch 70 : files 507 and next 2\n",
      "Batch 71 : files 509 and next 2\n",
      "Batch 72 : files 511 and next 2\n",
      "Batch 73 : files 513 and next 2\n",
      "Batch 74 : files 515 and next 2\n",
      "Batch 75 : files 517 and next 2\n",
      "Batch 76 : files 519 and next 2\n",
      "Batch 77 : files 521 and next 2\n",
      "Batch 78 : files 523 and next 2\n",
      "Batch 79 : files 525 and next 2\n",
      "Batch 80 : files 527 and next 2\n",
      "Batch 81 : files 529 and next 2\n",
      "Batch 82 : files 531 and next 2\n",
      "Batch 83 : files 533 and next 2\n",
      "Batch 84 : files 535 and next 2\n",
      "Batch 85 : files 537 and next 2\n",
      "Batch 86 : files 539 and next 2\n",
      "Batch 87 : files 541 and next 2\n",
      "Batch 88 : files 543 and next 2\n",
      "Batch 89 : files 545 and next 2\n",
      "Batch 90 : files 547 and next 2\n",
      "Batch 91 : files 549 and next 2\n",
      "Batch 92 : files 551 and next 2\n",
      "Batch 93 : files 553 and next 2\n",
      "Batch 94 : files 555 and next 2\n",
      "Batch 95 : files 557 and next 2\n",
      "Batch 96 : files 559 and next 2\n",
      "Batch 97 : files 561 and next 2\n",
      "Batch 98 : files 563 and next 2\n",
      "Batch 99 : files 565 and next 2\n",
      "Batch 100 : files 567 and next 2\n",
      "Batch 101 : files 569 and next 2\n",
      "Batch 102 : files 571 and next 2\n",
      "Batch 103 : files 573 and next 2\n",
      "Batch 104 : files 575 and next 2\n",
      "Batch 105 : files 577 and next 2\n",
      "Batch 106 : files 579 and next 2\n",
      "Batch 107 : files 581 and next 2\n",
      "Batch 108 : files 583 and next 2\n",
      "Batch 109 : files 585 and next 2\n",
      "Batch 110 : files 587 and next 2\n",
      "Batch 111 : files 589 and next 2\n",
      "Batch 112 : files 591 and next 2\n",
      "Batch 113 : files 593 and next 2\n",
      "Batch 114 : files 595 and next 2\n",
      "Batch 115 : files 597 and next 2\n",
      "Batch 116 : files 599 and next 2\n",
      "Batch 117 : files 601 and next 2\n",
      "Batch 118 : files 603 and next 2\n",
      "Batch 119 : files 605 and next 2\n",
      "Batch 120 : files 607 and next 2\n",
      "Batch 121 : files 609 and next 2\n",
      "Batch 122 : files 611 and next 2\n",
      "Batch 123 : files 613 and next 2\n",
      "Batch 124 : files 615 and next 2\n",
      "Batch 125 : files 617 and next 2\n",
      "Batch 126 : files 619 and next 2\n",
      "Batch 127 : files 621 and next 2\n",
      "Batch 128 : files 623 and next 2\n",
      "Batch 129 : files 625 and next 2\n",
      "Batch 130 : files 627 and next 2\n",
      "Batch 131 : files 629 and next 2\n",
      "Batch 132 : files 631 and next 2\n",
      "Batch 133 : files 633 and next 2\n",
      "Batch 134 : files 635 and next 2\n",
      "Batch 135 : files 637 and next 2\n",
      "Batch 136 : files 639 and next 2\n",
      "Batch 137 : files 641 and next 2\n",
      "Batch 138 : files 643 and next 2\n",
      "Batch 139 : files 645 and next 2\n",
      "Batch 140 : files 647 and next 2\n",
      "Batch 141 : files 649 and next 2\n",
      "Batch 142 : files 651 and next 2\n",
      "Batch 143 : files 653 and next 2\n",
      "Batch 144 : files 655 and next 2\n",
      "Batch 145 : files 657 and next 2\n",
      "Batch 146 : files 659 and next 2\n",
      "Batch 147 : files 661 and next 2\n",
      "Batch 148 : files 663 and next 2\n",
      "Batch 149 : files 665 and next 2\n",
      "Batch 150 : files 667 and next 2\n",
      "Batch 151 : files 669 and next 2\n",
      "Batch 152 : files 671 and next 2\n",
      "Batch 153 : files 673 and next 2\n",
      "Batch 154 : files 675 and next 2\n",
      "Batch 155 : files 677 and next 2\n",
      "Batch 156 : files 679 and next 2\n",
      "Batch 157 : files 681 and next 2\n",
      "Batch 158 : files 683 and next 2\n",
      "Batch 159 : files 685 and next 2\n",
      "Batch 160 : files 687 and next 2\n",
      "Batch 161 : files 689 and next 2\n",
      "Batch 162 : files 691 and next 2\n",
      "Batch 163 : files 693 and next 2\n",
      "Batch 164 : files 695 and next 2\n",
      "Batch 165 : files 697 and next 2\n",
      "Batch 166 : files 699 and next 2\n",
      "Batch 167 : files 701 and next 2\n",
      "Batch 168 : files 703 and next 2\n",
      "Batch 169 : files 705 and next 2\n",
      "Batch 170 : files 707 and next 2\n",
      "Batch 171 : files 709 and next 2\n",
      "Batch 172 : files 711 and next 2\n",
      "Batch 173 : files 713 and next 2\n",
      "Batch 174 : files 715 and next 2\n",
      "Batch 175 : files 717 and next 2\n",
      "Batch 176 : files 719 and next 2\n",
      "Batch 177 : files 721 and next 2\n",
      "Batch 178 : files 723 and next 2\n",
      "Batch 179 : files 725 and next 2\n",
      "Batch 180 : files 727 and next 2\n",
      "Batch 181 : files 729 and next 2\n",
      "Batch 182 : files 731 and next 2\n",
      "Batch 183 : files 733 and next 2\n",
      "Batch 184 : files 735 and next 2\n",
      "Batch 185 : files 737 and next 2\n",
      "Batch 186 : files 739 and next 2\n",
      "Batch 187 : files 741 and next 2\n",
      "Batch 188 : files 743 and next 2\n",
      "Batch 189 : files 745 and next 2\n",
      "Batch 190 : files 747 and next 2\n",
      "Batch 191 : files 749 and next 2\n",
      "Batch 192 : files 751 and next 2\n",
      "Batch 193 : files 753 and next 2\n",
      "Batch 194 : files 755 and next 2\n",
      "Batch 195 : files 757 and next 2\n",
      "Batch 196 : files 759 and next 2\n",
      "Batch 197 : files 761 and next 2\n",
      "Batch 198 : files 763 and next 2\n",
      "Batch 199 : files 765 and next 2\n",
      "Batch 200 : files 767 and next 2\n",
      "Batch 201 : files 769 and next 2\n",
      "Batch 202 : files 771 and next 2\n",
      "Batch 203 : files 773 and next 2\n",
      "Batch 204 : files 775 and next 2\n",
      "Batch 205 : files 777 and next 2\n",
      "Batch 206 : files 779 and next 2\n",
      "Batch 207 : files 781 and next 2\n",
      "Batch 208 : files 783 and next 2\n",
      "Batch 209 : files 785 and next 2\n",
      "Batch 210 : files 787 and next 2\n",
      "Batch 211 : files 789 and next 2\n",
      "Batch 212 : files 791 and next 2\n",
      "Batch 213 : files 793 and next 2\n",
      "Batch 214 : files 795 and next 2\n",
      "Batch 215 : files 797 and next 2\n",
      "Batch 216 : files 799 and next 2\n",
      "Batch 217 : files 801 and next 2\n",
      "Batch 218 : files 803 and next 2\n",
      "Batch 219 : files 805 and next 2\n",
      "Batch 220 : files 807 and next 2\n",
      "Batch 221 : files 809 and next 2\n",
      "Batch 222 : files 811 and next 2\n",
      "Batch 223 : files 813 and next 2\n",
      "Batch 224 : files 815 and next 2\n",
      "Batch 225 : files 817 and next 2\n",
      "Batch 226 : files 819 and next 2\n",
      "Batch 227 : files 821 and next 2\n",
      "Batch 228 : files 823 and next 2\n",
      "Batch 229 : files 825 and next 2\n",
      "Batch 230 : files 827 and next 2\n",
      "Batch 231 : files 829 and next 2\n"
>>>>>>> a2f8ece5f88e193aba00735193ae24430ef15191
     ]
    }
   ],
   "source": [
    "#\n",
    "#  Execution de la boucle de predictions\n",
    "#\n",
    "# init test\n",
    "df_all_labels = df_test_init\n",
    "list_test_hash = []\n",
    "\n",
    "# Si fichier de predictions partielles disponible\n",
    "if os.path.exists(filename_test_res_partiels):\n",
    "    print(\"Predictions partielles disponibles\")\n",
    "    df_pred_partiel = pd.read_csv(filename_test_res_partiels, sep=\",\")\n",
    "    if len(df_pred_partiel)>0 :\n",
    "      nan_count = df_pred_partiel['64bits'].isna().sum()\n",
    "      init_idx_pred = df_test_init.shape[0] - nan_count # initialise la boucle du 1er graph non predit\n",
    "      indices_a_mettre_a_jour = df_all_labels.index[:init_idx_pred]\n",
    "      print(f\"Nombre de graphs predits: {init_idx_pred}\")\n",
    "      print(f\"Nombre de graphs A PREDIRE : {nan_count}\")\n",
    "\n",
    "      # Mettre à jour les valeurs des 25 premières lignes de df_pred avec celles de df_pred_partiel\n",
    "      df_pred_partiel_head = df_pred_partiel.head(init_idx_pred)\n",
    "      if len(df_pred_partiel_head) == len(indices_a_mettre_a_jour):\n",
    "        df_all_labels.loc[indices_a_mettre_a_jour] = df_pred_partiel_head.values\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> a2f8ece5f88e193aba00735193ae24430ef15191
    "    else:\n",
    "      nan_count = df_test_init.shape[0]\n",
    "      init_idx_pred = 0\n",
    "      print(f\"Fichier dispo, mais Nombre de graphs predits: 0\")\n",
    "      print(f\"Nombre de graphs A PREDIRE : tous\")\n",
    "\n",
    "# Si pas de fichier de predictions partielles disponible\n",
    "else:\n",
    "    print(\"Pas de prediction partielle disponible\")\n",
    "    nan_count = df_test_init.shape[0]\n",
    "    init_idx_pred = 0  # initialise la boucle du 1er graph non predit\n",
    "\n",
    "batch_data_size = 2\n",
    "batch_files_size = 2\n",
    "num_feature_dim = 20 #dim=20 pour V12, dim=15 pour V11\n",
    "\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "all_counts = {\n",
    "    'node_types': [],\n",
    "    'operations': [],\n",
    "    'registers': [],\n",
    "    'memory_patterns': [],\n",
    "    'immediates': [],\n",
    "    'unique_hashes': [],\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pour tous les hash du fichiers test\n",
    "    print(f\"**** Dataset Processing for Test\")\n",
    "    count_batch = 0\n",
    "    total_batch = round((len(df_test_init)-init_idx_pred)/batch_files_size , 0)\n",
    "\n",
    "    # on commence la boucle à l'index definit au dessus\n",
    "    for count_test in range(init_idx_pred, len(df_test_init), batch_files_size):\n",
    "\n",
    "        hash_names = df_test_init.iloc[count_test:count_test + batch_files_size]['name'].to_list()\n",
    "        print(f\"Batch {count_batch}/{total_batch} : files {count_test} and next {batch_files_size}\")\n",
    "        full_file_test_name = [os.path.join(test_path_dir, file+'.json') for file in hash_names]\n",
    "        #print(full_file_test_name)\n",
    "\n",
    "        batch_graph_list = process_batch(full_file_test_name)\n",
    "        #print(\"2 :\",len(batch_graph_list))\n",
    "        #tokenizer.fit(batch_graph_list)\n",
    "\n",
    "        # Tokenization for the current batch\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list)\n",
    "        for key in all_counts:\n",
    "            if key in tokens:\n",
    "                all_counts[key].extend(tokens[key])\n",
    "        tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "        test_dataset = AssemblyGraphDataset(batch_graph_list, tokenizer,  node_feature_dim=num_feature_dim)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_data_size, shuffle=False)\n",
    "\n",
    "        # loop on the batch from the dataloader\n",
    "        for batch in test_dataloader:\n",
    "            batch_hash_list = hash_names\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            # Create batch vector for global pooling\n",
    "            num_graphs = batch.num_graphs #obtain the number of graphs in the batch\n",
    "            #print(\"3:\", num_graphs)\n",
    "            num_nodes_per_graph = torch.bincount(batch.batch).tolist() # get number of nodes per graph.\n",
    "            if any(num_nodes_per_graph): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(torch.bincount(batch.batch).tolist())])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "\n",
    "            if len(prediction_labels) != len(batch_hash_list):\n",
    "                print(f\"*** error: len(prediction_labels)={len(prediction_labels)}, len(hash_names)={len(batch_hash_list)}\")\n",
    "\n",
    "            for i, hash_val in enumerate(batch_hash_list):\n",
    "                row_index = df_all_labels[df_all_labels['name'] == hash_val].index\n",
    "                if (not row_index.empty) and (i < len(prediction_labels)):\n",
    "\n",
    "                    label_list = prediction_labels[i].tolist()  # Convert tensor row to list\n",
    "                    int_label_list = [int(val) for val in label_list]  # Convert to int\n",
    "                    # Update the DataFrame\n",
    "                    for j in range(len(int_label_list)):\n",
    "                        df_all_labels.iloc[row_index, j+1] = int_label_list[j]\n",
    "                else:\n",
    "                    print(f\"Warning: hash_val '{hash_val}' not found in df_all_labels.\")\n",
    "\n",
    "        count_batch += 1\n",
    "        # on sauve les results partiels\n",
    "        df_all_labels.to_csv(filename_test_res_partiels, header=True ,index=False )\n",
    "\n",
    "    print(f\"Test end: nb pred labels: {len(df_all_labels)} \")\n",
    "    df_all_labels.to_csv(filename_test_results, header=True ,index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0iDIHzzvs7Y",
    "outputId": "34f6d940-1c1e-4435-d1c0-7cfa99cb9df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 454)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06a62f5604e232041634ef91c3a01bf7e543f055e62a64...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57f3c73b47ff31a26e05317c8680fa8dfa359c3e38b054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6da7aa61a776ef6e84eeb2e9cf76df84059054d7abd7f1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b23acaa5391daaf488a69d62ac4400cc7858591738bc8a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c0065a36127f18e6606d92cb86c5786689137ad4313d70...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...   \n",
       "1  4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...   \n",
       "2  2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...   \n",
       "3  243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...   \n",
       "4  533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...   \n",
       "5  06a62f5604e232041634ef91c3a01bf7e543f055e62a64...   \n",
       "6  57f3c73b47ff31a26e05317c8680fa8dfa359c3e38b054...   \n",
       "7  6da7aa61a776ef6e84eeb2e9cf76df84059054d7abd7f1...   \n",
       "8  b23acaa5391daaf488a69d62ac4400cc7858591738bc8a...   \n",
       "9  c0065a36127f18e6606d92cb86c5786689137ad4313d70...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                NaN     NaN         NaN   \n",
       "1                                NaN     NaN         NaN   \n",
       "2                                NaN     NaN         NaN   \n",
       "3                                NaN     NaN         NaN   \n",
       "4                                NaN     NaN         NaN   \n",
       "5                                NaN     NaN         NaN   \n",
       "6                                NaN     NaN         NaN   \n",
       "7                                NaN     NaN         NaN   \n",
       "8                                NaN     NaN         NaN   \n",
       "9                                NaN     NaN         NaN   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                            NaN                           NaN   \n",
       "1                            NaN                           NaN   \n",
       "2                            NaN                           NaN   \n",
       "3                            NaN                           NaN   \n",
       "4                            NaN                           NaN   \n",
       "5                            NaN                           NaN   \n",
       "6                            NaN                           NaN   \n",
       "7                            NaN                           NaN   \n",
       "8                            NaN                           NaN   \n",
       "9                            NaN                           NaN   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                NaN                 NaN                  NaN   \n",
       "1                NaN                 NaN                  NaN   \n",
       "2                NaN                 NaN                  NaN   \n",
       "3                NaN                 NaN                  NaN   \n",
       "4                NaN                 NaN                  NaN   \n",
       "5                NaN                 NaN                  NaN   \n",
       "6                NaN                 NaN                  NaN   \n",
       "7                NaN                 NaN                  NaN   \n",
       "8                NaN                 NaN                  NaN   \n",
       "9                NaN                 NaN                  NaN   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "1              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "2              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "3              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "4              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "5              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "6              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "7              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "8              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "9              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                   NaN                  NaN                    NaN   \n",
       "1                   NaN                  NaN                    NaN   \n",
       "2                   NaN                  NaN                    NaN   \n",
       "3                   NaN                  NaN                    NaN   \n",
       "4                   NaN                  NaN                    NaN   \n",
       "5                   NaN                  NaN                    NaN   \n",
       "6                   NaN                  NaN                    NaN   \n",
       "7                   NaN                  NaN                    NaN   \n",
       "8                   NaN                  NaN                    NaN   \n",
       "9                   NaN                  NaN                    NaN   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0         NaN       NaN   NaN  \n",
       "1         NaN       NaN   NaN  \n",
       "2         NaN       NaN   NaN  \n",
       "3         NaN       NaN   NaN  \n",
       "4         NaN       NaN   NaN  \n",
       "5         NaN       NaN   NaN  \n",
       "6         NaN       NaN   NaN  \n",
       "7         NaN       NaN   NaN  \n",
       "8         NaN       NaN   NaN  \n",
       "9         NaN       NaN   NaN  \n",
       "\n",
       "[10 rows x 454 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_all_labels.shape)\n",
    "df_all_labels.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
