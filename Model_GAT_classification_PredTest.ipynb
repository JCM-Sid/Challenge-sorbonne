{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15777,
     "status": "ok",
     "timestamp": 1742720364859,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "da509e60-8c4e-49f1-8128-e720702bae6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\n"
     ]
    }
   ],
   "source": [
    "# Challenge Sorbonne - DST\n",
    "#\n",
    "#!pip install torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25892,
     "status": "ok",
     "timestamp": 1742720504835,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "963f3e01-2c76-4511-da95-a91064c5a181"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 2051,
     "status": "ok",
     "timestamp": 1742720512835,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "43e12de7-4147-4287-a704-7e0dffa83b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/usr/users/msiac/msiac_10/Sorbonne' : #DCE\n",
    "    training_path_dir =   \"./training\"\n",
    "    test_path_dir = \"./test\"\n",
    "\n",
    "    train_meta_data = 'training_set_metadata.csv'\n",
    "    model_save_file =  'model_sorbonne_weights.pth'\n",
    "\n",
    "    split_char = '/'\n",
    "    filename_test_results = \"test_prediction.csv\"\n",
    "    filename_trained = \"list_hash_trained.csv\"\n",
    "    file_split_training = 'split_training-dce.csv'\n",
    "\n",
    "elif current_directory == '/content': # Google Colab\n",
    "    training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "    \n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/split_training-colab.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_set_to_predict.csv'\n",
    "    filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "    filename_trained =      '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/list_hash_trained.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    \n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\".\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  r'..\\..\\Data\\ChallengeSorbonne\\training_set_metadata.csv'\n",
    "    file_split_training = r'.\\split_training-pc.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\"\n",
    "    test_init_file = r\".\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\".\\test_prediction.csv\"\n",
    "    \n",
    "    model_save_file =  r'.\\model_sorbonne_weights.pth'\n",
    "    filename_test_results = r\".\\test_prediction.csv\"\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\": #PC2\n",
    "    split_char = '\\\\'\n",
    "    \n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\".\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  'training_set_metadata.csv'\n",
    "    file_split_training = r'.\\split_training-pc.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_path_dir = r\"G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\"\n",
    "    test_init_file = \"test_set_to_predict.csv\"\n",
    "    filename_test_results = \"test_prediction.csv\"\n",
    "    model_save_file =  'model_sorbonne_weights.pth'\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "# read CSV input and save in df\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "df_meta_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1742720516871,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "wCU7s5ZbtSu9",
    "outputId": "b6be6075-4089-4471-e8df-b5d10d46ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23102 entries, 0 to 23101\n",
      "Columns: 454 entries, name to yoda\n",
      "dtypes: int64(453), object(1)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meta_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1742720570710,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "b1fe1803-1abf-430b-cd54-b231adc20e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\list_hash_trained.csv\n",
      "taille : 5076\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>793a1bda32069f37ba201167d90e8c16c004008c4fd467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819d0b70a905ae5f8bef6c47423964359c2a90a168414f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>892e7b1463b98d6f5b41579c6b314de3a723af2c67d88f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0                                      files_trained\n",
       "1  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "2  793a1bda32069f37ba201167d90e8c16c004008c4fd467...\n",
       "3  819d0b70a905ae5f8bef6c47423964359c2a90a168414f...\n",
       "4  892e7b1463b98d6f5b41579c6b314de3a723af2c67d88f..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1719,
     "status": "ok",
     "timestamp": 1742720581568,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "1735a612-3320-4dc1-d85e-a6bb1e15ba58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0' 'rep_500' 'rep_1000']\n",
      "['D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04333849d7eb415a3986dc1afe0041723184b7f47aeeec5ffa07a0f9c222fee3.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04390cd4165bf34f62658cea0fa16116037e391a86ed59d09493c9d141f92276.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043a5b4be0a8f0500e88a2ffc91d15bf4212fa12a3f07be04654b67adc41d21c.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043cf998d7059cacaf626f443181c67f91adeb2bc7088ca918e1aee94838d23d.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043e2a8241a6eb0aa66de40e35e75dce0f1d047a6d9e4c934b482f7c588f9c61.json']\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_500']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique()[:3])\n",
    "print(full_file_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1742721120476,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0'):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    file_with_err = 0\n",
    "    count_files = 0\n",
    "    max_file = 0\n",
    "\n",
    "    #file_split_training = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\split_training-colab.csv\"\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "            #print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 100_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                print(\"size\", file_size/1000, full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1742720599926,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "class HierarchicalAssemblyTokenizer:\n",
    "    def __init__(self):\n",
    "        # Tokenizer implementation from previous code\n",
    "        # Abbreviated for brevity but would be the full implementation\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "    def fit(self, graph_l, min_freq=1):\n",
    "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        self.unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        # Extract features from node labels\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                # Extract node type (JCC, INST)\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                # Extract operation (xor, push, mov)\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                # Extract registers\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                # Extract memory reference patterns\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                # Extract immediate values\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            self.unique_hashes.append(item['name'])\n",
    "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "            for i, hash_val in enumerate(self.unique_hashes):\n",
    "                self.hash_to_id[hash_val] = i\n",
    "                self.id_to_hash[i] = hash_val\n",
    "\n",
    "        # Build vocabularies with frequency filtering\n",
    "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
    "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
    "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
    "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_from_counts(self, all_counts, min_freq=3):\n",
    "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
    "        self.node_type_vocab.clear()\n",
    "        self.operation_vocab.clear()\n",
    "        self.register_vocab.clear()\n",
    "        self.memory_pattern_vocab.clear()\n",
    "        self.immediate_vocab.clear()\n",
    "\n",
    "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
    "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
    "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
    "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
    "\n",
    "        self.unique_hashes = all_counts['unique_hashes']\n",
    "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "        for i, hash_val in enumerate(self.unique_hashes):\n",
    "            self.hash_to_id[hash_val] = i\n",
    "            self.id_to_hash[i] = hash_val\n",
    "\n",
    "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
    "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
    "        counter = Counter(tokens)\n",
    "        # Add special UNK token\n",
    "        vocab_dict[self.UNK_TOKEN] = 0\n",
    "\n",
    "        # Add tokens that meet minimum frequency\n",
    "        idx = 1\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                vocab_dict[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def _memory_to_pattern(self, mem_ref):\n",
    "        \"\"\"Convert memory reference to pattern\"\"\"\n",
    "        # Replace registers with REG placeholder\n",
    "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
    "        # Replace immediate values with IMM placeholder\n",
    "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
    "        return pattern.strip()\n",
    "\n",
    "    def tokenize(self, node_label):\n",
    "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
    "        features = {\n",
    "            'node_type': self.UNK_TOKEN,\n",
    "            'operation': self.UNK_TOKEN,\n",
    "            'registers': [],\n",
    "            'memory_pattern': self.UNK_TOKEN,\n",
    "            'immediate': self.UNK_TOKEN\n",
    "        }\n",
    "\n",
    "        # Extract node type\n",
    "        node_type_match = self.patterns['node_type'].search(node_label)\n",
    "        if node_type_match:\n",
    "            node_type = node_type_match.group(1)\n",
    "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract operation\n",
    "        op_match = self.patterns['operation'].search(node_label)\n",
    "        if op_match:\n",
    "            operation = op_match.group(1)\n",
    "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract registers\n",
    "        reg_matches = self.patterns['registers'].findall(node_label)\n",
    "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
    "\n",
    "        # Extract memory reference\n",
    "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
    "        if mem_matches:\n",
    "            pattern = self._memory_to_pattern(mem_matches[0])\n",
    "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract immediate values\n",
    "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
    "        if imm_matches:\n",
    "            imm = imm_matches[0]\n",
    "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
    "\n",
    "        return features\n",
    "\n",
    "    def encode_nodelabel(self, node_label):\n",
    "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
    "        features = self.tokenize(node_label)\n",
    "\n",
    "        # Encode each feature\n",
    "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
    "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
    "\n",
    "        # Encode registers (take up to 3, pad if fewer)\n",
    "        register_indices = []\n",
    "        for i in range(min(3, len(features['registers']))):\n",
    "            reg = features['registers'][i]\n",
    "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
    "            register_indices.append(reg_idx)\n",
    "\n",
    "        # Pad register indices if needed\n",
    "        while len(register_indices) < 3:\n",
    "            register_indices.append(0)  # 0 for padding\n",
    "\n",
    "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
    "            features['memory_pattern'],\n",
    "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        immediate_idx = self.immediate_vocab.get(\n",
    "            features['immediate'],\n",
    "            self.immediate_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Combine all indices into a feature vector\n",
    "        encoded = np.array([\n",
    "            node_type_idx,\n",
    "            operation_idx,\n",
    "            register_indices[0],\n",
    "            register_indices[1],\n",
    "            register_indices[2],\n",
    "            memory_pattern_idx,\n",
    "            immediate_idx\n",
    "        ], dtype=np.int64)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_graph(self, digraph):\n",
    "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
    "        node_features = {}\n",
    "\n",
    "        for node_id in digraph.nodes():\n",
    "            label = digraph.nodes[node_id].get('label', '')\n",
    "            node_features[node_id] = self.encode_nodelabel(label)\n",
    "\n",
    "        return node_features\n",
    "\n",
    "    def encode_hash(self, hash_file):\n",
    "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return the size of each vocabulary\"\"\"\n",
    "        return {\n",
    "            'node_type': len(self.node_type_vocab),\n",
    "            'operation': len(self.operation_vocab),\n",
    "            'register': len(self.register_vocab),\n",
    "            'memory_pattern': len(self.memory_pattern_vocab),\n",
    "            'immediate': len(self.immediate_vocab)\n",
    "        }\n",
    "\n",
    "    def get_tokens_and_counts(self, graph_l):\n",
    "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            unique_hashes.append(item['name'])\n",
    "\n",
    "        return {\n",
    "            'node_types': node_types,\n",
    "            'operations': operations,\n",
    "            'registers': registers,\n",
    "            'memory_patterns': memory_patterns,\n",
    "            'immediates': immediates,\n",
    "            'unique_hashes': unique_hashes,\n",
    "        }\n",
    "\n",
    "class AssemblyGraphDataset:\n",
    "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
    "        \"\"\"\n",
    "        Prepares a dataset of assembly graphs for GAT model training\n",
    "        Args:\n",
    "            graph_list: List of NetworkX digraphs representing assembly code\n",
    "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
    "            node_feature_dim: Dimension of node embeddings\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        # Initialize embedding layers for each feature type\n",
    "        num_feature_type = 5\n",
    "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
    "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
    "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
    "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
    "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
    "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
    "\n",
    "        # Process graphs into PyTorch Geometric Data objects\n",
    "        self.data_list = []\n",
    "        for graph in self.graph_list:\n",
    "            # process graph info\n",
    "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
    "\n",
    "    def _process_graph(self, graph, hash_name):\n",
    "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
    "        # Create a node ID mapping for consecutive IDs\n",
    "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "        # Get node features\n",
    "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
    "\n",
    "        # Convert to tensor-friendly format\n",
    "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
    "        for node_id, features in node_features_dict.items():\n",
    "            x[node_mapping[node_id]] = torch.tensor(features)\n",
    "\n",
    "        # Create edge index\n",
    "        edge_list = list(graph.edges())\n",
    "        if not edge_list:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
    "\n",
    "        # Endcode Hash name\n",
    "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
    "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
    "\n",
    "        # Get embeddings before creating Data object\n",
    "        x = self.get_embeddings(x)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
    "        return data\n",
    "\n",
    "    def optimize_edge_index(self, edge_list, node_mapping):\n",
    "        \"\"\"\n",
    "        Optimizes the creation of edge_index tensor for graph representation.\n",
    "        Args:\n",
    "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
    "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Optimized edge_index tensor.\n",
    "        \"\"\"\n",
    "        if not edge_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "        num_edges = len(edge_list)\n",
    "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "\n",
    "        for i, (src, tgt) in enumerate(edge_list):\n",
    "            src_indices[i] = node_mapping[src]\n",
    "            tgt_indices[i] = node_mapping[tgt]\n",
    "\n",
    "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
    "        # Split features into their components\n",
    "        node_type_idx = x[:, 0]\n",
    "        operation_idx = x[:, 1]\n",
    "        register_idx1 = x[:, 2]\n",
    "        register_idx2 = x[:, 3]\n",
    "        register_idx3 = x[:, 4]\n",
    "        memory_pattern_idx = x[:, 5]\n",
    "        immediate_idx = x[:, 6]\n",
    "\n",
    "        # Get embeddings for each component\n",
    "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
    "        operation_emb = self.operation_embedding(operation_idx)\n",
    "\n",
    "        # Combine register embeddings (average them)\n",
    "        register_emb = (self.register_embedding(register_idx1) +\n",
    "                        self.register_embedding(register_idx2) +\n",
    "                        self.register_embedding(register_idx3)) / 3\n",
    "\n",
    "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
    "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        return torch.cat([\n",
    "            node_type_emb,\n",
    "            operation_emb,\n",
    "            register_emb,\n",
    "            memory_pattern_emb,\n",
    "            immediate_emb\n",
    "        ], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Graph Attention Network for assembly code analysis\n",
    "Args:\n",
    "    dataset: AssemblyGraphDataset instance\n",
    "    hidden_dim: Hidden dimension of GAT layers\n",
    "    output_dim: Output dimension of node embeddings\n",
    "    heads: Number of attention heads\n",
    "    dropout: Dropout rate\n",
    "\"\"\"\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, x, edge_index, hash_encoded, batch):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742721142838,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "INiZZqldtSu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# init parameters to loop the file loading by batch\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "random_seed = 42\n",
    "\n",
    "def split_dataloader(dataset, list_of_hash, batch_size, validation_split=0.5, shuffle=True):\n",
    "    random.seed(random_seed) #set random seed for random module.\n",
    "    torch.manual_seed(random_seed) #set random seed for torch.\n",
    "\n",
    "    validation_size = int(len(dataset) * validation_split)\n",
    "    train_size = len(dataset) - validation_size\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    validation_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    validation_dataset = Subset(dataset, validation_indices)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # Shuffle is handled by the initial indices shuffling.\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_hash = [list_of_hash[i] for i in train_indices]\n",
    "    val_hash = [list_of_hash[i] for i in validation_indices]\n",
    "\n",
    "    return train_dataloader, validation_dataloader, train_hash, val_hash\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                if os.path.exists(full_path_file):\n",
    "                    file_size = os.path.getsize(full_path_file)\n",
    "                    if file_size/1000 > 100_000: #file too big\n",
    "                        print(\"size issue : skip this file test\", file_size/1000, full_path_file)\n",
    "                        continue\n",
    "\n",
    "                digraph_str = f.read()\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append({\n",
    "                    'name': hash_file,\n",
    "                    'graph_input' : G\n",
    "                })\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
    "    if os.path.exists(model_save_file):\n",
    "      if torch.cuda.is_available():\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "      else:\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        count_batch = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # get the different components of the dataset\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
    "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # save model trained\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
    "    if os.path.exists(model_save_file):\n",
    "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "\n",
    "    model.eval() #set the model to evaluation mode.\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "    hash_v = []\n",
    "\n",
    "    count_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_label.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #hash_v.append(list_hash_batch)\n",
    "\n",
    "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "\n",
    "    return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) +1\n",
    "\n",
    "    hash_v = []\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    all_counts = {\n",
    "        'node_types': [],\n",
    "        'operations': [],\n",
    "        'registers': [],\n",
    "        'memory_patterns': [],\n",
    "        'immediates': [],\n",
    "        'unique_hashes': [],\n",
    "    }\n",
    "    print(f\"**** Tokenization Processing\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "\n",
    "        #print(f\"Tokenization Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if len(batch_graph_list) == 0:\n",
    "            print(\"graph list = 0\")\n",
    "            break\n",
    "\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list) # you will need to implement this\n",
    "        all_counts.update(tokens)\n",
    "\n",
    "    # Create a tokenizer from token counts\n",
    "    tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "    print(f\"**** Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        print(f\"Dataset Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if len(batch_graph_list) == 0:\n",
    "            print(\"graph list = 0\")\n",
    "            break\n",
    "\n",
    "        dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
    "        curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
    "\n",
    "        list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
    "\n",
    "        if mode == 'train':\n",
    "            train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=3)\n",
    "            hash_t.append(list_of_hash)\n",
    "        else:\n",
    "            pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
    "            all_true_label.append(true_labels.cpu())\n",
    "            all_pred_label.append(pred_labels.cpu())\n",
    "            hash_v.append(list_of_hash)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # save all labels as tensor to return if mode= 'validation'\n",
    "    if mode == 'validation':\n",
    "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "\n",
    "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "    elif mode == 'train':\n",
    "        return hash_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODE TEST ONLY\n",
    "\n",
    "Generate prediction based on the requested list of hash (no true labels available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 454)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...   \n",
       "1  4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...   \n",
       "2  2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...   \n",
       "3  243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...   \n",
       "4  533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                NaN     NaN         NaN   \n",
       "1                                NaN     NaN         NaN   \n",
       "2                                NaN     NaN         NaN   \n",
       "3                                NaN     NaN         NaN   \n",
       "4                                NaN     NaN         NaN   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                            NaN                           NaN   \n",
       "1                            NaN                           NaN   \n",
       "2                            NaN                           NaN   \n",
       "3                            NaN                           NaN   \n",
       "4                            NaN                           NaN   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                NaN                 NaN                  NaN   \n",
       "1                NaN                 NaN                  NaN   \n",
       "2                NaN                 NaN                  NaN   \n",
       "3                NaN                 NaN                  NaN   \n",
       "4                NaN                 NaN                  NaN   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "1              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "2              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "3              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "4              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                   NaN                  NaN                    NaN   \n",
       "1                   NaN                  NaN                    NaN   \n",
       "2                   NaN                  NaN                    NaN   \n",
       "3                   NaN                  NaN                    NaN   \n",
       "4                   NaN                  NaN                    NaN   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0         NaN       NaN   NaN  \n",
       "1         NaN       NaN   NaN  \n",
       "2         NaN       NaN   NaN  \n",
       "3         NaN       NaN   NaN  \n",
       "4         NaN       NaN   NaN  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_init_file = r\"test_set_to_predict-JC.csv\"\n",
    "#filename_test_results = r\"test_prediction.csv\"\n",
    "test_path_dir = r\"G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\"\n",
    "df_test_init = pd.read_csv(test_init_file, sep=\",\")\n",
    "col_to_predict = df_test_init.columns\n",
    "print(df_test_init.shape)\n",
    "df_test_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mts_g2jUtSu-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run: 453 labels to predict for 3000 graphs \n",
      "from files located: G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\n",
      "\n",
      "**** Dataset Processing for Test\n",
      "Batch 0 : files 0 and next 2\n",
      "Batch 1 : files 2 and next 2\n",
      "Batch 2 : files 4 and next 2\n",
      "Batch 3 : files 6 and next 2\n",
      "Batch 4 : files 8 and next 2\n",
      "Batch 5 : files 10 and next 2\n",
      "Batch 6 : files 12 and next 2\n",
      "Batch 7 : files 14 and next 2\n",
      "Batch 8 : files 16 and next 2\n",
      "Batch 9 : files 18 and next 2\n",
      "Batch 10 : files 20 and next 2\n",
      "Batch 11 : files 22 and next 2\n",
      "Batch 12 : files 24 and next 2\n",
      "Batch 13 : files 26 and next 2\n",
      "Batch 14 : files 28 and next 2\n",
      "Batch 15 : files 30 and next 2\n",
      "Batch 16 : files 32 and next 2\n",
      "Batch 17 : files 34 and next 2\n",
      "Batch 18 : files 36 and next 2\n",
      "Batch 19 : files 38 and next 2\n",
      "Batch 20 : files 40 and next 2\n",
      "Batch 21 : files 42 and next 2\n",
      "Batch 22 : files 44 and next 2\n",
      "Batch 23 : files 46 and next 2\n",
      "Batch 24 : files 48 and next 2\n",
      "Batch 25 : files 50 and next 2\n",
      "Batch 26 : files 52 and next 2\n",
      "Batch 27 : files 54 and next 2\n",
      "Batch 28 : files 56 and next 2\n",
      "Batch 29 : files 58 and next 2\n",
      "Batch 30 : files 60 and next 2\n",
      "Batch 31 : files 62 and next 2\n",
      "Batch 32 : files 64 and next 2\n",
      "Batch 33 : files 66 and next 2\n",
      "Batch 34 : files 68 and next 2\n",
      "Batch 35 : files 70 and next 2\n",
      "Batch 36 : files 72 and next 2\n",
      "Batch 37 : files 74 and next 2\n",
      "Batch 38 : files 76 and next 2\n",
      "Batch 39 : files 78 and next 2\n",
      "Batch 40 : files 80 and next 2\n",
      "Batch 41 : files 82 and next 2\n",
      "Batch 42 : files 84 and next 2\n",
      "Batch 43 : files 86 and next 2\n",
      "Batch 44 : files 88 and next 2\n",
      "Batch 45 : files 90 and next 2\n",
      "Batch 46 : files 92 and next 2\n",
      "Batch 47 : files 94 and next 2\n",
      "Batch 48 : files 96 and next 2\n",
      "Batch 49 : files 98 and next 2\n",
      "Batch 50 : files 100 and next 2\n",
      "Batch 51 : files 102 and next 2\n",
      "Batch 52 : files 104 and next 2\n",
      "Batch 53 : files 106 and next 2\n",
      "Batch 54 : files 108 and next 2\n",
      "Batch 55 : files 110 and next 2\n",
      "Batch 56 : files 112 and next 2\n",
      "Batch 57 : files 114 and next 2\n",
      "Batch 58 : files 116 and next 2\n",
      "Batch 59 : files 118 and next 2\n",
      "Batch 60 : files 120 and next 2\n",
      "Batch 61 : files 122 and next 2\n",
      "Batch 62 : files 124 and next 2\n",
      "Batch 63 : files 126 and next 2\n",
      "Batch 64 : files 128 and next 2\n",
      "Batch 65 : files 130 and next 2\n",
      "Batch 66 : files 132 and next 2\n",
      "Batch 67 : files 134 and next 2\n",
      "Batch 68 : files 136 and next 2\n",
      "Batch 69 : files 138 and next 2\n",
      "Batch 70 : files 140 and next 2\n",
      "Batch 71 : files 142 and next 2\n",
      "Batch 72 : files 144 and next 2\n",
      "Batch 73 : files 146 and next 2\n",
      "Batch 74 : files 148 and next 2\n",
      "Batch 75 : files 150 and next 2\n",
      "Batch 76 : files 152 and next 2\n",
      "Batch 77 : files 154 and next 2\n",
      "Batch 78 : files 156 and next 2\n",
      "Batch 79 : files 158 and next 2\n",
      "Batch 80 : files 160 and next 2\n",
      "Batch 81 : files 162 and next 2\n",
      "Batch 82 : files 164 and next 2\n",
      "Batch 83 : files 166 and next 2\n",
      "Batch 84 : files 168 and next 2\n",
      "Batch 85 : files 170 and next 2\n",
      "Batch 86 : files 172 and next 2\n",
      "size issue : skip this file test 153139.48 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\88c013bcbbca55dda8c44f764466d9a73b555cc6a0930dedb9033a70f76993b6.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val 'e59ba77f0494ada1da088effb9162755b9a527372889a19316e401bcefb684a4' not found in df_all_labels.\n",
      "Batch 87 : files 174 and next 2\n",
      "Batch 88 : files 176 and next 2\n",
      "Batch 89 : files 178 and next 2\n",
      "Batch 90 : files 180 and next 2\n",
      "Batch 91 : files 182 and next 2\n",
      "Batch 92 : files 184 and next 2\n",
      "Batch 93 : files 186 and next 2\n",
      "Batch 94 : files 188 and next 2\n",
      "Batch 95 : files 190 and next 2\n",
      "Batch 96 : files 192 and next 2\n",
      "Batch 97 : files 194 and next 2\n",
      "Batch 98 : files 196 and next 2\n",
      "Batch 99 : files 198 and next 2\n",
      "Batch 100 : files 200 and next 2\n",
      "Batch 101 : files 202 and next 2\n",
      "Batch 102 : files 204 and next 2\n",
      "Batch 103 : files 206 and next 2\n",
      "Batch 104 : files 208 and next 2\n",
      "Batch 105 : files 210 and next 2\n",
      "Batch 106 : files 212 and next 2\n",
      "size issue : skip this file test 145305.137 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\c8734a60649045b3514b5b3b9d49d2ca334bfb749ee9d5a7e5b8bf2a6fe26370.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val 'c8734a60649045b3514b5b3b9d49d2ca334bfb749ee9d5a7e5b8bf2a6fe26370' not found in df_all_labels.\n",
      "Batch 107 : files 214 and next 2\n",
      "Batch 108 : files 216 and next 2\n",
      "Batch 109 : files 218 and next 2\n",
      "Batch 110 : files 220 and next 2\n",
      "Batch 111 : files 222 and next 2\n",
      "Batch 112 : files 224 and next 2\n",
      "Batch 113 : files 226 and next 2\n",
      "Batch 114 : files 228 and next 2\n",
      "Batch 115 : files 230 and next 2\n",
      "Batch 116 : files 232 and next 2\n",
      "Batch 117 : files 234 and next 2\n",
      "Batch 118 : files 236 and next 2\n",
      "Batch 119 : files 238 and next 2\n",
      "Batch 120 : files 240 and next 2\n",
      "Batch 121 : files 242 and next 2\n",
      "Batch 122 : files 244 and next 2\n",
      "Batch 123 : files 246 and next 2\n",
      "Batch 124 : files 248 and next 2\n",
      "Batch 125 : files 250 and next 2\n",
      "Batch 126 : files 252 and next 2\n",
      "Batch 127 : files 254 and next 2\n",
      "Batch 128 : files 256 and next 2\n",
      "Batch 129 : files 258 and next 2\n",
      "Batch 130 : files 260 and next 2\n",
      "Batch 131 : files 262 and next 2\n",
      "Batch 132 : files 264 and next 2\n",
      "Batch 133 : files 266 and next 2\n",
      "Batch 134 : files 268 and next 2\n",
      "Batch 135 : files 270 and next 2\n",
      "Batch 136 : files 272 and next 2\n",
      "Batch 137 : files 274 and next 2\n",
      "Batch 138 : files 276 and next 2\n",
      "Batch 139 : files 278 and next 2\n",
      "Batch 140 : files 280 and next 2\n",
      "Batch 141 : files 282 and next 2\n",
      "Batch 142 : files 284 and next 2\n",
      "Batch 143 : files 286 and next 2\n",
      "Batch 144 : files 288 and next 2\n",
      "Batch 145 : files 290 and next 2\n",
      "Batch 146 : files 292 and next 2\n",
      "Batch 147 : files 294 and next 2\n",
      "Batch 148 : files 296 and next 2\n",
      "Batch 149 : files 298 and next 2\n",
      "Batch 150 : files 300 and next 2\n",
      "Batch 151 : files 302 and next 2\n",
      "Batch 152 : files 304 and next 2\n",
      "Batch 153 : files 306 and next 2\n",
      "Batch 154 : files 308 and next 2\n",
      "Batch 155 : files 310 and next 2\n",
      "Batch 156 : files 312 and next 2\n",
      "Batch 157 : files 314 and next 2\n",
      "size issue : skip this file test 201392.681 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\fc6383997b0ded4ceda987db3c0ea28c3bcaf9c47515d1b1acd772bd3fe01593.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val '36903bd20ba6fcd2a0983b505a0cb52d299cc617ced05a930974951bb96a2909' not found in df_all_labels.\n",
      "Batch 158 : files 316 and next 2\n",
      "Batch 159 : files 318 and next 2\n",
      "Batch 160 : files 320 and next 2\n",
      "Batch 161 : files 322 and next 2\n",
      "Batch 162 : files 324 and next 2\n",
      "Batch 163 : files 326 and next 2\n",
      "Batch 164 : files 328 and next 2\n",
      "Batch 165 : files 330 and next 2\n",
      "Batch 166 : files 332 and next 2\n",
      "Batch 167 : files 334 and next 2\n",
      "Batch 168 : files 336 and next 2\n",
      "Batch 169 : files 338 and next 2\n",
      "Batch 170 : files 340 and next 2\n",
      "Batch 171 : files 342 and next 2\n",
      "Batch 172 : files 344 and next 2\n",
      "Batch 173 : files 346 and next 2\n",
      "Batch 174 : files 348 and next 2\n",
      "Batch 175 : files 350 and next 2\n",
      "Batch 176 : files 352 and next 2\n",
      "Batch 177 : files 354 and next 2\n",
      "Batch 178 : files 356 and next 2\n",
      "Batch 179 : files 358 and next 2\n",
      "Batch 180 : files 360 and next 2\n",
      "Batch 181 : files 362 and next 2\n",
      "Batch 182 : files 364 and next 2\n",
      "Batch 183 : files 366 and next 2\n",
      "Batch 184 : files 368 and next 2\n",
      "Batch 185 : files 370 and next 2\n",
      "Batch 186 : files 372 and next 2\n",
      "Batch 187 : files 374 and next 2\n",
      "Batch 188 : files 376 and next 2\n",
      "Batch 189 : files 378 and next 2\n",
      "Batch 190 : files 380 and next 2\n",
      "Batch 191 : files 382 and next 2\n",
      "Batch 192 : files 384 and next 2\n",
      "Batch 193 : files 386 and next 2\n",
      "Batch 194 : files 388 and next 2\n",
      "Batch 195 : files 390 and next 2\n",
      "Batch 196 : files 392 and next 2\n",
      "Batch 197 : files 394 and next 2\n",
      "Batch 198 : files 396 and next 2\n",
      "Batch 199 : files 398 and next 2\n",
      "size issue : skip this file test 132465.984 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\a310924aee59ecdca6f35125860ec92527b884a1d23390f767f85205166f13f7.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val 'f61e7e54a593fd266079542a829ac822934381fb29b1c7f82ae153fbbcf89b55' not found in df_all_labels.\n",
      "Batch 200 : files 400 and next 2\n",
      "Batch 201 : files 402 and next 2\n",
      "Batch 202 : files 404 and next 2\n",
      "Batch 203 : files 406 and next 2\n",
      "Batch 204 : files 408 and next 2\n",
      "Batch 205 : files 410 and next 2\n",
      "Batch 206 : files 412 and next 2\n",
      "Batch 207 : files 414 and next 2\n",
      "Batch 208 : files 416 and next 2\n",
      "Batch 209 : files 418 and next 2\n",
      "Batch 210 : files 420 and next 2\n",
      "Batch 211 : files 422 and next 2\n",
      "Batch 212 : files 424 and next 2\n",
      "Batch 213 : files 426 and next 2\n",
      "size issue : skip this file test 171359.812 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\5aa679416c0ac127fbbcb888977164d89a1446fe5783a2277b34e0a6f17ca99e.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val '5aa679416c0ac127fbbcb888977164d89a1446fe5783a2277b34e0a6f17ca99e' not found in df_all_labels.\n",
      "Batch 214 : files 428 and next 2\n",
      "Batch 215 : files 430 and next 2\n",
      "Batch 216 : files 432 and next 2\n",
      "Batch 217 : files 434 and next 2\n",
      "Batch 218 : files 436 and next 2\n",
      "Batch 219 : files 438 and next 2\n",
      "Batch 220 : files 440 and next 2\n",
      "Batch 221 : files 442 and next 2\n",
      "Batch 222 : files 444 and next 2\n",
      "Batch 223 : files 446 and next 2\n",
      "Batch 224 : files 448 and next 2\n",
      "Batch 225 : files 450 and next 2\n",
      "Batch 226 : files 452 and next 2\n",
      "Batch 227 : files 454 and next 2\n",
      "Batch 228 : files 456 and next 2\n",
      "Batch 229 : files 458 and next 2\n",
      "Batch 230 : files 460 and next 2\n",
      "Batch 231 : files 462 and next 2\n",
      "Batch 232 : files 464 and next 2\n",
      "Batch 233 : files 466 and next 2\n",
      "Batch 234 : files 468 and next 2\n",
      "Batch 235 : files 470 and next 2\n",
      "Batch 236 : files 472 and next 2\n",
      "Batch 237 : files 474 and next 2\n",
      "Batch 238 : files 476 and next 2\n",
      "Batch 239 : files 478 and next 2\n",
      "Batch 240 : files 480 and next 2\n",
      "Batch 241 : files 482 and next 2\n",
      "Batch 242 : files 484 and next 2\n",
      "Batch 243 : files 486 and next 2\n",
      "Batch 244 : files 488 and next 2\n",
      "Batch 245 : files 490 and next 2\n",
      "Batch 246 : files 492 and next 2\n",
      "Batch 247 : files 494 and next 2\n",
      "Batch 248 : files 496 and next 2\n",
      "Batch 249 : files 498 and next 2\n",
      "Batch 250 : files 500 and next 2\n",
      "Batch 251 : files 502 and next 2\n",
      "Batch 252 : files 504 and next 2\n",
      "size issue : skip this file test 108512.129 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\3cd887e2fae7a101dc6be7348f86f37265d7cd09e40b6d485bfe318a99981310.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val '3cd887e2fae7a101dc6be7348f86f37265d7cd09e40b6d485bfe318a99981310' not found in df_all_labels.\n",
      "Batch 253 : files 506 and next 2\n",
      "Batch 254 : files 508 and next 2\n",
      "Batch 255 : files 510 and next 2\n",
      "Batch 256 : files 512 and next 2\n",
      "Batch 257 : files 514 and next 2\n",
      "Batch 258 : files 516 and next 2\n",
      "Batch 259 : files 518 and next 2\n",
      "Batch 260 : files 520 and next 2\n",
      "Batch 261 : files 522 and next 2\n",
      "Batch 262 : files 524 and next 2\n",
      "Batch 263 : files 526 and next 2\n",
      "Batch 264 : files 528 and next 2\n",
      "Batch 265 : files 530 and next 2\n",
      "Batch 266 : files 532 and next 2\n",
      "Batch 267 : files 534 and next 2\n",
      "Batch 268 : files 536 and next 2\n",
      "Batch 269 : files 538 and next 2\n",
      "Batch 270 : files 540 and next 2\n",
      "Batch 271 : files 542 and next 2\n",
      "Batch 272 : files 544 and next 2\n",
      "Batch 273 : files 546 and next 2\n",
      "Batch 274 : files 548 and next 2\n",
      "Batch 275 : files 550 and next 2\n",
      "Batch 276 : files 552 and next 2\n",
      "Batch 277 : files 554 and next 2\n",
      "Batch 278 : files 556 and next 2\n",
      "Batch 279 : files 558 and next 2\n",
      "Batch 280 : files 560 and next 2\n",
      "Batch 281 : files 562 and next 2\n",
      "Batch 282 : files 564 and next 2\n",
      "Batch 283 : files 566 and next 2\n",
      "Batch 284 : files 568 and next 2\n",
      "Batch 285 : files 570 and next 2\n",
      "Batch 286 : files 572 and next 2\n",
      "Batch 287 : files 574 and next 2\n",
      "Batch 288 : files 576 and next 2\n",
      "Batch 289 : files 578 and next 2\n",
      "Batch 290 : files 580 and next 2\n",
      "Batch 291 : files 582 and next 2\n",
      "Batch 292 : files 584 and next 2\n",
      "Batch 293 : files 586 and next 2\n",
      "Batch 294 : files 588 and next 2\n",
      "Batch 295 : files 590 and next 2\n",
      "Batch 296 : files 592 and next 2\n",
      "Batch 297 : files 594 and next 2\n",
      "Batch 298 : files 596 and next 2\n",
      "Batch 299 : files 598 and next 2\n",
      "Batch 300 : files 600 and next 2\n",
      "Batch 301 : files 602 and next 2\n",
      "Batch 302 : files 604 and next 2\n",
      "Batch 303 : files 606 and next 2\n",
      "Batch 304 : files 608 and next 2\n",
      "Batch 305 : files 610 and next 2\n",
      "Batch 306 : files 612 and next 2\n",
      "Batch 307 : files 614 and next 2\n",
      "Batch 308 : files 616 and next 2\n",
      "Batch 309 : files 618 and next 2\n",
      "Batch 310 : files 620 and next 2\n",
      "Batch 311 : files 622 and next 2\n",
      "Batch 312 : files 624 and next 2\n",
      "Batch 313 : files 626 and next 2\n",
      "Batch 314 : files 628 and next 2\n",
      "Batch 315 : files 630 and next 2\n",
      "Batch 316 : files 632 and next 2\n",
      "Batch 317 : files 634 and next 2\n",
      "Batch 318 : files 636 and next 2\n",
      "Batch 319 : files 638 and next 2\n",
      "Batch 320 : files 640 and next 2\n",
      "Batch 321 : files 642 and next 2\n",
      "Batch 322 : files 644 and next 2\n",
      "Batch 323 : files 646 and next 2\n",
      "Batch 324 : files 648 and next 2\n",
      "Batch 325 : files 650 and next 2\n",
      "Batch 326 : files 652 and next 2\n",
      "Batch 327 : files 654 and next 2\n",
      "Batch 328 : files 656 and next 2\n",
      "Batch 329 : files 658 and next 2\n",
      "Batch 330 : files 660 and next 2\n",
      "Batch 331 : files 662 and next 2\n",
      "Batch 332 : files 664 and next 2\n",
      "Batch 333 : files 666 and next 2\n",
      "Batch 334 : files 668 and next 2\n",
      "Batch 335 : files 670 and next 2\n",
      "Batch 336 : files 672 and next 2\n",
      "Batch 337 : files 674 and next 2\n",
      "Batch 338 : files 676 and next 2\n",
      "Batch 339 : files 678 and next 2\n",
      "Batch 340 : files 680 and next 2\n",
      "Batch 341 : files 682 and next 2\n",
      "Batch 342 : files 684 and next 2\n",
      "Batch 343 : files 686 and next 2\n",
      "Batch 344 : files 688 and next 2\n",
      "Batch 345 : files 690 and next 2\n",
      "Batch 346 : files 692 and next 2\n",
      "Batch 347 : files 694 and next 2\n",
      "Batch 348 : files 696 and next 2\n",
      "Batch 349 : files 698 and next 2\n",
      "Batch 350 : files 700 and next 2\n",
      "Batch 351 : files 702 and next 2\n",
      "Batch 352 : files 704 and next 2\n",
      "Batch 353 : files 706 and next 2\n",
      "Batch 354 : files 708 and next 2\n",
      "Batch 355 : files 710 and next 2\n",
      "Batch 356 : files 712 and next 2\n",
      "Batch 357 : files 714 and next 2\n",
      "Batch 358 : files 716 and next 2\n",
      "Batch 359 : files 718 and next 2\n",
      "Batch 360 : files 720 and next 2\n",
      "Batch 361 : files 722 and next 2\n",
      "Batch 362 : files 724 and next 2\n",
      "Batch 363 : files 726 and next 2\n",
      "Batch 364 : files 728 and next 2\n",
      "Batch 365 : files 730 and next 2\n",
      "Batch 366 : files 732 and next 2\n",
      "Batch 367 : files 734 and next 2\n",
      "Batch 368 : files 736 and next 2\n",
      "Batch 369 : files 738 and next 2\n",
      "Batch 370 : files 740 and next 2\n",
      "Batch 371 : files 742 and next 2\n",
      "Batch 372 : files 744 and next 2\n",
      "Batch 373 : files 746 and next 2\n",
      "Batch 374 : files 748 and next 2\n",
      "Batch 375 : files 750 and next 2\n",
      "Batch 376 : files 752 and next 2\n",
      "Batch 377 : files 754 and next 2\n",
      "Batch 378 : files 756 and next 2\n",
      "Batch 379 : files 758 and next 2\n",
      "Batch 380 : files 760 and next 2\n",
      "Batch 381 : files 762 and next 2\n",
      "Batch 382 : files 764 and next 2\n",
      "Batch 383 : files 766 and next 2\n",
      "Batch 384 : files 768 and next 2\n",
      "Batch 385 : files 770 and next 2\n",
      "Batch 386 : files 772 and next 2\n",
      "Batch 387 : files 774 and next 2\n",
      "Batch 388 : files 776 and next 2\n",
      "Batch 389 : files 778 and next 2\n",
      "Batch 390 : files 780 and next 2\n",
      "Batch 391 : files 782 and next 2\n",
      "Batch 392 : files 784 and next 2\n",
      "Batch 393 : files 786 and next 2\n",
      "Batch 394 : files 788 and next 2\n",
      "Batch 395 : files 790 and next 2\n",
      "Batch 396 : files 792 and next 2\n",
      "Batch 397 : files 794 and next 2\n",
      "Batch 398 : files 796 and next 2\n",
      "Batch 399 : files 798 and next 2\n",
      "Batch 400 : files 800 and next 2\n",
      "Batch 401 : files 802 and next 2\n",
      "Batch 402 : files 804 and next 2\n",
      "Batch 403 : files 806 and next 2\n",
      "Batch 404 : files 808 and next 2\n",
      "Batch 405 : files 810 and next 2\n",
      "Batch 406 : files 812 and next 2\n",
      "Batch 407 : files 814 and next 2\n",
      "Batch 408 : files 816 and next 2\n",
      "Batch 409 : files 818 and next 2\n",
      "Batch 410 : files 820 and next 2\n",
      "Batch 411 : files 822 and next 2\n",
      "Batch 412 : files 824 and next 2\n",
      "Batch 413 : files 826 and next 2\n",
      "Batch 414 : files 828 and next 2\n",
      "Batch 415 : files 830 and next 2\n",
      "Batch 416 : files 832 and next 2\n",
      "Batch 417 : files 834 and next 2\n",
      "Batch 418 : files 836 and next 2\n",
      "Batch 419 : files 838 and next 2\n",
      "Batch 420 : files 840 and next 2\n",
      "Batch 421 : files 842 and next 2\n",
      "Batch 422 : files 844 and next 2\n",
      "Batch 423 : files 846 and next 2\n",
      "Batch 424 : files 848 and next 2\n",
      "Batch 425 : files 850 and next 2\n",
      "Batch 426 : files 852 and next 2\n",
      "Batch 427 : files 854 and next 2\n",
      "Batch 428 : files 856 and next 2\n",
      "Batch 429 : files 858 and next 2\n",
      "Batch 430 : files 860 and next 2\n",
      "Batch 431 : files 862 and next 2\n",
      "Batch 432 : files 864 and next 2\n",
      "Batch 433 : files 866 and next 2\n",
      "Batch 434 : files 868 and next 2\n",
      "Batch 435 : files 870 and next 2\n",
      "Batch 436 : files 872 and next 2\n",
      "Batch 437 : files 874 and next 2\n",
      "Batch 438 : files 876 and next 2\n",
      "Batch 439 : files 878 and next 2\n",
      "Batch 440 : files 880 and next 2\n",
      "Batch 441 : files 882 and next 2\n",
      "Batch 442 : files 884 and next 2\n",
      "Batch 443 : files 886 and next 2\n",
      "Batch 444 : files 888 and next 2\n",
      "Batch 445 : files 890 and next 2\n",
      "Batch 446 : files 892 and next 2\n",
      "Batch 447 : files 894 and next 2\n",
      "Batch 448 : files 896 and next 2\n",
      "Batch 449 : files 898 and next 2\n",
      "Batch 450 : files 900 and next 2\n",
      "Batch 451 : files 902 and next 2\n",
      "Batch 452 : files 904 and next 2\n",
      "Batch 453 : files 906 and next 2\n",
      "Batch 454 : files 908 and next 2\n",
      "Batch 455 : files 910 and next 2\n",
      "Batch 456 : files 912 and next 2\n",
      "Batch 457 : files 914 and next 2\n",
      "Batch 458 : files 916 and next 2\n",
      "Batch 459 : files 918 and next 2\n",
      "Batch 460 : files 920 and next 2\n",
      "Batch 461 : files 922 and next 2\n",
      "Batch 462 : files 924 and next 2\n",
      "Batch 463 : files 926 and next 2\n",
      "Batch 464 : files 928 and next 2\n",
      "Batch 465 : files 930 and next 2\n",
      "Batch 466 : files 932 and next 2\n",
      "Batch 467 : files 934 and next 2\n",
      "Batch 468 : files 936 and next 2\n",
      "Batch 469 : files 938 and next 2\n",
      "Batch 470 : files 940 and next 2\n",
      "Batch 471 : files 942 and next 2\n",
      "Batch 472 : files 944 and next 2\n",
      "Batch 473 : files 946 and next 2\n",
      "Batch 474 : files 948 and next 2\n",
      "Batch 475 : files 950 and next 2\n",
      "Batch 476 : files 952 and next 2\n",
      "Batch 477 : files 954 and next 2\n",
      "Batch 478 : files 956 and next 2\n",
      "Batch 479 : files 958 and next 2\n",
      "Batch 480 : files 960 and next 2\n",
      "Batch 481 : files 962 and next 2\n",
      "Batch 482 : files 964 and next 2\n",
      "Batch 483 : files 966 and next 2\n",
      "Batch 484 : files 968 and next 2\n",
      "Batch 485 : files 970 and next 2\n",
      "Batch 486 : files 972 and next 2\n",
      "Batch 487 : files 974 and next 2\n",
      "Batch 488 : files 976 and next 2\n",
      "Batch 489 : files 978 and next 2\n",
      "Batch 490 : files 980 and next 2\n",
      "Batch 491 : files 982 and next 2\n",
      "Batch 492 : files 984 and next 2\n",
      "Batch 493 : files 986 and next 2\n",
      "Batch 494 : files 988 and next 2\n",
      "Batch 495 : files 990 and next 2\n",
      "Batch 496 : files 992 and next 2\n",
      "Batch 497 : files 994 and next 2\n",
      "Batch 498 : files 996 and next 2\n",
      "Batch 499 : files 998 and next 2\n",
      "Batch 500 : files 1000 and next 2\n",
      "Batch 501 : files 1002 and next 2\n",
      "Batch 502 : files 1004 and next 2\n",
      "Batch 503 : files 1006 and next 2\n",
      "Batch 504 : files 1008 and next 2\n",
      "Batch 505 : files 1010 and next 2\n",
      "Batch 506 : files 1012 and next 2\n",
      "Batch 507 : files 1014 and next 2\n",
      "Batch 508 : files 1016 and next 2\n",
      "Batch 509 : files 1018 and next 2\n",
      "Batch 510 : files 1020 and next 2\n",
      "Batch 511 : files 1022 and next 2\n",
      "Batch 512 : files 1024 and next 2\n",
      "Batch 513 : files 1026 and next 2\n",
      "Batch 514 : files 1028 and next 2\n",
      "Batch 515 : files 1030 and next 2\n",
      "Batch 516 : files 1032 and next 2\n",
      "Batch 517 : files 1034 and next 2\n",
      "Batch 518 : files 1036 and next 2\n",
      "Batch 519 : files 1038 and next 2\n",
      "Batch 520 : files 1040 and next 2\n",
      "Batch 521 : files 1042 and next 2\n",
      "Batch 522 : files 1044 and next 2\n",
      "Batch 523 : files 1046 and next 2\n",
      "Batch 524 : files 1048 and next 2\n",
      "Batch 525 : files 1050 and next 2\n",
      "Batch 526 : files 1052 and next 2\n",
      "Batch 527 : files 1054 and next 2\n",
      "Batch 528 : files 1056 and next 2\n",
      "Batch 529 : files 1058 and next 2\n",
      "Batch 530 : files 1060 and next 2\n",
      "Batch 531 : files 1062 and next 2\n",
      "Batch 532 : files 1064 and next 2\n",
      "Batch 533 : files 1066 and next 2\n",
      "Batch 534 : files 1068 and next 2\n",
      "Batch 535 : files 1070 and next 2\n",
      "Batch 536 : files 1072 and next 2\n",
      "Batch 537 : files 1074 and next 2\n",
      "Batch 538 : files 1076 and next 2\n",
      "Batch 539 : files 1078 and next 2\n",
      "Batch 540 : files 1080 and next 2\n",
      "size issue : skip this file test 155746.245 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\\49da580656e51214d59702a1d983eff143af3560a344f524fe86326c53fb5ddb.json\n",
      "*** error: len(prediction_labels)=1, len(hash_names)=2\n",
      "Warning: hash_val '4acf52ccc134f2086fafd2c3005e8890edd9e6e5c7a8a7fdb37566e23fa141e4' not found in df_all_labels.\n",
      "Batch 541 : files 1082 and next 2\n",
      "Batch 542 : files 1084 and next 2\n",
      "Batch 543 : files 1086 and next 2\n",
      "Batch 544 : files 1088 and next 2\n",
      "Batch 545 : files 1090 and next 2\n",
      "Batch 546 : files 1092 and next 2\n",
      "Batch 547 : files 1094 and next 2\n",
      "Batch 548 : files 1096 and next 2\n",
      "Batch 549 : files 1098 and next 2\n",
      "Batch 550 : files 1100 and next 2\n",
      "Batch 551 : files 1102 and next 2\n",
      "Batch 552 : files 1104 and next 2\n",
      "Batch 553 : files 1106 and next 2\n",
      "Batch 554 : files 1108 and next 2\n",
      "Batch 555 : files 1110 and next 2\n",
      "Batch 556 : files 1112 and next 2\n",
      "Batch 557 : files 1114 and next 2\n",
      "Batch 558 : files 1116 and next 2\n",
      "Batch 559 : files 1118 and next 2\n",
      "Batch 560 : files 1120 and next 2\n",
      "Batch 561 : files 1122 and next 2\n",
      "Batch 562 : files 1124 and next 2\n",
      "Batch 563 : files 1126 and next 2\n",
      "Batch 564 : files 1128 and next 2\n",
      "Batch 565 : files 1130 and next 2\n",
      "Batch 566 : files 1132 and next 2\n",
      "Batch 567 : files 1134 and next 2\n",
      "Batch 568 : files 1136 and next 2\n",
      "Batch 569 : files 1138 and next 2\n",
      "Batch 570 : files 1140 and next 2\n",
      "Batch 571 : files 1142 and next 2\n",
      "Batch 572 : files 1144 and next 2\n",
      "Batch 573 : files 1146 and next 2\n",
      "Batch 574 : files 1148 and next 2\n",
      "Batch 575 : files 1150 and next 2\n",
      "Batch 576 : files 1152 and next 2\n",
      "Batch 577 : files 1154 and next 2\n",
      "Batch 578 : files 1156 and next 2\n",
      "Batch 579 : files 1158 and next 2\n",
      "Batch 580 : files 1160 and next 2\n",
      "Batch 581 : files 1162 and next 2\n",
      "Batch 582 : files 1164 and next 2\n",
      "Batch 583 : files 1166 and next 2\n",
      "Batch 584 : files 1168 and next 2\n",
      "Batch 585 : files 1170 and next 2\n",
      "Batch 586 : files 1172 and next 2\n",
      "Batch 587 : files 1174 and next 2\n",
      "Batch 588 : files 1176 and next 2\n",
      "Batch 589 : files 1178 and next 2\n",
      "Batch 590 : files 1180 and next 2\n",
      "Batch 591 : files 1182 and next 2\n",
      "Batch 592 : files 1184 and next 2\n",
      "Batch 593 : files 1186 and next 2\n",
      "Batch 594 : files 1188 and next 2\n",
      "Batch 595 : files 1190 and next 2\n",
      "Batch 596 : files 1192 and next 2\n",
      "Batch 597 : files 1194 and next 2\n",
      "Batch 598 : files 1196 and next 2\n",
      "Batch 599 : files 1198 and next 2\n",
      "Batch 600 : files 1200 and next 2\n",
      "Batch 601 : files 1202 and next 2\n",
      "Batch 602 : files 1204 and next 2\n",
      "Batch 603 : files 1206 and next 2\n",
      "Batch 604 : files 1208 and next 2\n",
      "Batch 605 : files 1210 and next 2\n",
      "Batch 606 : files 1212 and next 2\n",
      "Batch 607 : files 1214 and next 2\n",
      "Batch 608 : files 1216 and next 2\n",
      "Batch 609 : files 1218 and next 2\n",
      "Batch 610 : files 1220 and next 2\n",
      "Batch 611 : files 1222 and next 2\n",
      "Batch 612 : files 1224 and next 2\n",
      "Batch 613 : files 1226 and next 2\n",
      "Batch 614 : files 1228 and next 2\n",
      "Batch 615 : files 1230 and next 2\n",
      "Batch 616 : files 1232 and next 2\n",
      "Batch 617 : files 1234 and next 2\n",
      "Batch 618 : files 1236 and next 2\n",
      "Batch 619 : files 1238 and next 2\n",
      "Batch 620 : files 1240 and next 2\n",
      "Batch 621 : files 1242 and next 2\n",
      "Batch 622 : files 1244 and next 2\n",
      "Batch 623 : files 1246 and next 2\n",
      "Batch 624 : files 1248 and next 2\n",
      "Batch 625 : files 1250 and next 2\n",
      "Batch 626 : files 1252 and next 2\n",
      "Batch 627 : files 1254 and next 2\n",
      "Batch 628 : files 1256 and next 2\n",
      "Batch 629 : files 1258 and next 2\n",
      "Batch 630 : files 1260 and next 2\n",
      "Batch 631 : files 1262 and next 2\n",
      "Batch 632 : files 1264 and next 2\n",
      "Batch 633 : files 1266 and next 2\n",
      "Batch 634 : files 1268 and next 2\n",
      "Batch 635 : files 1270 and next 2\n",
      "Batch 636 : files 1272 and next 2\n",
      "Batch 637 : files 1274 and next 2\n",
      "Batch 638 : files 1276 and next 2\n",
      "Batch 639 : files 1278 and next 2\n",
      "Batch 640 : files 1280 and next 2\n",
      "Batch 641 : files 1282 and next 2\n",
      "Batch 642 : files 1284 and next 2\n",
      "Batch 643 : files 1286 and next 2\n",
      "Batch 644 : files 1288 and next 2\n",
      "Batch 645 : files 1290 and next 2\n",
      "Batch 646 : files 1292 and next 2\n",
      "Batch 647 : files 1294 and next 2\n",
      "Batch 648 : files 1296 and next 2\n",
      "Batch 649 : files 1298 and next 2\n",
      "Batch 650 : files 1300 and next 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test run: {df_test_init.shape[1]-1} labels to predict for {df_test_init.shape[0]} graphs \\nfrom files located: {test_path_dir}\\n\")\n",
    "\n",
    "batch_data_size = 2\n",
    "batch_files_size = 2\n",
    "num_feature_dim = 15\n",
    "\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "#init test\n",
    "df_all_labels = df_test_init\n",
    "list_test_hash = []\n",
    "#all_pred_labels = pd.DataFrame(columns=col_to_predict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pour tous les hash du fichiers test\n",
    "    print(f\"**** Dataset Processing for Test\")\n",
    "    count_batch = 0  \n",
    "    for count_test in range(0, len(df_test_init), batch_files_size):\n",
    "      \n",
    "        hash_names = df_test_init.iloc[count_test:count_test + batch_files_size]['name'].to_list()\n",
    "        print(f\"Batch {count_batch} : files {count_test} and next {batch_files_size}\")\n",
    "        full_file_test_name = [os.path.join(test_path_dir, file+'.json') for file in hash_names]\n",
    "        #print(full_file_test_name)\n",
    "\n",
    "        batch_graph_list = process_batch(full_file_test_name)\n",
    "        #print(\"2 :\",len(batch_graph_list))\n",
    "        tokenizer.fit(batch_graph_list)\n",
    "\n",
    "        test_dataset = AssemblyGraphDataset(batch_graph_list, tokenizer,  node_feature_dim=num_feature_dim)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_data_size, shuffle=False)\n",
    "        \n",
    "        # loop on the batch from the dataloader\n",
    "        for batch in test_dataloader:\n",
    "            batch_hash_list = hash_names\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            # Create batch vector for global pooling\n",
    "            num_graphs = batch.num_graphs #obtain the number of graphs in the batch\n",
    "            #print(\"3:\", num_graphs)\n",
    "            num_nodes_per_graph = torch.bincount(batch.batch).tolist() # get number of nodes per graph.\n",
    "            if any(num_nodes_per_graph): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(torch.bincount(batch.batch).tolist())])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "\n",
    "            if len(prediction_labels) != len(batch_hash_list):\n",
    "                print(f\"*** error: len(prediction_labels)={len(prediction_labels)}, len(hash_names)={len(batch_hash_list)}\")\n",
    "\n",
    "            for i, hash_val in enumerate(batch_hash_list):\n",
    "                row_index = df_all_labels[df_all_labels['name'] == hash_val].index\n",
    "                if (not row_index.empty) and (i < len(prediction_labels)):\n",
    "                    \n",
    "                    label_list = prediction_labels[i].tolist()  # Convert tensor row to list\n",
    "                    int_label_list = [int(val) for val in label_list]  # Convert to int\n",
    "                    # Update the DataFrame\n",
    "                    for j in range(len(int_label_list)):\n",
    "                        df_all_labels.iloc[row_index, j+1] = int_label_list[j]\n",
    "                else:\n",
    "                    print(f\"Warning: hash_val '{hash_val}' not found in df_all_labels.\")\n",
    "\n",
    "        count_batch += 1\n",
    "\n",
    "    print(f\"Test end: nb pred labels: {len(df_all_labels)} \")\n",
    "    df_all_labels.to_csv(filename_test_results, header=True ,index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 454)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06a62f5604e232041634ef91c3a01bf7e543f055e62a64...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57f3c73b47ff31a26e05317c8680fa8dfa359c3e38b054...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6da7aa61a776ef6e84eeb2e9cf76df84059054d7abd7f1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b23acaa5391daaf488a69d62ac4400cc7858591738bc8a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c0065a36127f18e6606d92cb86c5786689137ad4313d70...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...   \n",
       "1  4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...   \n",
       "2  2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...   \n",
       "3  243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...   \n",
       "4  533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...   \n",
       "5  06a62f5604e232041634ef91c3a01bf7e543f055e62a64...   \n",
       "6  57f3c73b47ff31a26e05317c8680fa8dfa359c3e38b054...   \n",
       "7  6da7aa61a776ef6e84eeb2e9cf76df84059054d7abd7f1...   \n",
       "8  b23acaa5391daaf488a69d62ac4400cc7858591738bc8a...   \n",
       "9  c0065a36127f18e6606d92cb86c5786689137ad4313d70...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                NaN     NaN         NaN   \n",
       "1                                NaN     NaN         NaN   \n",
       "2                                NaN     NaN         NaN   \n",
       "3                                NaN     NaN         NaN   \n",
       "4                                NaN     NaN         NaN   \n",
       "5                                NaN     NaN         NaN   \n",
       "6                                NaN     NaN         NaN   \n",
       "7                                NaN     NaN         NaN   \n",
       "8                                NaN     NaN         NaN   \n",
       "9                                NaN     NaN         NaN   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                            NaN                           NaN   \n",
       "1                            NaN                           NaN   \n",
       "2                            NaN                           NaN   \n",
       "3                            NaN                           NaN   \n",
       "4                            NaN                           NaN   \n",
       "5                            NaN                           NaN   \n",
       "6                            NaN                           NaN   \n",
       "7                            NaN                           NaN   \n",
       "8                            NaN                           NaN   \n",
       "9                            NaN                           NaN   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                NaN                 NaN                  NaN   \n",
       "1                NaN                 NaN                  NaN   \n",
       "2                NaN                 NaN                  NaN   \n",
       "3                NaN                 NaN                  NaN   \n",
       "4                NaN                 NaN                  NaN   \n",
       "5                NaN                 NaN                  NaN   \n",
       "6                NaN                 NaN                  NaN   \n",
       "7                NaN                 NaN                  NaN   \n",
       "8                NaN                 NaN                  NaN   \n",
       "9                NaN                 NaN                  NaN   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "1              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "2              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "3              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "4              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "5              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "6              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "7              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "8              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "9              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                   NaN                  NaN                    NaN   \n",
       "1                   NaN                  NaN                    NaN   \n",
       "2                   NaN                  NaN                    NaN   \n",
       "3                   NaN                  NaN                    NaN   \n",
       "4                   NaN                  NaN                    NaN   \n",
       "5                   NaN                  NaN                    NaN   \n",
       "6                   NaN                  NaN                    NaN   \n",
       "7                   NaN                  NaN                    NaN   \n",
       "8                   NaN                  NaN                    NaN   \n",
       "9                   NaN                  NaN                    NaN   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0         NaN       NaN   NaN  \n",
       "1         NaN       NaN   NaN  \n",
       "2         NaN       NaN   NaN  \n",
       "3         NaN       NaN   NaN  \n",
       "4         NaN       NaN   NaN  \n",
       "5         NaN       NaN   NaN  \n",
       "6         NaN       NaN   NaN  \n",
       "7         NaN       NaN   NaN  \n",
       "8         NaN       NaN   NaN  \n",
       "9         NaN       NaN   NaN  \n",
       "\n",
       "[10 rows x 454 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_all_labels.shape)\n",
    "df_all_labels.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
