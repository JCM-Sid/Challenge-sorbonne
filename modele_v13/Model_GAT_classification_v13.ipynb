{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Sorbonne - DST\n",
    "# Module entrainement à la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3523,
     "status": "ok",
     "timestamp": 1743022078234,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "59fbdd14-a668-45e3-a568-95abf69ed525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset, Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3356,
     "status": "ok",
     "timestamp": 1743022081591,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "21408444-d6e8-4d4b-9845-5f96c313f686"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init Inputs files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1743022082826,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "129bed9b-247d-42e9-f5ed-786a494b04c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/content': # Google Colab\n",
    "    training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "\n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/split_training-colab-v12.csv'\n",
    "    filename_trained =      '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/list_hash_trained.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    #test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_set_to_predict.csv'\n",
    "    #filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v13\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    #filename_trained = r\"..\\Pred_test_v13\\list_hash_trained.csv\" \n",
    "    filename_trained = r\"C:\\Users\\jch_m\\OneDrive\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_training-pc-v13.csv'\n",
    "\n",
    "    #model_save_file =  r'..\\Pred_test_v13\\model_sorbonne_weights.pth'\n",
    "    model_save_file = r'C:\\Users\\jch_m\\OneDrive\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v12\": #PC2\n",
    "    split_char = '\\\\'\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    #filename_trained = r\"..\\Pred_test_v13\\list_hash_trained.csv\" \n",
    "    filename_trained = r\"C:\\Users\\jch_m\\OneDrive\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_training-pc-v13.csv'\n",
    "\n",
    "    #model_save_file =  r'..\\Pred_test_v13\\model_sorbonne_weights.pth'\n",
    "    model_save_file = r'C:\\Users\\jch_m\\OneDrive\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "# read meta data CSV input and save in df\n",
    "random_seed = 42\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "df_meta_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1743022082873,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "6a0d2500-7f21-447d-ba57-3dc495d196bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Pred_test_v13\\list_hash_trained.csv\n",
      "taille : 183\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86f133e9201e6131cae209f5b3c67f48cd5e23666b9ee0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3c72731f12ebb779bc4c5f0e05a62ca7785d82fe852b2d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414cb126efb9b3e0520ff6504ad2448e383aa284aeca01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73a82d73d442107e3be9f1b10142f9f780746e14e750e3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "1  86f133e9201e6131cae209f5b3c67f48cd5e23666b9ee0...\n",
       "2  3c72731f12ebb779bc4c5f0e05a62ca7785d82fe852b2d...\n",
       "3  414cb126efb9b3e0520ff6504ad2448e383aa284aeca01...\n",
       "4  73a82d73d442107e3be9f1b10142f9f780746e14e750e3..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1743022082888,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "ec6a1c40-769e-461e-8442-c9e8cba7be7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0' 'rep_500' 'rep_1000']\n",
      "['G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\61c0810a23580cf492a6ba4f7654566108331e7a4134c968c2d6a05261b2d8a1.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\de76670dc04153703f7d346fdcd0b13af2dd1a7eaae3e494f7b0777cdf97533d.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\44edcf113309783b1af298ce908c7546c0e48b7699a23ef5e9fd6228fc973b59.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\652b78411eeb580f14d4f6ce5c22b5c931b18464ed3f8d5fdd03112ae2c04d59.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\0303959c78d21b55fc3421ef30a80da761a5a5e2a6f8f844fdadfbd41e7f1c50.json']\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_500']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique()[:3])\n",
    "print(full_file_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1743022082938,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0', max_file=0):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    count_files = 0\n",
    "\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "            #print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 100_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743022082946,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "84w6HNzrOKjz",
    "outputId": "618e03b9-f421-4306-eb80-ebecfa7c12e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "Number of files with error: 1\n",
      "2 9 1\n"
     ]
    }
   ],
   "source": [
    "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch='rep_500',max_file=10)\n",
    "print(len(list_val), len(list_train), len(list_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init modele et fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1743022083004,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "\n",
    "class AssemblyFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.max_registers_to_embed = 3  # Embed up to 3 registers\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "        # Set to store all encountered registers for vocabulary building\n",
    "        self.all_registers = set()\n",
    "\n",
    "    def tokenize_line(self, line):\n",
    "        tokens = {}\n",
    "        if match := self.patterns['node_type'].search(line):\n",
    "            tokens['node_type'] = match.group(1)\n",
    "        if match := self.patterns['operation'].search(line):\n",
    "            tokens['operation'] = match.group(1)\n",
    "        if matches := self.patterns['registers'].findall(line):\n",
    "            tokens['registers'] = [reg.lower() for reg in matches]\n",
    "            self.all_registers.update(tokens['registers'])\n",
    "        if match := self.patterns['memory_ref'].search(line):\n",
    "            tokens['memory_ref'] = match.group(1)\n",
    "        if match := self.patterns['immediate'].search(line):\n",
    "            tokens['immediate'] = match.group(0)\n",
    "        return tokens\n",
    "\n",
    "    def extract_features(self, digraph):\n",
    "        all_instruction_features = {}\n",
    "        for node_id, node_data in digraph.nodes(data=True):\n",
    "            label = node_data.get('label', '')\n",
    "            instruction_features_list = []\n",
    "            instructions = label.split('\\n')\n",
    "            for i, line in enumerate(instructions):\n",
    "                tokens = self.tokenize_line(line)\n",
    "                instruction_features = {}\n",
    "\n",
    "                if 'operation' in tokens:\n",
    "                    instruction_features['operation'] = tokens['operation']\n",
    "                if 'node_type' in tokens:\n",
    "                    instruction_features['node_type'] = tokens['node_type']\n",
    "                if 'memory_ref' in tokens:\n",
    "                    instruction_features['memory_ref'] = tokens['memory_ref']\n",
    "                if 'immediate' in tokens:\n",
    "                    instruction_features['immediate'] = tokens['immediate']\n",
    "\n",
    "                # Binary features for register presence (before vocabulary is built)\n",
    "                for reg in self.all_registers:\n",
    "                    instruction_features[f'has_register_{reg}'] = 1 if 'registers' in tokens and reg in tokens['registers'] else 0\n",
    "\n",
    "                # Position in the sequence\n",
    "                instruction_features['position'] = i\n",
    "                instruction_features_list.append(instruction_features)\n",
    "\n",
    "            if len(instruction_features_list) == 1:\n",
    "                all_instruction_features[node_id] = instruction_features_list[0]\n",
    "            else:\n",
    "                all_instruction_features[node_id] = instruction_features_list\n",
    "        return all_instruction_features\n",
    "\n",
    "    def build_vocabularies(self, input_data_list):\n",
    "        node_types = set()\n",
    "        operations = set()\n",
    "        registers = set()\n",
    "        memory_patterns = set()\n",
    "        immediates = set()\n",
    "\n",
    "        for item in input_data_list:\n",
    "            if isinstance(item, nx.DiGraph):\n",
    "                graph = item\n",
    "                for _, node_data in graph.nodes(data=True):\n",
    "                    label = node_data.get('label', '')\n",
    "                    for line in label.split('\\n'):\n",
    "                        tokens = self.tokenize_line(line)\n",
    "                        if 'node_type' in tokens:\n",
    "                            node_types.add(tokens['node_type'])\n",
    "                        if 'operation' in tokens:\n",
    "                            operations.add(tokens['operation'])\n",
    "                        if 'registers' in tokens:\n",
    "                            registers.update(tokens['registers'])\n",
    "                        if 'memory_ref' in tokens:\n",
    "                            memory_patterns.add(tokens['memory_ref'])\n",
    "                        if 'immediate' in tokens:\n",
    "                            immediates.add(tokens['immediate'])\n",
    "            elif isinstance(item, str):\n",
    "                assembly_code = item\n",
    "                for line in assembly_code.strip().split('\\n'):\n",
    "                    tokens = self.tokenize_line(line)\n",
    "                    if 'node_type' in tokens:\n",
    "                        node_types.add(tokens['node_type'])\n",
    "                    if 'operation' in tokens:\n",
    "                        operations.add(tokens['operation'])\n",
    "                    if 'registers' in tokens:\n",
    "                        registers.update(tokens['registers'])\n",
    "                    if 'memory_ref' in tokens:\n",
    "                        memory_patterns.add(tokens['memory_ref'])\n",
    "                    if 'immediate' in tokens:\n",
    "                        immediates.add(tokens['immediate'])\n",
    "\n",
    "        self.node_type_vocab = {token: i + 1 for i, token in enumerate(sorted(list(node_types)))}\n",
    "        self.node_type_vocab[self.UNK_TOKEN] = 0\n",
    "        self.operation_vocab = {token: i + 1 for i, token in enumerate(sorted(list(operations)))}\n",
    "        self.operation_vocab[self.UNK_TOKEN] = 0\n",
    "        self.register_vocab = {token: i + 1 for i, token in enumerate(sorted(list(registers)))}\n",
    "        self.register_vocab[self.UNK_TOKEN] = 0\n",
    "        self.memory_pattern_vocab = {token: i + 1 for i, token in enumerate(sorted(list(memory_patterns)))}\n",
    "        self.memory_pattern_vocab[self.UNK_TOKEN] = 0\n",
    "        self.immediate_vocab = {token: i + 1 for i, token in enumerate(sorted(list(immediates)))}\n",
    "        self.immediate_vocab[self.UNK_TOKEN] = 0\n",
    "\n",
    "    def embed_instruction(self, instruction_features, embedding_dim=60):\n",
    "        \"\"\"Embeds a single assembly instruction feature dictionary.\"\"\"\n",
    "        embedding = []\n",
    "\n",
    "        # Embed operation (if present)\n",
    "        op_embedding = np.zeros(embedding_dim)\n",
    "        if 'operation' in instruction_features and instruction_features['operation'] in self.operation_vocab:\n",
    "            op_index = self.operation_vocab[instruction_features['operation']]\n",
    "            op_embedding[op_index % embedding_dim] = 1.0  # Simple one-hot-like hashing\n",
    "        embedding.extend(op_embedding)\n",
    "\n",
    "        # Embed node type (if present)\n",
    "        node_type_embedding = np.zeros(embedding_dim // 2)\n",
    "        if 'node_type' in instruction_features and instruction_features['node_type'] in self.node_type_vocab:\n",
    "            node_type_index = self.node_type_vocab[instruction_features['node_type']]\n",
    "            node_type_embedding[node_type_index % (embedding_dim // 2)] = 1.0\n",
    "        embedding.extend(node_type_embedding)\n",
    "\n",
    "        # Embed registers (up to max_registers_to_embed)\n",
    "        registers_present = [reg.split('_')[-1] for reg in instruction_features if reg.startswith('has_register_') and instruction_features[reg] == 1]\n",
    "        register_embedding = np.zeros(self.max_registers_to_embed * (embedding_dim // 3)) # Allocate space\n",
    "        for i, reg in enumerate(registers_present[:self.max_registers_to_embed]):\n",
    "            if reg in self.register_vocab:\n",
    "                reg_index = self.register_vocab[reg]\n",
    "                register_embedding[i * (embedding_dim // 3) + (reg_index % (embedding_dim // 3))] = 1.0\n",
    "        embedding.extend(register_embedding)\n",
    "\n",
    "        # Embed memory pattern (if present)\n",
    "        mem_embedding = np.zeros(embedding_dim // 2)\n",
    "        if 'memory_ref' in instruction_features and instruction_features['memory_ref'] in self.memory_pattern_vocab:\n",
    "            mem_index = self.memory_pattern_vocab[instruction_features['memory_ref']]\n",
    "            mem_embedding[mem_index % (embedding_dim // 2)] = 1.0\n",
    "        embedding.extend(mem_embedding)\n",
    "\n",
    "        # Embed immediate (just presence for now)\n",
    "        has_immediate = 1 if 'immediate' in instruction_features else 0\n",
    "        embedding.append(has_immediate)\n",
    "\n",
    "        # Position\n",
    "        position = instruction_features.get('position', 0)\n",
    "        embedding.append(position / 10.0) # Normalize position\n",
    "\n",
    "        return np.array(embedding[:60]) # Truncate to max 60 dimensions\n",
    "\n",
    "    def embed_graph(self, digraph):\n",
    "        \"\"\"Embeds all instructions within the nodes of a digraph.\"\"\"\n",
    "        embedded_nodes = {}\n",
    "        raw_features = self.extract_features(digraph)\n",
    "        for node_id, features in raw_features.items():\n",
    "            if isinstance(features, list):  # Multiple instructions in a node\n",
    "                embedded_nodes[node_id] = [self.embed_instruction(f) for f in features]\n",
    "            else:  # Single instruction in a node\n",
    "                embedded_nodes[node_id] = self.embed_instruction(features)\n",
    "        return embedded_nodes\n",
    "\n",
    "class AssemblyGraphDataset(Dataset):\n",
    "    def __init__(self, graph_list, feature_extractor, hash_dim=512):\n",
    "        self.graph_list = graph_list\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.hash_dim = hash_dim\n",
    "        self.processed_data = self._process_all_graphs()\n",
    "\n",
    "    def _process_all_graphs(self):\n",
    "        processed_data = []\n",
    "        for graph_item in self.graph_list: # Iterate through the list of graph items\n",
    "            graph = graph_item # Access the NetworkX graph from the dictionary\n",
    "            embedded_nodes = self.feature_extractor.embed_graph(graph)\n",
    "\n",
    "            # Create PyTorch Geometric Data object\n",
    "            node_features = []\n",
    "            node_order = sorted(graph.nodes()) # Ensure consistent node order\n",
    "            for node_id in node_order:\n",
    "                embedding = embedded_nodes.get(node_id)\n",
    "                if isinstance(embedding, list):\n",
    "                    # For simplicity, if a node has multiple instructions, we take the first embedding\n",
    "                    # You might want a more sophisticated way to handle this (e.g., averaging, using RNN)\n",
    "                    node_features.append(embedding[0])\n",
    "                elif embedding is not None:\n",
    "                    node_features.append(embedding)\n",
    "                else:\n",
    "                    node_features.append(np.zeros(60)) # Handle missing embeddings\n",
    "\n",
    "            # Get edges as a list of tuples\n",
    "            edges = list(graph.edges())\n",
    "            if not edges:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            else:\n",
    "                # Convert node labels to indices if they are not already numeric\n",
    "                node_to_index = {node: i for i, node in enumerate(graph.nodes())}\n",
    "                indexed_edges = [(node_to_index[u], node_to_index[v]) for u, v in edges]\n",
    "                edge_index = torch.tensor(indexed_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "            # Dummy hash encoding (replace with your actual hash encoding)\n",
    "            hash_encoded = torch.randn(len(node_features), self.hash_dim)\n",
    "\n",
    "            # Convert lists of numpy arrays to single numpy arrays\n",
    "            if isinstance(node_features, list):\n",
    "                node_features_np = np.array(node_features)\n",
    "            else:\n",
    "                node_features_np = node_features  # Already a numpy array\n",
    "\n",
    "            # Convert the numpy arrays to torch tensors\n",
    "            node_features_tensor = torch.tensor(node_features_np, dtype=torch.float)\n",
    "\n",
    "            data = Data(x=node_features_tensor,\n",
    "                        edge_index=edge_index,\n",
    "                        hash_encoded=hash_encoded)\n",
    "            processed_data.append(data)\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collates a list of PyTorch Geometric Data objects into a Batch object.\n",
    "    Args:\n",
    "        batch (list): A list of `torch_geometric.data.Data` objects.\n",
    "    Returns:\n",
    "        torch_geometric.data.Batch: A `Batch` object that combines the data\n",
    "            objects in the `batch`.\n",
    "    \"\"\"\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "# Define the AssemblyGAT model\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        x, edge_index, hash_encoded, batch = data.x, data.edge_index, data.hash_encoded, data.batch\n",
    "\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "#######################################\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    list_hash = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                digraph_str = f.read()\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "                f.close()\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append(G)\n",
    "                list_hash.append(hash_file)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr, list_hash\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(model, train_loader, y_data, val_loader=None, epochs=10, lr=0.001, weight_decay=5e-4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the AssemblyGAT model.\n",
    "    Args:\n",
    "        model (nn.Module): The AssemblyGAT model to train\n",
    "        train_loader (DataLoader): DataLoader containing training data\n",
    "        val_loader (DataLoader, optional): DataLoader containing validation data\n",
    "        epochs (int): Number of training epochs\n",
    "        lr (float): Learning rate\n",
    "        weight_decay (float): Weight decay for regularization\n",
    "        device (str): Device to train on ('cuda' or 'cpu')    \n",
    "    Returns:\n",
    "        model (nn.Module): The trained model\n",
    "        history (dict): Training history with loss and accuracy metrics\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            y_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch)\n",
    "            \n",
    "            # Assuming batch.y contains the labels\n",
    "            loss = criterion(output, y_data)\n",
    "            #print(output.shape, y_data.shape)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            _, true_labels = torch.max(y_data, 1)  # Get the class index from y_data\n",
    "            total += y_data.size(0)\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "    \"\"\"\n",
    "    Processes files in batches for tokenization and model training/evaluation in a single loop.\n",
    "    Args:\n",
    "        full_file_paths (list): List of full file paths to process.\n",
    "        batch_files_size (int): Number of files to process in each batch for tokenization.\n",
    "        batch_data_size (int): Batch size for the DataLoader during training/evaluation.\n",
    "        num_feature_dim (int, optional): Dimension of node features. Defaults to None.\n",
    "        mode (str): 'train' or other mode (e.g., 'eval'). Defaults to 'train'.\n",
    "        epochs (int): Number of training epochs per batch (if mode is 'train'). Defaults to 3.\n",
    "        tokenizer: Tokenizer object with methods like get_tokens_and_counts and fit_from_counts.\n",
    "        process_batch (callable): Function to process a batch of file paths and return a list of graph-like objects.\n",
    "        train_model (callable): Function to train the model with a DataLoader and other info.\n",
    "    \"\"\"\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) + 1\n",
    "\n",
    "    hash_t = []\n",
    "\n",
    "    print(\"**** Tokenization and Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        batch_num = i // batch_files_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list, hash_t = process_batch(batch_files)\n",
    "        if not batch_graph_list:\n",
    "            print(\"graph list is empty for this batch\")\n",
    "            continue\n",
    "        \n",
    "        #    print(hash_t)\n",
    "        hash_y_list = df_meta_train[df_meta_train['name'].isin(hash_t)].drop(columns=['name']).values\n",
    "        y_data_tensor = torch.tensor(hash_y_list, dtype=torch.float32)\n",
    "        \n",
    "        # Create the AssemblyGraphDataset # Create the DataLoader\n",
    "        extractor.build_vocabularies(batch_graph_list)\n",
    "        assembly_dataset = AssemblyGraphDataset(batch_graph_list, extractor)\n",
    "        dataloader = DataLoader(assembly_dataset, batch_size=batch_data_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        # Train model\n",
    "        trained_model, history = train_model(\n",
    "            model, dataloader, y_data_tensor, epochs=epochs, lr=0.001, \n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        print(f\"Loss: {history['train_loss'][-3:]} Acc : {history['train_acc'][-3:]}\")\n",
    "        \n",
    "        # Save the model\n",
    "        torch.save(trained_model.state_dict(), model_save_file)\n",
    "        # Sauver les hashs entrainés\n",
    "        with open(filename_trained, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for item in hash_t:\n",
    "                writer.writerow([item])\n",
    "            csvfile.close()\n",
    "    \n",
    "    return hash_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Train for Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avU1v3vVtSu-",
    "outputId": "1c290e1b-31d3-4874-b0af-921df5e29187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch: rep_0 ========== \n",
      "Number of files with error: 0\n",
      "There are 68 files for train\n",
      "**** Tokenization and Dataset Processing & Training\n",
      "Processing batch 1/69: 1 files\n",
      "graph list is empty for this batch\n",
      "Processing batch 2/69: 1 files\n",
      "Loss: [567.6217651367188, 564.8442993164062, 561.700927734375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 3/69: 1 files\n",
      "Loss: [323.8314208984375, 321.2229919433594, 318.3843688964844] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 4/69: 1 files\n",
      "Loss: [144.16510009765625, 142.29550170898438, 140.57269287109375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 5/69: 1 files\n",
      "Loss: [232.66976928710938, 229.7301025390625, 226.94032287597656] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 6/69: 1 files\n",
      "Loss: [212.0784912109375, 209.9432373046875, 207.92747497558594] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 7/69: 1 files\n",
      "Loss: [123.66917419433594, 121.96797943115234, 120.24020385742188] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 8/69: 1 files\n",
      "Loss: [82.20223999023438, 81.11927032470703, 80.05947875976562] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 9/69: 1 files\n",
      "Loss: [192.9576873779297, 190.77220153808594, 190.3850860595703] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 10/69: 1 files\n",
      "Loss: [257.7511291503906, 255.14053344726562, 252.8424072265625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 11/69: 1 files\n",
      "Loss: [309.38677978515625, 307.2725830078125, 305.37054443359375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 12/69: 1 files\n",
      "Loss: [227.2602996826172, 225.56129455566406, 223.8787078857422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 13/69: 1 files\n",
      "Loss: [327.14056396484375, 325.5130615234375, 323.8043518066406] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 14/69: 1 files\n",
      "Loss: [263.0711975097656, 261.42889404296875, 259.81097412109375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 15/69: 1 files\n",
      "Loss: [234.5520477294922, 233.0657501220703, 231.60736083984375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 16/69: 1 files\n",
      "Loss: [76.98619842529297, 76.51048278808594, 76.07139587402344] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 17/69: 1 files\n",
      "Loss: [154.7402801513672, 153.50282287597656, 152.07064819335938] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 18/69: 1 files\n",
      "Loss: [62.90655517578125, 62.40308380126953, 61.921077728271484] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 19/69: 1 files\n",
      "Loss: [191.04905700683594, 189.19161987304688, 188.34571838378906] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 20/69: 1 files\n",
      "Loss: [351.7964782714844, 350.4337463378906, 348.7908935546875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 21/69: 1 files\n",
      "Loss: [60.527320861816406, 59.84295654296875, 59.218753814697266] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 22/69: 1 files\n",
      "Loss: [57.263816833496094, 56.82278060913086, 56.39891815185547] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 23/69: 1 files\n",
      "Loss: [21.706634521484375, 21.455371856689453, 21.254085540771484] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 24/69: 1 files\n",
      "Loss: [186.3642578125, 184.19903564453125, 183.32199096679688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 25/69: 1 files\n",
      "Loss: [175.03839111328125, 174.31484985351562, 173.3693084716797] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 26/69: 1 files\n",
      "Loss: [78.07539367675781, 77.51062774658203, 76.79753875732422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 27/69: 1 files\n",
      "Loss: [62.65242385864258, 62.0885124206543, 61.715667724609375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 28/69: 1 files\n",
      "Loss: [211.80752563476562, 210.4840850830078, 209.20472717285156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 29/69: 1 files\n",
      "Loss: [28.452587127685547, 27.933170318603516, 27.587993621826172] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 30/69: 1 files\n",
      "Loss: [45.64362335205078, 45.03861999511719, 44.47938537597656] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 31/69: 1 files\n",
      "Loss: [101.78157806396484, 100.898681640625, 100.14241790771484] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 32/69: 1 files\n",
      "Loss: [53.24257278442383, 52.95851516723633, 52.46373748779297] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 33/69: 1 files\n",
      "Loss: [40.9649772644043, 40.473548889160156, 40.135772705078125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 34/69: 1 files\n",
      "Loss: [13.319124221801758, 12.995088577270508, 12.768747329711914] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 35/69: 1 files\n",
      "Loss: [335.68121337890625, 331.6197509765625, 328.9391784667969] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 36/69: 1 files\n",
      "Loss: [22.715030670166016, 22.44942855834961, 22.096712112426758] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 37/69: 1 files\n",
      "Loss: [38.541221618652344, 38.138877868652344, 37.765811920166016] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 38/69: 1 files\n",
      "Loss: [200.16525268554688, 198.75112915039062, 196.89813232421875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 39/69: 1 files\n",
      "Loss: [193.4844207763672, 192.11085510253906, 190.88107299804688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 40/69: 1 files\n",
      "Loss: [29.353540420532227, 29.017223358154297, 28.95366668701172] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 41/69: 1 files\n",
      "Loss: [275.9496154785156, 274.4837646484375, 272.9132995605469] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 42/69: 1 files\n",
      "Loss: [44.56391143798828, 44.15160369873047, 43.69744110107422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 43/69: 1 files\n",
      "Loss: [224.73284912109375, 223.60765075683594, 222.45399475097656] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 44/69: 1 files\n",
      "Loss: [217.06849670410156, 216.05345153808594, 215.0694580078125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 45/69: 1 files\n",
      "Loss: [331.1656494140625, 329.708984375, 328.4416809082031] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 46/69: 1 files\n",
      "Loss: [302.7176208496094, 300.98406982421875, 299.35882568359375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 47/69: 1 files\n",
      "Loss: [107.00091552734375, 106.28627014160156, 105.64861297607422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 48/69: 1 files\n",
      "Loss: [110.78157806396484, 111.02194213867188, 110.35787963867188] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 49/69: 1 files\n",
      "Loss: [116.18087005615234, 115.80394744873047, 114.2363052368164] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 50/69: 1 files\n",
      "Loss: [475.7074890136719, 473.2789306640625, 471.02008056640625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 51/69: 1 files\n",
      "Loss: [175.7517852783203, 174.81826782226562, 173.8719940185547] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 52/69: 1 files\n",
      "Loss: [284.5829772949219, 283.09112548828125, 281.4328918457031] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 53/69: 1 files\n",
      "Loss: [223.15013122558594, 221.99697875976562, 220.79708862304688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 54/69: 1 files\n",
      "Loss: [384.8333740234375, 383.2245788574219, 381.5884094238281] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 55/69: 1 files\n",
      "Loss: [262.73565673828125, 261.31365966796875, 259.91552734375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 56/69: 1 files\n",
      "Loss: [292.63287353515625, 291.19921875, 289.7982482910156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 57/69: 1 files\n",
      "Loss: [465.6950378417969, 463.8780517578125, 462.16064453125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 58/69: 1 files\n",
      "Loss: [347.4584045410156, 346.11962890625, 344.7394714355469] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 59/69: 1 files\n",
      "Loss: [147.73118591308594, 146.6217498779297, 145.6805419921875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 60/69: 1 files\n",
      "Loss: [32.95594024658203, 32.558631896972656, 32.193485260009766] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 61/69: 1 files\n",
      "Loss: [21.589069366455078, 21.373157501220703, 21.055294036865234] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 62/69: 1 files\n",
      "Loss: [27.94121551513672, 27.610971450805664, 27.217456817626953] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 63/69: 1 files\n",
      "Loss: [295.6490173339844, 293.62066650390625, 291.5668029785156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 64/69: 1 files\n",
      "Loss: [24.9475040435791, 24.928773880004883, 24.322158813476562] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 65/69: 1 files\n",
      "Loss: [55.82246017456055, 55.22890853881836, 54.62895584106445] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 66/69: 1 files\n",
      "Loss: [406.1819763183594, 403.7900085449219, 401.5965576171875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 67/69: 1 files\n",
      "Loss: [314.453857421875, 312.76446533203125, 311.08416748046875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 68/69: 1 files\n",
      "Loss: [239.67913818359375, 238.436279296875, 237.15452575683594] Acc : [0.0, 0.0, 0.0]\n",
      "************ End of training ***********************\n",
      "========== batch: rep_500 ========== \n",
      "Number of files with error: 0\n",
      "There are 499 files for train\n",
      "**** Tokenization and Dataset Processing & Training\n",
      "Processing batch 1/500: 1 files\n",
      "Loss: [402.2333984375, 400.6314697265625, 398.9719543457031] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 2/500: 1 files\n",
      "Loss: [41.224788665771484, 40.72669219970703, 40.155216217041016] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 3/500: 1 files\n",
      "Loss: [264.0272216796875, 262.5937194824219, 261.0591735839844] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 4/500: 1 files\n",
      "Loss: [394.8115234375, 392.98077392578125, 391.2063293457031] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 5/500: 1 files\n",
      "Loss: [60.887081146240234, 60.20557403564453, 59.638328552246094] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 6/500: 1 files\n",
      "Loss: [49.532806396484375, 49.092777252197266, 48.664939880371094] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 7/500: 1 files\n",
      "Loss: [104.42330169677734, 102.42566680908203, 100.44204711914062] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 8/500: 1 files\n",
      "Loss: [151.43994140625, 150.3328857421875, 149.31585693359375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 9/500: 1 files\n",
      "Loss: [144.01065063476562, 142.9293212890625, 141.92355346679688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 10/500: 1 files\n",
      "Loss: [221.79788208007812, 220.8276824951172, 219.88671875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 11/500: 1 files\n",
      "Loss: [462.997314453125, 461.267822265625, 459.7604675292969] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 12/500: 1 files\n",
      "Loss: [111.33816528320312, 110.56632995605469, 109.72341918945312] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 13/500: 1 files\n",
      "Loss: [180.2200927734375, 179.30567932128906, 178.35594177246094] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 14/500: 1 files\n",
      "Loss: [21.66558265686035, 21.44220542907715, 21.220766067504883] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 15/500: 1 files\n",
      "Loss: [45.488800048828125, 45.14315414428711, 44.73194122314453] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 16/500: 1 files\n",
      "Loss: [19.162372589111328, 18.919403076171875, 18.690284729003906] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 17/500: 1 files\n",
      "Loss: [255.12448120117188, 253.83453369140625, 252.4495086669922] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 18/500: 1 files\n",
      "Loss: [256.90081787109375, 255.2136993408203, 253.6259002685547] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 19/500: 1 files\n",
      "Loss: [290.44281005859375, 288.92822265625, 287.4515686035156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 20/500: 1 files\n",
      "Loss: [254.23513793945312, 253.11329650878906, 251.983154296875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 21/500: 1 files\n",
      "Loss: [230.2772979736328, 229.51513671875, 227.9755859375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 22/500: 1 files\n",
      "Loss: [286.3704833984375, 284.9731750488281, 283.2457580566406] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 23/500: 1 files\n",
      "Loss: [236.76185607910156, 235.2095184326172, 234.32781982421875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 24/500: 1 files\n",
      "Loss: [239.8848876953125, 238.75086975097656, 237.6031494140625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 25/500: 1 files\n",
      "Loss: [249.04391479492188, 248.14187622070312, 246.39410400390625] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 26/500: 1 files\n",
      "Loss: [161.56991577148438, 160.6375732421875, 159.7583770751953] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 27/500: 1 files\n",
      "Loss: [233.90301513671875, 230.57037353515625, 229.97215270996094] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 28/500: 1 files\n",
      "Loss: [71.60999298095703, 71.12550354003906, 70.41671752929688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 29/500: 1 files\n",
      "Loss: [109.04711151123047, 108.11510467529297, 107.2898178100586] Acc : [0.0, 1.0, 1.0]\n",
      "Processing batch 30/500: 1 files\n",
      "Loss: [194.86289978027344, 192.8970184326172, 191.024658203125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 31/500: 1 files\n",
      "Loss: [190.1858673095703, 188.8148956298828, 187.478515625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 32/500: 1 files\n",
      "Loss: [176.41160583496094, 175.12820434570312, 173.88084411621094] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 33/500: 1 files\n",
      "Loss: [108.08247375488281, 107.1759262084961, 106.5757064819336] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 34/500: 1 files\n",
      "Loss: [123.48011016845703, 122.36294555664062, 121.35245513916016] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 35/500: 1 files\n",
      "Loss: [177.77520751953125, 176.2776336669922, 174.84429931640625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 36/500: 1 files\n",
      "Loss: [191.95289611816406, 190.52362060546875, 189.06678771972656] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 37/500: 1 files\n",
      "Loss: [150.54701232910156, 149.27320861816406, 147.9961395263672] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 38/500: 1 files\n",
      "Loss: [80.81819915771484, 80.14287567138672, 79.35881042480469] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 39/500: 1 files\n",
      "Loss: [247.31649780273438, 245.85513305664062, 244.4744415283203] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 40/500: 1 files\n",
      "Loss: [258.80499267578125, 257.2486572265625, 255.75839233398438] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 41/500: 1 files\n",
      "Loss: [107.7195053100586, 106.67779541015625, 105.72270202636719] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 42/500: 1 files\n",
      "Loss: [190.7486572265625, 189.27684020996094, 187.65451049804688] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 43/500: 1 files\n",
      "Loss: [46.50042724609375, 45.98427200317383, 45.44786834716797] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 44/500: 1 files\n",
      "Loss: [206.36402893066406, 205.3590850830078, 204.34759521484375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 45/500: 1 files\n",
      "Loss: [163.50341796875, 162.89675903320312, 162.25135803222656] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 46/500: 1 files\n",
      "Loss: [263.63116455078125, 262.67376708984375, 261.6827392578125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 47/500: 1 files\n",
      "Loss: [270.58526611328125, 268.75018310546875, 267.1275634765625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 48/500: 1 files\n",
      "Loss: [195.33270263671875, 193.9767608642578, 192.84817504882812] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 49/500: 1 files\n",
      "Loss: [145.8362274169922, 144.69125366210938, 143.7381134033203] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 50/500: 1 files\n",
      "Loss: [507.80645751953125, 505.1664733886719, 502.84332275390625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 51/500: 1 files\n",
      "Loss: [77.34839630126953, 76.45857238769531, 75.71725463867188] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 52/500: 1 files\n",
      "Loss: [131.28204345703125, 130.298828125, 129.33961486816406] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 53/500: 1 files\n",
      "Loss: [394.58575439453125, 392.7658996582031, 391.0911865234375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 54/500: 1 files\n",
      "Loss: [78.3509521484375, 77.7425308227539, 76.7996826171875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 55/500: 1 files\n",
      "Loss: [291.686767578125, 290.27545166015625, 288.76739501953125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 56/500: 1 files\n",
      "Loss: [144.5767822265625, 143.98207092285156, 143.3767852783203] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 57/500: 1 files\n",
      "Loss: [237.42420959472656, 236.31056213378906, 235.31448364257812] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 58/500: 1 files\n",
      "Loss: [78.62681579589844, 78.03191375732422, 77.49234008789062] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 59/500: 1 files\n",
      "Loss: [40.590763092041016, 40.03593444824219, 39.524993896484375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 60/500: 1 files\n",
      "Loss: [84.87236785888672, 84.15846252441406, 83.50360107421875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 61/500: 1 files\n",
      "Loss: [88.33711242675781, 87.72920227050781, 87.10423278808594] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 62/500: 1 files\n",
      "Loss: [176.59193420410156, 175.35726928710938, 174.13914489746094] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 63/500: 1 files\n",
      "Loss: [141.37771606445312, 140.68385314941406, 139.85562133789062] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 64/500: 1 files\n",
      "Loss: [125.6248550415039, 124.71656799316406, 123.89498138427734] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 65/500: 1 files\n",
      "Loss: [171.52381896972656, 170.50888061523438, 169.885986328125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 66/500: 1 files\n",
      "Loss: [87.2947998046875, 86.59317779541016, 85.95671844482422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 67/500: 1 files\n",
      "Loss: [81.47045135498047, 80.85470581054688, 80.30531311035156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 68/500: 1 files\n",
      "Loss: [123.03167724609375, 122.33296203613281, 121.66703033447266] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 69/500: 1 files\n",
      "Loss: [51.943092346191406, 51.42389678955078, 50.77664566040039] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 70/500: 1 files\n",
      "Loss: [110.70210266113281, 109.79043579101562, 108.94231414794922] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 71/500: 1 files\n",
      "Loss: [147.26803588867188, 145.8517608642578, 144.66098022460938] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 72/500: 1 files\n",
      "Loss: [219.50784301757812, 218.59500122070312, 217.75222778320312] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 73/500: 1 files\n",
      "Loss: [103.47918701171875, 102.81600952148438, 102.03273010253906] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 74/500: 1 files\n",
      "Loss: [120.34632873535156, 119.4754867553711, 118.74009704589844] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 75/500: 1 files\n",
      "Loss: [101.22528839111328, 100.38573455810547, 99.53827667236328] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 76/500: 1 files\n",
      "Loss: [149.24282836914062, 148.63174438476562, 147.8146209716797] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 77/500: 1 files\n",
      "Loss: [187.5772247314453, 186.77798461914062, 185.95993041992188] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 78/500: 1 files\n",
      "Loss: [296.8132629394531, 294.8472595214844, 293.1924133300781] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 79/500: 1 files\n",
      "Loss: [55.954071044921875, 55.472862243652344, 54.773460388183594] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 80/500: 1 files\n",
      "Loss: [56.08938980102539, 55.294189453125, 54.59758377075195] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 81/500: 1 files\n",
      "Loss: [158.01950073242188, 156.93966674804688, 155.76336669921875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 82/500: 1 files\n",
      "Loss: [142.21116638183594, 141.53463745117188, 140.87474060058594] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 83/500: 1 files\n",
      "Loss: [186.61793518066406, 185.54476928710938, 184.47247314453125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 84/500: 1 files\n",
      "Loss: [412.4110412597656, 410.1740417480469, 407.98529052734375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 85/500: 1 files\n",
      "Loss: [262.4971923828125, 261.17962646484375, 259.86126708984375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 86/500: 1 files\n",
      "Loss: [90.83060455322266, 90.3755874633789, 88.16634368896484] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 87/500: 1 files\n",
      "Loss: [392.7718200683594, 391.2421569824219, 389.6914978027344] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 88/500: 1 files\n",
      "Loss: [151.95474243164062, 150.9459228515625, 149.58065795898438] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 89/500: 1 files\n",
      "Loss: [220.05633544921875, 218.83950805664062, 217.5111083984375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 90/500: 1 files\n",
      "Loss: [384.9569091796875, 382.7803955078125, 380.6514892578125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 91/500: 1 files\n",
      "Loss: [23.059974670410156, 22.527828216552734, 22.151697158813477] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 92/500: 1 files\n",
      "Loss: [106.64237976074219, 105.9576644897461, 105.2860336303711] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 93/500: 1 files\n",
      "Loss: [74.38890075683594, 73.89414978027344, 73.2913589477539] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 94/500: 1 files\n",
      "Loss: [236.93756103515625, 235.62071228027344, 234.5488739013672] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 95/500: 1 files\n",
      "Loss: [185.7990264892578, 184.6052703857422, 183.48472595214844] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 96/500: 1 files\n",
      "Loss: [60.552799224853516, 59.858646392822266, 59.131439208984375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 97/500: 1 files\n",
      "Loss: [208.36390686035156, 207.17633056640625, 205.84742736816406] Acc : [0.0, 1.0, 1.0]\n",
      "Processing batch 98/500: 1 files\n",
      "Loss: [151.6523895263672, 150.583251953125, 149.47174072265625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 99/500: 1 files\n",
      "Loss: [382.9055480957031, 381.1734619140625, 379.5443420410156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 100/500: 1 files\n",
      "Loss: [57.88210678100586, 57.14246368408203, 56.377838134765625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 101/500: 1 files\n",
      "Loss: [160.72242736816406, 159.64694213867188, 158.58946228027344] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 102/500: 1 files\n",
      "Loss: [197.3392791748047, 195.34385681152344, 193.551025390625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 103/500: 1 files\n",
      "Loss: [305.4582824707031, 303.72735595703125, 302.02947998046875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 104/500: 1 files\n",
      "Loss: [229.60667419433594, 228.63449096679688, 227.71694946289062] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 105/500: 1 files\n",
      "Loss: [176.0542755126953, 174.96121215820312, 173.78857421875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 106/500: 1 files\n",
      "Loss: [154.78115844726562, 153.75137329101562, 152.7533416748047] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 107/500: 1 files\n",
      "Loss: [290.00885009765625, 288.56658935546875, 287.2865295410156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 108/500: 1 files\n",
      "Loss: [135.60279846191406, 134.7773895263672, 134.4707794189453] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 109/500: 1 files\n",
      "Loss: [334.5892639160156, 333.1236877441406, 331.9085998535156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 110/500: 1 files\n",
      "Loss: [169.2773895263672, 167.5446319580078, 167.02182006835938] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 111/500: 1 files\n",
      "Loss: [191.3785858154297, 190.36309814453125, 189.3443603515625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 112/500: 1 files\n",
      "Loss: [210.6523895263672, 209.2794189453125, 207.97283935546875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 113/500: 1 files\n",
      "Loss: [111.77864074707031, 110.75626373291016, 109.80937194824219] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 114/500: 1 files\n",
      "Loss: [222.18258666992188, 220.76856994628906, 219.46116638183594] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 115/500: 1 files\n",
      "Loss: [208.22988891601562, 204.6844482421875, 202.7887725830078] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 116/500: 1 files\n",
      "Loss: [102.49219512939453, 101.80140686035156, 101.15423583984375] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 117/500: 1 files\n",
      "Loss: [163.94082641601562, 162.85374450683594, 161.78968811035156] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 118/500: 1 files\n",
      "Loss: [385.8927917480469, 384.6650390625, 383.4716491699219] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 119/500: 1 files\n",
      "Loss: [167.9564208984375, 166.8856658935547, 165.8639678955078] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 120/500: 1 files\n",
      "Loss: [113.97799682617188, 113.36851501464844, 112.87529754638672] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 121/500: 1 files\n",
      "Loss: [77.71917724609375, 77.19916534423828, 76.6513442993164] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 122/500: 1 files\n",
      "Loss: [211.441650390625, 210.51568603515625, 209.4395294189453] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 123/500: 1 files\n",
      "Loss: [222.39524841308594, 221.34527587890625, 220.39877319335938] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 124/500: 1 files\n",
      "Loss: [251.40634155273438, 249.4156494140625, 247.87741088867188] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 125/500: 1 files\n",
      "Loss: [221.9263916015625, 220.7771453857422, 219.66122436523438] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 126/500: 1 files\n",
      "Loss: [219.54965209960938, 217.51319885253906, 216.43634033203125] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 127/500: 1 files\n",
      "Loss: [219.08502197265625, 217.75778198242188, 216.3795166015625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 128/500: 1 files\n",
      "Loss: [66.3667221069336, 65.95535278320312, 65.71588897705078] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 129/500: 1 files\n",
      "Loss: [178.24755859375, 177.21539306640625, 176.3748779296875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 130/500: 1 files\n",
      "Loss: [186.75491333007812, 185.83309936523438, 185.0015106201172] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 131/500: 1 files\n",
      "Loss: [322.7542724609375, 320.71624755859375, 318.9091491699219] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 132/500: 1 files\n",
      "Loss: [154.35784912109375, 153.46609497070312, 152.69703674316406] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 133/500: 1 files\n",
      "Loss: [178.57704162597656, 177.42855834960938, 176.3425750732422] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 134/500: 1 files\n",
      "Loss: [14.829916000366211, 14.716787338256836, 14.483903884887695] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 135/500: 1 files\n",
      "Loss: [54.26368713378906, 53.61482238769531, 52.95686340332031] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 136/500: 1 files\n",
      "Loss: [44.693626403808594, 44.20304489135742, 43.65332794189453] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 137/500: 1 files\n",
      "Loss: [48.90951156616211, 47.962135314941406, 47.84258270263672] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 138/500: 1 files\n",
      "Loss: [90.32261657714844, 89.46904754638672, 89.22233581542969] Acc : [0.0, 1.0, 1.0]\n",
      "Processing batch 139/500: 1 files\n",
      "Loss: [38.474456787109375, 38.063533782958984, 37.57005310058594] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 140/500: 1 files\n",
      "Loss: [240.2882080078125, 238.12657165527344, 236.51651000976562] Acc : [1.0, 1.0, 1.0]\n",
      "Processing batch 141/500: 1 files\n",
      "Loss: [190.72378540039062, 188.5927734375, 186.61685180664062] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 142/500: 1 files\n",
      "Loss: [351.3548889160156, 348.9415283203125, 346.82281494140625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 143/500: 1 files\n",
      "Loss: [349.0301513671875, 347.72711181640625, 346.28729248046875] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 144/500: 1 files\n",
      "Loss: [517.7605590820312, 516.5736083984375, 515.3814697265625] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 145/500: 1 files\n",
      "Loss: [46.6410026550293, 46.06110763549805, 45.54955291748047] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 146/500: 1 files\n",
      "Loss: [294.2569580078125, 292.6451110839844, 291.1793518066406] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 147/500: 1 files\n",
      "Loss: [37.130191802978516, 36.732234954833984, 36.384159088134766] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 148/500: 1 files\n",
      "Loss: [121.5226821899414, 120.39141845703125, 119.70730590820312] Acc : [0.0, 0.0, 0.0]\n",
      "Processing batch 149/500: 1 files\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Main init and training sequence\n",
    "###############################################################\n",
    "# Batch params\n",
    "batch_data_size = 1\n",
    "batch_files_size = 1\n",
    "node_feature_dim = 60\n",
    "epochs =  10\n",
    "\n",
    "# Initialize model\n",
    "\n",
    "extractor = AssemblyFeatureExtractor()\n",
    "model = AssemblyGAT(node_feature_dim=node_feature_dim, hidden_dim=64, output_dim=453)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "for i in range(0, 1500, 500):\n",
    "  batch_curr = f'rep_{i}'\n",
    "  print(f\"========== batch: {batch_curr} ========== \")\n",
    "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=5, rep_batch=batch_curr, max_file=500)\n",
    "\n",
    "  # train\n",
    "  hash_t = run_files_in_batches(list_train, batch_files_size=batch_files_size, mode='train')\n",
    "  print(\"************ End of training ***********************\")\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
