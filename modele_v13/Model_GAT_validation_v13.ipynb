{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Sorbonne - DST\n",
    "# Module VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16312,
     "status": "ok",
     "timestamp": 1742994966616,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "9fc125ff-3caf-4a58-9271-536d388255fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset, Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18759,
     "status": "ok",
     "timestamp": 1742994988870,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "3972c2f6-e1a6-4945-ab3e-8c82cf0ec802"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1742995143971,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "3d29a151-3bad-42c9-aee5-94cc5d8faa90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/content': # Google Colab\n",
    "    training_path_dir =   '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set'\n",
    "    test_path_dir =       '/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set'\n",
    "    train_meta_data =     '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "\n",
    "    model_save_file =     '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge-v12/split_val-colab-v12.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    test_init_file =        '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_set_to_predict.csv'\n",
    "    filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "    filename_trained =      '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/list_hash_trained.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\\modele_v13\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"..\\Pred_test_v13\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_training-pc-v13.csv' # entrainement\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_val-pc-v13.csv' # validation\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v13\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\\modele_v13\": #PC2\n",
    "    split_char = '\\\\'\n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\"..\\Pred_test_v13\\list_hash_trained.csv\"\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_training-pc-v13.csv' # entrainement\n",
    "    file_split_training = r'..\\Pred_test_v13\\split_val-pc-v13.csv' # validation\n",
    "\n",
    "    model_save_file =  r'..\\Pred_test_v13\\model_sorbonne_weights.pth'\n",
    "    train_meta_data =  r'..\\training_set_metadata.csv'\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "random_seed = 42\n",
    "# read CSV input and save in df\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "df_meta_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1742995083637,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "wCU7s5ZbtSu9",
    "outputId": "f90a395e-1598-4430-ec12-023d86f1c231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23102 entries, 0 to 23101\n",
      "Columns: 454 entries, name to yoda\n",
      "dtypes: int64(453), object(1)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meta_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1742995088880,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "3e8a7303-8f99-4b35-a6de-ec04819f534e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Pred_test_v13\\list_hash_trained.csv\n",
      "taille : 115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86f133e9201e6131cae209f5b3c67f48cd5e23666b9ee0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3c72731f12ebb779bc4c5f0e05a62ca7785d82fe852b2d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414cb126efb9b3e0520ff6504ad2448e383aa284aeca01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73a82d73d442107e3be9f1b10142f9f780746e14e750e3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "1  86f133e9201e6131cae209f5b3c67f48cd5e23666b9ee0...\n",
       "2  3c72731f12ebb779bc4c5f0e05a62ca7785d82fe852b2d...\n",
       "3  414cb126efb9b3e0520ff6504ad2448e383aa284aeca01...\n",
       "4  73a82d73d442107e3be9f1b10142f9f780746e14e750e3..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "nb_trained_graph = len(df_files_trained)\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1742995476399,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "864f8ce9-421f-4a0c-cd1f-b51c6c97c2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0']\n",
      "['G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\40936a67037f8fd8e215b045f9cdf9c55840411316a62c85c8b54f75c6b0a5c8.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\368d2b5ccfd49f01942f462037710146ec3ca5ca8a5318a092bc49bfebfe8bad.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\75010f6d21b0b4451b5465ab8f46b385bb1edc15ed50634f4120352edc49cf3a.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\95e944577e686782e9d725e2e79ef9657a3f3b4f3b9cb265487bcd8e55cba95d.json', 'G:\\\\.shortcut-targets-by-id\\\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\\\Sorbonne_Data_challenge\\\\folder_training_set\\\\72356f26be98c580e23ca7baebe665c781fb2484575b9b05ad38676ad74ffb9e.json']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84 entries, 0 to 83\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   orign path  84 non-null     object\n",
      " 1   batch       84 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_0']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique())\n",
    "print(full_file_list[:5])\n",
    "print(df_training_split.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1742995606261,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0', max_file=0):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    file_with_err = 0\n",
    "    count_files = 0\n",
    "    #max_file = 0\n",
    "\n",
    "    #file_split_training = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\split_training-colab.csv\"\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\",\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "        #    print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 100_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                print(\"size\", file_size/1000, full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        list_val_hash.append(full_path_file)\n",
    "        \"\"\"\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "        \"\"\"\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1742995610025,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "84w6HNzrOKjz",
    "outputId": "bca9b4b0-03c1-45ba-d968-2873979f2cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 149418.131 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f819c070084c5ab608d6a1d58e7a42bfe8a304aa0003e70a7de72c9fec835f27.json\n",
      "Number of files with error: 1\n",
      "81 0 1\n"
     ]
    }
   ],
   "source": [
    "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.01, rep_batch='rep_0')\n",
    "print(len(list_val), len(list_train), len(list_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init modele and functions for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742995613755,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "# function to extract and tokenize the graph features\n",
    "class AssemblyFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.max_registers_to_embed = 3  # Embed up to 3 registers\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "        # Set to store all encountered registers for vocabulary building\n",
    "        self.all_registers = set()\n",
    "\n",
    "    def tokenize_line(self, line):\n",
    "        tokens = {}\n",
    "        if match := self.patterns['node_type'].search(line):\n",
    "            tokens['node_type'] = match.group(1)\n",
    "        if match := self.patterns['operation'].search(line):\n",
    "            tokens['operation'] = match.group(1)\n",
    "        if matches := self.patterns['registers'].findall(line):\n",
    "            tokens['registers'] = [reg.lower() for reg in matches]\n",
    "            self.all_registers.update(tokens['registers'])\n",
    "        if match := self.patterns['memory_ref'].search(line):\n",
    "            tokens['memory_ref'] = match.group(1)\n",
    "        if match := self.patterns['immediate'].search(line):\n",
    "            tokens['immediate'] = match.group(0)\n",
    "        return tokens\n",
    "\n",
    "    def extract_features(self, digraph):\n",
    "        all_instruction_features = {}\n",
    "        for node_id, node_data in digraph.nodes(data=True):\n",
    "            label = node_data.get('label', '')\n",
    "            instruction_features_list = []\n",
    "            instructions = label.split('\\n')\n",
    "            for i, line in enumerate(instructions):\n",
    "                tokens = self.tokenize_line(line)\n",
    "                instruction_features = {}\n",
    "\n",
    "                if 'operation' in tokens:\n",
    "                    instruction_features['operation'] = tokens['operation']\n",
    "                if 'node_type' in tokens:\n",
    "                    instruction_features['node_type'] = tokens['node_type']\n",
    "                if 'memory_ref' in tokens:\n",
    "                    instruction_features['memory_ref'] = tokens['memory_ref']\n",
    "                if 'immediate' in tokens:\n",
    "                    instruction_features['immediate'] = tokens['immediate']\n",
    "\n",
    "                # Binary features for register presence (before vocabulary is built)\n",
    "                for reg in self.all_registers:\n",
    "                    instruction_features[f'has_register_{reg}'] = 1 if 'registers' in tokens and reg in tokens['registers'] else 0\n",
    "\n",
    "                # Position in the sequence\n",
    "                instruction_features['position'] = i\n",
    "                instruction_features_list.append(instruction_features)\n",
    "\n",
    "            if len(instruction_features_list) == 1:\n",
    "                all_instruction_features[node_id] = instruction_features_list[0]\n",
    "            else:\n",
    "                all_instruction_features[node_id] = instruction_features_list\n",
    "        return all_instruction_features\n",
    "\n",
    "    def build_vocabularies(self, input_data_list):\n",
    "        node_types = set()\n",
    "        operations = set()\n",
    "        registers = set()\n",
    "        memory_patterns = set()\n",
    "        immediates = set()\n",
    "\n",
    "        for item in input_data_list:\n",
    "            if isinstance(item, nx.DiGraph):\n",
    "                graph = item\n",
    "                for _, node_data in graph.nodes(data=True):\n",
    "                    label = node_data.get('label', '')\n",
    "                    for line in label.split('\\n'):\n",
    "                        tokens = self.tokenize_line(line)\n",
    "                        if 'node_type' in tokens:\n",
    "                            node_types.add(tokens['node_type'])\n",
    "                        if 'operation' in tokens:\n",
    "                            operations.add(tokens['operation'])\n",
    "                        if 'registers' in tokens:\n",
    "                            registers.update(tokens['registers'])\n",
    "                        if 'memory_ref' in tokens:\n",
    "                            memory_patterns.add(tokens['memory_ref'])\n",
    "                        if 'immediate' in tokens:\n",
    "                            immediates.add(tokens['immediate'])\n",
    "            elif isinstance(item, str):\n",
    "                assembly_code = item\n",
    "                for line in assembly_code.strip().split('\\n'):\n",
    "                    tokens = self.tokenize_line(line)\n",
    "                    if 'node_type' in tokens:\n",
    "                        node_types.add(tokens['node_type'])\n",
    "                    if 'operation' in tokens:\n",
    "                        operations.add(tokens['operation'])\n",
    "                    if 'registers' in tokens:\n",
    "                        registers.update(tokens['registers'])\n",
    "                    if 'memory_ref' in tokens:\n",
    "                        memory_patterns.add(tokens['memory_ref'])\n",
    "                    if 'immediate' in tokens:\n",
    "                        immediates.add(tokens['immediate'])\n",
    "\n",
    "        self.node_type_vocab = {token: i + 1 for i, token in enumerate(sorted(list(node_types)))}\n",
    "        self.node_type_vocab[self.UNK_TOKEN] = 0\n",
    "        self.operation_vocab = {token: i + 1 for i, token in enumerate(sorted(list(operations)))}\n",
    "        self.operation_vocab[self.UNK_TOKEN] = 0\n",
    "        self.register_vocab = {token: i + 1 for i, token in enumerate(sorted(list(registers)))}\n",
    "        self.register_vocab[self.UNK_TOKEN] = 0\n",
    "        self.memory_pattern_vocab = {token: i + 1 for i, token in enumerate(sorted(list(memory_patterns)))}\n",
    "        self.memory_pattern_vocab[self.UNK_TOKEN] = 0\n",
    "        self.immediate_vocab = {token: i + 1 for i, token in enumerate(sorted(list(immediates)))}\n",
    "        self.immediate_vocab[self.UNK_TOKEN] = 0\n",
    "\n",
    "    def embed_instruction(self, instruction_features, embedding_dim=60):\n",
    "        \"\"\"Embeds a single assembly instruction feature dictionary.\"\"\"\n",
    "        embedding = []\n",
    "\n",
    "        # Embed operation (if present)\n",
    "        op_embedding = np.zeros(embedding_dim)\n",
    "        if 'operation' in instruction_features and instruction_features['operation'] in self.operation_vocab:\n",
    "            op_index = self.operation_vocab[instruction_features['operation']]\n",
    "            op_embedding[op_index % embedding_dim] = 1.0  # Simple one-hot-like hashing\n",
    "        embedding.extend(op_embedding)\n",
    "\n",
    "        # Embed node type (if present)\n",
    "        node_type_embedding = np.zeros(embedding_dim // 2)\n",
    "        if 'node_type' in instruction_features and instruction_features['node_type'] in self.node_type_vocab:\n",
    "            node_type_index = self.node_type_vocab[instruction_features['node_type']]\n",
    "            node_type_embedding[node_type_index % (embedding_dim // 2)] = 1.0\n",
    "        embedding.extend(node_type_embedding)\n",
    "\n",
    "        # Embed registers (up to max_registers_to_embed)\n",
    "        registers_present = [reg.split('_')[-1] for reg in instruction_features if reg.startswith('has_register_') and instruction_features[reg] == 1]\n",
    "        register_embedding = np.zeros(self.max_registers_to_embed * (embedding_dim // 3)) # Allocate space\n",
    "        for i, reg in enumerate(registers_present[:self.max_registers_to_embed]):\n",
    "            if reg in self.register_vocab:\n",
    "                reg_index = self.register_vocab[reg]\n",
    "                register_embedding[i * (embedding_dim // 3) + (reg_index % (embedding_dim // 3))] = 1.0\n",
    "        embedding.extend(register_embedding)\n",
    "\n",
    "        # Embed memory pattern (if present)\n",
    "        mem_embedding = np.zeros(embedding_dim // 2)\n",
    "        if 'memory_ref' in instruction_features and instruction_features['memory_ref'] in self.memory_pattern_vocab:\n",
    "            mem_index = self.memory_pattern_vocab[instruction_features['memory_ref']]\n",
    "            mem_embedding[mem_index % (embedding_dim // 2)] = 1.0\n",
    "        embedding.extend(mem_embedding)\n",
    "\n",
    "        # Embed immediate (just presence for now)\n",
    "        has_immediate = 1 if 'immediate' in instruction_features else 0\n",
    "        embedding.append(has_immediate)\n",
    "\n",
    "        # Position\n",
    "        position = instruction_features.get('position', 0)\n",
    "        embedding.append(position / 10.0) # Normalize position\n",
    "\n",
    "        return np.array(embedding[:60]) # Truncate to max 60 dimensions\n",
    "\n",
    "    def embed_graph(self, digraph):\n",
    "        \"\"\"Embeds all instructions within the nodes of a digraph.\"\"\"\n",
    "        embedded_nodes = {}\n",
    "        raw_features = self.extract_features(digraph)\n",
    "        for node_id, features in raw_features.items():\n",
    "            if isinstance(features, list):  # Multiple instructions in a node\n",
    "                embedded_nodes[node_id] = [self.embed_instruction(f) for f in features]\n",
    "            else:  # Single instruction in a node\n",
    "                embedded_nodes[node_id] = self.embed_instruction(features)\n",
    "        return embedded_nodes\n",
    "\n",
    "# Function to create dataloader\n",
    "#\n",
    "class AssemblyGraphDataset(Dataset):\n",
    "    def __init__(self, graph_list, feature_extractor, hash_dim=512):\n",
    "        self.graph_list = graph_list\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.hash_dim = hash_dim\n",
    "        self.processed_data = self._process_all_graphs()\n",
    "\n",
    "    def _process_all_graphs(self):\n",
    "        processed_data = []\n",
    "        for graph_item in self.graph_list: # Iterate through the list of graph items\n",
    "            graph = graph_item # Access the NetworkX graph from the dictionary\n",
    "            embedded_nodes = self.feature_extractor.embed_graph(graph)\n",
    "\n",
    "            # Create PyTorch Geometric Data object\n",
    "            node_features = []\n",
    "            node_order = sorted(graph.nodes()) # Ensure consistent node order\n",
    "            for node_id in node_order:\n",
    "                embedding = embedded_nodes.get(node_id)\n",
    "                if isinstance(embedding, list):\n",
    "                    # For simplicity, if a node has multiple instructions, we take the first embedding\n",
    "                    # You might want a more sophisticated way to handle this (e.g., averaging, using RNN)\n",
    "                    node_features.append(embedding[0])\n",
    "                elif embedding is not None:\n",
    "                    node_features.append(embedding)\n",
    "                else:\n",
    "                    node_features.append(np.zeros(60)) # Handle missing embeddings\n",
    "\n",
    "            # Get edges as a list of tuples\n",
    "            edges = list(graph.edges())\n",
    "            if not edges:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            else:\n",
    "                # Convert node labels to indices if they are not already numeric\n",
    "                node_to_index = {node: i for i, node in enumerate(graph.nodes())}\n",
    "                indexed_edges = [(node_to_index[u], node_to_index[v]) for u, v in edges]\n",
    "                edge_index = torch.tensor(indexed_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "            # Dummy hash encoding (replace with your actual hash encoding)\n",
    "            hash_encoded = torch.randn(len(node_features), self.hash_dim)\n",
    "\n",
    "            # Convert lists of numpy arrays to single numpy arrays\n",
    "            if isinstance(node_features, list):\n",
    "                node_features_np = np.array(node_features)\n",
    "            else:\n",
    "                node_features_np = node_features  # Already a numpy array\n",
    "\n",
    "            # Convert the numpy arrays to torch tensors\n",
    "            node_features_tensor = torch.tensor(node_features_np, dtype=torch.float)\n",
    "\n",
    "            data = Data(x=node_features_tensor,\n",
    "                        edge_index=edge_index,\n",
    "                        hash_encoded=hash_encoded)\n",
    "            processed_data.append(data)\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collates a list of PyTorch Geometric Data objects into a Batch object.\n",
    "    Args:\n",
    "        batch (list): A list of `torch_geometric.data.Data` objects.\n",
    "    Returns:\n",
    "        torch_geometric.data.Batch: A `Batch` object that combines the data\n",
    "            objects in the `batch`.\n",
    "    \"\"\"\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "# Define the AssemblyGAT model\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        x, edge_index, hash_encoded, batch = data.x, data.edge_index, data.hash_encoded, data.batch\n",
    "\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "#######################################\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    list_hash = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                digraph_str = f.read()\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "                f.close()\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append(G)\n",
    "                list_hash.append(hash_file)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr, list_hash\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "\n",
    "def val_model(model, val_loader, y_data, device='cuda'):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    all_pred_label =[]\n",
    "    all_true_label = []\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "    }\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            y_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(batch)\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_data.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #print(\"pred & y \", output.shape, y_data.shape )\n",
    "            total += y_data.shape[0]*y_data.shape[1]\n",
    "            correct += (prediction_labels == y_data).sum().item()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f'Accuracy of the network on the validation set: {100*accuracy:.2f}%')\n",
    "    if not all_true_label:\n",
    "        print(\"Warning: No true labels collected during validation. Returning empty tensors.\")\n",
    "        return torch.empty(0, dtype=torch.long).cpu().numpy(), torch.empty(0, dtype=torch.long).cpu().numpy()\n",
    "    else:\n",
    "        true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "        pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "        return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "    \"\"\"\n",
    "    Processes files in batches for tokenization and model training/evaluation in a single loop.\n",
    "    Args:\n",
    "        full_file_paths (list): List of full file paths to process.\n",
    "        batch_files_size (int): Number of files to process in each batch for tokenization.\n",
    "        batch_data_size (int): Batch size for the DataLoader during training/evaluation.\n",
    "        num_feature_dim (int, optional): Dimension of node features. Defaults to None.\n",
    "        mode (str): 'train' or other mode (e.g., 'eval'). Defaults to 'train'.\n",
    "        epochs (int): Number of training epochs per batch (if mode is 'train'). Defaults to 3.\n",
    "        tokenizer: Tokenizer object with methods like get_tokens_and_counts and fit_from_counts.\n",
    "        process_batch (callable): Function to process a batch of file paths and return a list of graph-like objects.\n",
    "        train_model (callable): Function to train the model with a DataLoader and other info.\n",
    "    \"\"\"\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) + 1\n",
    "\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    print(\"**** Tokenization and Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        batch_num = i // batch_files_size + 1\n",
    "        print(f\"Processing batch {batch_num}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list, hash_t = process_batch(batch_files)\n",
    "        if not batch_graph_list:\n",
    "            print(\"graph list is empty for this batch\")\n",
    "            continue\n",
    "        \n",
    "        #    print(hash_t)\n",
    "        hash_y_list = df_meta_train[df_meta_train['name'].isin(hash_t)].drop(columns=['name']).values\n",
    "        #print(hash_y_list)\n",
    "        y_data_tensor = torch.tensor(hash_y_list, dtype=torch.float32)\n",
    "        \n",
    "        # Create the AssemblyGraphDataset # Create the DataLoader\n",
    "        extractor.build_vocabularies(batch_graph_list)\n",
    "        assembly_dataset = AssemblyGraphDataset(batch_graph_list, extractor)\n",
    "        dataloader = DataLoader(assembly_dataset, batch_size=batch_data_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        # Train model\n",
    "        pred_labels, true_labels = val_model(model, dataloader, y_data_tensor, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        all_true_label.append(true_labels.cpu())\n",
    "        all_pred_label.append(pred_labels.cpu())\n",
    "\n",
    "    all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "    all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "    return all_pred_label_tensor, all_true_label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1012311,
     "status": "ok",
     "timestamp": 1742996849138,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "avU1v3vVtSu-",
    "outputId": "6997934b-0488-425c-8002-82e6e68dcf47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run validation on ..\\Pred_test_v13\\model_sorbonne_weights.pth trained on 115 graphs\n",
      "\n",
      "========== batch: rep_0 ========== \n",
      "size 149418.131 G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_training_set\\f819c070084c5ab608d6a1d58e7a42bfe8a304aa0003e70a7de72c9fec835f27.json\n",
      "Number of files with error: 1\n",
      "There are 81 files for validation\n",
      "**** Tokenization and Dataset Processing & Training\n",
      "Processing batch 1/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.88%\n",
      "Processing batch 2/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.87%\n",
      "Processing batch 3/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.21%\n",
      "Processing batch 4/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 48.45%\n",
      "Processing batch 5/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 49.78%\n",
      "Processing batch 6/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.11%\n",
      "Processing batch 7/41: 2 files\n",
      "pred & y  torch.Size([1, 453]) torch.Size([1, 453])\n",
      "Accuracy of the network on the validation set: 50.55%\n",
      "Processing batch 8/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.32%\n",
      "Processing batch 9/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.55%\n",
      "Processing batch 10/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 53.09%\n",
      "Processing batch 11/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 53.53%\n",
      "Processing batch 12/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.11%\n",
      "Processing batch 13/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.32%\n",
      "Processing batch 14/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.88%\n",
      "Processing batch 15/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.32%\n",
      "Processing batch 16/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 53.53%\n",
      "Processing batch 17/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.11%\n",
      "Processing batch 18/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.88%\n",
      "Processing batch 19/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 54.86%\n",
      "Processing batch 20/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.88%\n",
      "Processing batch 21/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.43%\n",
      "Processing batch 22/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.33%\n",
      "Processing batch 23/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.43%\n",
      "Processing batch 24/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.00%\n",
      "Processing batch 25/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.76%\n",
      "Processing batch 26/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.66%\n",
      "Processing batch 27/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.77%\n",
      "Processing batch 28/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 49.67%\n",
      "Processing batch 29/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.99%\n",
      "Processing batch 30/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 48.45%\n",
      "Processing batch 31/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.77%\n",
      "Processing batch 32/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 48.90%\n",
      "Processing batch 33/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.55%\n",
      "Processing batch 34/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 51.77%\n",
      "Processing batch 35/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 48.68%\n",
      "Processing batch 36/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.65%\n",
      "Processing batch 37/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 49.67%\n",
      "Processing batch 38/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 50.11%\n",
      "Processing batch 39/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.98%\n",
      "Processing batch 40/41: 2 files\n",
      "pred & y  torch.Size([2, 453]) torch.Size([2, 453])\n",
      "Accuracy of the network on the validation set: 52.21%\n",
      "Processing batch 41/41: 1 files\n",
      "pred & y  torch.Size([1, 453]) torch.Size([1, 453])\n",
      "Accuracy of the network on the validation set: 49.23%\n",
      "\n",
      "**** F1 score: 0.11804 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Main init and training sequence\n",
    "###############################################################\n",
    "# Batch params\n",
    "batch_data_size = 2\n",
    "batch_files_size = 2\n",
    "node_feature_dim = 60\n",
    "epochs =  25\n",
    "\n",
    "# Initialize model\n",
    "print(f\"Run validation on {model_save_file} trained on {nb_trained_graph} graphs\\n\")\n",
    "extractor = AssemblyFeatureExtractor()\n",
    "model = AssemblyGAT(node_feature_dim=node_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "batch_curr = 'rep_0'\n",
    "for i in range(0, 400, 500):\n",
    "  batch_curr = f'rep_{i}'\n",
    "  print(f\"========== batch: {batch_curr} ========== \")\n",
    "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=5, rep_batch=batch_curr, max_file=500)\n",
    "\n",
    "  # validate\n",
    "  all_pred_label_tensor, all_true_label_tensor = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode='validation')\n",
    "  f1_macro = f1_score(all_true_label_tensor, all_pred_label_tensor, average='macro', zero_division=1)\n",
    "  print(f\"\\n**** F1 score: {f1_macro:.5f} \\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
