{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx1v6NiktSu7",
        "outputId": "91020a71-bd20-40c6-d707-71d77b2490ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device is cpu\n"
          ]
        }
      ],
      "source": [
        "# Challenge Sorbonne - DST\n",
        "#\n",
        "#!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "#import rarfile\n",
        "import networkx as nx\n",
        "import io\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, Subset\n",
        "from torch.optim import AdamW\n",
        "#!pip install torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "import networkx as nx\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import tqdm as notebook_tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device is\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGZYw4ZtSu8",
        "outputId": "0ea8e980-53f9-4946-b4f3-61a6719e1ae0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\ncurrent_directory = os.getcwd()\\n\\nprint(f\"The current working directory is: {current_directory}\")\\n\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\n\\n#folder_id = \"1EyZZzvIdiLNepEIDJaLict6-JBT-uwlK\"\\ntraining_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set\"\\n\\nif os.path.exists(training_path_dir):\\n    print(f\"Accessing {training_path_dir}\")\\n'"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for Google Colab only\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#folder_id = \"1EyZZzvIdiLNepEIDJaLict6-JBT-uwlK\"\n",
        "training_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set\"\n",
        "\n",
        "if os.path.exists(training_path_dir):\n",
        "    print(f\"Accessing {training_path_dir}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eLIITiH84EOo",
        "outputId": "1aa36926-2d85-4368-a664-dd17519c07f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current working directory is: c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>64-bit execution via heavens gate</th>\n",
              "      <th>64bits</th>\n",
              "      <th>PEB access</th>\n",
              "      <th>accept command line arguments</th>\n",
              "      <th>access the Windows event log</th>\n",
              "      <th>act as TCP client</th>\n",
              "      <th>allocate RW memory</th>\n",
              "      <th>allocate RWX memory</th>\n",
              "      <th>allocate memory</th>\n",
              "      <th>...</th>\n",
              "      <th>winzip</th>\n",
              "      <th>wise</th>\n",
              "      <th>worm</th>\n",
              "      <th>write and execute a file</th>\n",
              "      <th>write clipboard data</th>\n",
              "      <th>write file on Linux</th>\n",
              "      <th>write file on Windows</th>\n",
              "      <th>write pipe</th>\n",
              "      <th>xorcrypt</th>\n",
              "      <th>yoda</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 454 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                name  \\\n",
              "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
              "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
              "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
              "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
              "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
              "\n",
              "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
              "0                                  0       1           0   \n",
              "1                                  0       0           0   \n",
              "2                                  0       0           0   \n",
              "3                                  0       0           0   \n",
              "4                                  0       0           0   \n",
              "\n",
              "   accept command line arguments  access the Windows event log  \\\n",
              "0                              0                             0   \n",
              "1                              0                             0   \n",
              "2                              0                             0   \n",
              "3                              0                             0   \n",
              "4                              0                             0   \n",
              "\n",
              "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
              "0                  0                   0                    0   \n",
              "1                  0                   0                    0   \n",
              "2                  0                   0                    0   \n",
              "3                  0                   0                    0   \n",
              "4                  0                   0                    0   \n",
              "\n",
              "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
              "0                0  ...       0     0     0                         0   \n",
              "1                0  ...       0     0     0                         0   \n",
              "2                0  ...       0     0     0                         0   \n",
              "3                0  ...       0     0     0                         0   \n",
              "4                0  ...       0     0     0                         0   \n",
              "\n",
              "   write clipboard data  write file on Linux  write file on Windows  \\\n",
              "0                     0                    0                      0   \n",
              "1                     0                    0                      1   \n",
              "2                     0                    0                      0   \n",
              "3                     0                    0                      0   \n",
              "4                     0                    0                      0   \n",
              "\n",
              "   write pipe  xorcrypt  yoda  \n",
              "0           0         0     0  \n",
              "1           0         0     0  \n",
              "2           0         0     0  \n",
              "3           0         0     0  \n",
              "4           0         0     0  \n",
              "\n",
              "[5 rows x 454 columns]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "current_directory = os.getcwd()\n",
        "print(f\"The current working directory is: {current_directory}\")\n",
        "\n",
        "# init du repertoire des données\n",
        "if current_directory == '/usr/users/msiac/msiac_10/Sorbonne' : #DCE\n",
        "    training_path_dir =   \"./training\"\n",
        "    test_path_dir = \"./test\"\n",
        "    \n",
        "    train_meta_data = 'training_set_metadata.csv'\n",
        "    model_save_file =  'model_sorbonne_weights.pth'\n",
        "\n",
        "    split_char = '/'\n",
        "    filename_test_results = \"test_prediction.csv\"\n",
        "    filename_trained = \"list_hash_trained.csv\"\n",
        "    file_split_training = 'split_training-dce.csv'\n",
        "\n",
        "elif current_directory == '/content': # Google Colab\n",
        "    training_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set\"\n",
        "    test_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set\"\n",
        "\n",
        "    train_meta_data =  '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
        "\n",
        "    model_save_file =  '/content/drive/MyDrive/model_sorbonne_weights.pth'\n",
        "    file_split_training = '/content/drive/MyDrive/split_training-colab.csv'\n",
        "    split_char = '/'\n",
        "\n",
        "    filename_test_results = '/content/drive/MyDrive/_test_prediction.csv'\n",
        "    filename_trained = '/content/drive/MyDrive/list_hash_trained.csv'\n",
        "\n",
        "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\": #PC\n",
        "    split_char = '\\\\'\n",
        "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
        "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
        "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
        "\n",
        "    test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
        "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
        "\n",
        "    train_meta_data =  r'..\\..\\Data\\ChallengeSorbonne\\training_set_metadata.csv'\n",
        "    file_split_training = r'.\\split_training-pc.csv'\n",
        "    model_save_file =  r'.\\model_sorbonne_weights.pth'\n",
        "    pre_fix_file = training_path_dir.split(split_char)[-1]\n",
        "    filename_test_results = os.path.join(current_directory, f\"{pre_fix_file}_test_prediction.csv\")\n",
        "    filename_trained = os.path.join(current_directory, f\"{pre_fix_file}_list_hash_trained.csv\")\n",
        "    \n",
        "# read CSV input and save in df\n",
        "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
        "df_meta_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCU7s5ZbtSu9",
        "outputId": "6c503d7a-d4ad-4a49-9ece-85bc2874120d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23102 entries, 0 to 23101\n",
            "Columns: 454 entries, name to yoda\n",
            "dtypes: int64(453), object(1)\n",
            "memory usage: 80.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df_meta_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bOuNPC0iZACE",
        "outputId": "4fc5b1bf-9f35-4765-da70-f79cce2922ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\training_list_hash_trained.csv\n",
            "taille : 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>files_trained</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [files_trained]\n",
              "Index: []"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(filename_trained)\n",
        "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
        "print(\"taille :\",len(df_files_trained))\n",
        "df_files_trained.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rep_0' 'rep_500' 'rep_1000']\n",
            "['D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04333849d7eb415a3986dc1afe0041723184b7f47aeeec5ffa07a0f9c222fee3.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04390cd4165bf34f62658cea0fa16116037e391a86ed59d09493c9d141f92276.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043a5b4be0a8f0500e88a2ffc91d15bf4212fa12a3f07be04654b67adc41d21c.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043cf998d7059cacaf626f443181c67f91adeb2bc7088ca918e1aee94838d23d.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043e2a8241a6eb0aa66de40e35e75dce0f1d047a6d9e4c934b482f7c588f9c61.json']\n"
          ]
        }
      ],
      "source": [
        "# load & check traing split\n",
        "df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
        "full_file_list = df_training_split[df_training_split['batch'] == 'rep_500']['orign path'].tolist()\n",
        "print(df_training_split['batch'].unique()[:3])\n",
        "print(full_file_list[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gU6S_PgtSu9"
      },
      "outputs": [],
      "source": [
        "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0'):\n",
        "    \"\"\"\n",
        "    Scan the training directory\n",
        "    get the hash name of the files, check and exclude hash file with error\n",
        "    retour a list of hash for training, validation and error\n",
        "    \"\"\"\n",
        "\n",
        "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
        "    file_with_err = 0\n",
        "    count_files = 0\n",
        "    max_file = 0\n",
        "\n",
        "    \"\"\"  \n",
        "    for root, _, files in os.walk(file_directory):\n",
        "        print(f\"There are {len(files)} files in the directory: {root}\")\n",
        "        print(f\"expected train size: {round(len(files) * (1-val_size))} , val size: {round(len(files) * val_size)}\")\n",
        "\n",
        "        full_file_list = [os.path.join(root, file) for file in files]\n",
        "    \"\"\"\n",
        "    #file_split_training = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\split_training-colab.csv\"\n",
        "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
        "    df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
        "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
        "\n",
        "    for full_path_file in full_file_list:\n",
        "        # test if filename in the trained list\n",
        "        hash_name = full_path_file.split('.jso')[0]\n",
        "        hash_name = hash_name.split(split_char)[-1]\n",
        "\n",
        "        # file already trained\n",
        "        if hash_name in df_files_trained['files_trained'].values:\n",
        "            print(\"trained\")\n",
        "            continue\n",
        "\n",
        "        # files not in metadata\n",
        "        if hash_name not in df_meta_train['name'].values:\n",
        "            list_err_hash.append(full_path_file)\n",
        "            print(\"meta\", hash_name, full_path_file)\n",
        "            continue\n",
        "\n",
        "        if os.path.exists(full_path_file):\n",
        "            file_size = os.path.getsize(full_path_file)\n",
        "            if file_size/1000 > 100_000: #file too big\n",
        "                list_err_hash.append(full_path_file)\n",
        "                print(\"size\", file_size/1000, full_path_file)\n",
        "                continue\n",
        "        else:\n",
        "            list_err_hash.append(full_path_file)\n",
        "            print(\"not exist\", full_path_file)\n",
        "            continue\n",
        "\n",
        "        # check if nb of files is within the max_files allowed\n",
        "        if max_file != 0 and count_files > max_file:\n",
        "            list_err_hash.append(full_path_file)\n",
        "            print(\"count\")\n",
        "            break\n",
        "\n",
        "        if count_files % (val_size*100) == 0:\n",
        "            list_val_hash.append(full_path_file)\n",
        "        else:\n",
        "            list_train_hash.append(full_path_file)\n",
        "        \n",
        "        count_files += 1\n",
        "\n",
        "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
        "    return list_train_hash, list_val_hash, list_err_hash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size 487.099\n",
            "size 8965.453\n",
            "size 375.321\n",
            "size 1398.408\n",
            "size 330.019\n",
            "size 428.247\n",
            "size 2302.446\n",
            "count\n",
            "Number of files with error: 1\n",
            "1 5 1\n"
          ]
        }
      ],
      "source": [
        "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch='rep_2500')\n",
        "print(len(list_val), len(list_train), len(list_err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "RLwlVa-5tSu9"
      },
      "outputs": [],
      "source": [
        "class HierarchicalAssemblyTokenizer:\n",
        "    def __init__(self):\n",
        "        # Tokenizer implementation from previous code\n",
        "        # Abbreviated for brevity but would be the full implementation\n",
        "        self.node_type_vocab = {}\n",
        "        self.operation_vocab = {}\n",
        "        self.register_vocab = {}\n",
        "        self.memory_pattern_vocab = {}\n",
        "        self.immediate_vocab = {}\n",
        "        self.UNK_TOKEN = \"<UNK>\"\n",
        "\n",
        "        # Regex patterns for parsing\n",
        "        self.patterns = {\n",
        "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
        "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
        "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
        "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
        "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
        "        }\n",
        "\n",
        "    def fit(self, graph_l, min_freq=1):\n",
        "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
        "        node_types = []\n",
        "        operations = []\n",
        "        registers = []\n",
        "        memory_patterns = []\n",
        "        immediates = []\n",
        "        self.unique_hashes = []\n",
        "        self.hash_to_id = {}\n",
        "        self.id_to_hash = {}\n",
        "\n",
        "        # Extract features from node labels\n",
        "        for item in graph_l:\n",
        "            graph = item['graph_input']\n",
        "            for node_id, node_attr in graph.nodes(data=True):\n",
        "                label = node_attr.get('label', '')\n",
        "\n",
        "                # Extract node type (JCC, INST)\n",
        "                node_type_match = self.patterns['node_type'].search(label)\n",
        "                if node_type_match:\n",
        "                    node_types.append(node_type_match.group(1))\n",
        "\n",
        "                # Extract operation (xor, push, mov)\n",
        "                op_match = self.patterns['operation'].search(label)\n",
        "                if op_match:\n",
        "                    operations.append(op_match.group(1))\n",
        "\n",
        "                # Extract registers\n",
        "                reg_matches = self.patterns['registers'].findall(label)\n",
        "                registers.extend(reg_matches)\n",
        "\n",
        "                # Extract memory reference patterns\n",
        "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
        "                for mem in mem_matches:\n",
        "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
        "                    pattern = self._memory_to_pattern(mem)\n",
        "                    memory_patterns.append(pattern)\n",
        "\n",
        "                # Extract immediate values\n",
        "                imm_matches = self.patterns['immediate'].findall(label)\n",
        "                immediates.extend(imm_matches)\n",
        "\n",
        "            self.unique_hashes.append(item['name'])\n",
        "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
        "            for i, hash_val in enumerate(self.unique_hashes):\n",
        "                self.hash_to_id[hash_val] = i\n",
        "                self.id_to_hash[i] = hash_val\n",
        "\n",
        "        # Build vocabularies with frequency filtering\n",
        "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
        "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
        "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
        "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
        "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def fit_from_counts(self, all_counts, min_freq=3):\n",
        "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
        "        self.node_type_vocab.clear()\n",
        "        self.operation_vocab.clear()\n",
        "        self.register_vocab.clear()\n",
        "        self.memory_pattern_vocab.clear()\n",
        "        self.immediate_vocab.clear()\n",
        "\n",
        "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
        "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
        "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
        "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
        "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
        "\n",
        "        self.unique_hashes = all_counts['unique_hashes']\n",
        "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
        "        for i, hash_val in enumerate(self.unique_hashes):\n",
        "            self.hash_to_id[hash_val] = i\n",
        "            self.id_to_hash[i] = hash_val\n",
        "\n",
        "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
        "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
        "        counter = Counter(tokens)\n",
        "        # Add special UNK token\n",
        "        vocab_dict[self.UNK_TOKEN] = 0\n",
        "\n",
        "        # Add tokens that meet minimum frequency\n",
        "        idx = 1\n",
        "        for token, count in counter.most_common():\n",
        "            if count >= min_freq:\n",
        "                vocab_dict[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "    def _memory_to_pattern(self, mem_ref):\n",
        "        \"\"\"Convert memory reference to pattern\"\"\"\n",
        "        # Replace registers with REG placeholder\n",
        "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
        "        # Replace immediate values with IMM placeholder\n",
        "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
        "        return pattern.strip()\n",
        "\n",
        "    def tokenize(self, node_label):\n",
        "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
        "        features = {\n",
        "            'node_type': self.UNK_TOKEN,\n",
        "            'operation': self.UNK_TOKEN,\n",
        "            'registers': [],\n",
        "            'memory_pattern': self.UNK_TOKEN,\n",
        "            'immediate': self.UNK_TOKEN\n",
        "        }\n",
        "\n",
        "        # Extract node type\n",
        "        node_type_match = self.patterns['node_type'].search(node_label)\n",
        "        if node_type_match:\n",
        "            node_type = node_type_match.group(1)\n",
        "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
        "\n",
        "        # Extract operation\n",
        "        op_match = self.patterns['operation'].search(node_label)\n",
        "        if op_match:\n",
        "            operation = op_match.group(1)\n",
        "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
        "\n",
        "        # Extract registers\n",
        "        reg_matches = self.patterns['registers'].findall(node_label)\n",
        "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
        "\n",
        "        # Extract memory reference\n",
        "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
        "        if mem_matches:\n",
        "            pattern = self._memory_to_pattern(mem_matches[0])\n",
        "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
        "\n",
        "        # Extract immediate values\n",
        "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
        "        if imm_matches:\n",
        "            imm = imm_matches[0]\n",
        "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
        "\n",
        "        return features\n",
        "\n",
        "    def encode_nodelabel(self, node_label):\n",
        "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
        "        features = self.tokenize(node_label)\n",
        "\n",
        "        # Encode each feature\n",
        "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
        "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
        "\n",
        "        # Encode registers (take up to 3, pad if fewer)\n",
        "        register_indices = []\n",
        "        for i in range(min(3, len(features['registers']))):\n",
        "            reg = features['registers'][i]\n",
        "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
        "            register_indices.append(reg_idx)\n",
        "\n",
        "        # Pad register indices if needed\n",
        "        while len(register_indices) < 3:\n",
        "            register_indices.append(0)  # 0 for padding\n",
        "\n",
        "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
        "            features['memory_pattern'],\n",
        "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
        "        )\n",
        "\n",
        "        immediate_idx = self.immediate_vocab.get(\n",
        "            features['immediate'],\n",
        "            self.immediate_vocab[self.UNK_TOKEN]\n",
        "        )\n",
        "\n",
        "        # Combine all indices into a feature vector\n",
        "        encoded = np.array([\n",
        "            node_type_idx,\n",
        "            operation_idx,\n",
        "            register_indices[0],\n",
        "            register_indices[1],\n",
        "            register_indices[2],\n",
        "            memory_pattern_idx,\n",
        "            immediate_idx\n",
        "        ], dtype=np.int64)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def encode_graph(self, digraph):\n",
        "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
        "        node_features = {}\n",
        "\n",
        "        for node_id in digraph.nodes():\n",
        "            label = digraph.nodes[node_id].get('label', '')\n",
        "            node_features[node_id] = self.encode_nodelabel(label)\n",
        "\n",
        "        return node_features\n",
        "\n",
        "    def encode_hash(self, hash_file):\n",
        "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
        "\n",
        "    def get_vocab_sizes(self):\n",
        "        \"\"\"Return the size of each vocabulary\"\"\"\n",
        "        return {\n",
        "            'node_type': len(self.node_type_vocab),\n",
        "            'operation': len(self.operation_vocab),\n",
        "            'register': len(self.register_vocab),\n",
        "            'memory_pattern': len(self.memory_pattern_vocab),\n",
        "            'immediate': len(self.immediate_vocab)\n",
        "        }\n",
        "\n",
        "    def get_tokens_and_counts(self, graph_l):\n",
        "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
        "        node_types = []\n",
        "        operations = []\n",
        "        registers = []\n",
        "        memory_patterns = []\n",
        "        immediates = []\n",
        "        unique_hashes = []\n",
        "        self.hash_to_id = {}\n",
        "        self.id_to_hash = {}\n",
        "\n",
        "        for item in graph_l:\n",
        "            graph = item['graph_input']\n",
        "            for node_id, node_attr in graph.nodes(data=True):\n",
        "                label = node_attr.get('label', '')\n",
        "\n",
        "                node_type_match = self.patterns['node_type'].search(label)\n",
        "                if node_type_match:\n",
        "                    node_types.append(node_type_match.group(1))\n",
        "\n",
        "                op_match = self.patterns['operation'].search(label)\n",
        "                if op_match:\n",
        "                    operations.append(op_match.group(1))\n",
        "\n",
        "                reg_matches = self.patterns['registers'].findall(label)\n",
        "                registers.extend(reg_matches)\n",
        "\n",
        "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
        "                for mem in mem_matches:\n",
        "                    pattern = self._memory_to_pattern(mem)\n",
        "                    memory_patterns.append(pattern)\n",
        "\n",
        "                imm_matches = self.patterns['immediate'].findall(label)\n",
        "                immediates.extend(imm_matches)\n",
        "\n",
        "            unique_hashes.append(item['name'])\n",
        "\n",
        "        return {\n",
        "            'node_types': node_types,\n",
        "            'operations': operations,\n",
        "            'registers': registers,\n",
        "            'memory_patterns': memory_patterns,\n",
        "            'immediates': immediates,\n",
        "            'unique_hashes': unique_hashes,\n",
        "        }\n",
        "\n",
        "class AssemblyGraphDataset:\n",
        "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
        "        \"\"\"\n",
        "        Prepares a dataset of assembly graphs for GAT model training\n",
        "        Args:\n",
        "            graph_list: List of NetworkX digraphs representing assembly code\n",
        "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
        "            node_feature_dim: Dimension of node embeddings\n",
        "        \"\"\"\n",
        "        self.graph_list = graph_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.node_feature_dim = node_feature_dim\n",
        "\n",
        "        # Initialize embedding layers for each feature type\n",
        "        num_feature_type = 5\n",
        "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
        "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
        "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
        "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
        "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
        "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
        "\n",
        "        # Process graphs into PyTorch Geometric Data objects\n",
        "        self.data_list = []\n",
        "        for graph in self.graph_list:\n",
        "            # process graph info\n",
        "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
        "\n",
        "    def _process_graph(self, graph, hash_name):\n",
        "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
        "        # Create a node ID mapping for consecutive IDs\n",
        "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
        "\n",
        "        # Get node features\n",
        "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
        "\n",
        "        # Convert to tensor-friendly format\n",
        "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
        "        for node_id, features in node_features_dict.items():\n",
        "            x[node_mapping[node_id]] = torch.tensor(features)\n",
        "\n",
        "        # Create edge index\n",
        "        edge_list = list(graph.edges())\n",
        "        if not edge_list:\n",
        "            # Handle case with no edges\n",
        "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "        else:\n",
        "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
        "\n",
        "        # Endcode Hash name\n",
        "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
        "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
        "\n",
        "        # Get embeddings before creating Data object\n",
        "        x = self.get_embeddings(x)\n",
        "\n",
        "        # Create Data object\n",
        "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
        "        return data\n",
        "\n",
        "    def optimize_edge_index(self, edge_list, node_mapping):\n",
        "        \"\"\"\n",
        "        Optimizes the creation of edge_index tensor for graph representation.\n",
        "        Args:\n",
        "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
        "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
        "        Returns:\n",
        "            torch.Tensor: Optimized edge_index tensor.\n",
        "        \"\"\"\n",
        "        if not edge_list:\n",
        "            return torch.zeros((2, 0), dtype=torch.long)\n",
        "\n",
        "        num_edges = len(edge_list)\n",
        "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
        "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
        "\n",
        "        for i, (src, tgt) in enumerate(edge_list):\n",
        "            src_indices[i] = node_mapping[src]\n",
        "            tgt_indices[i] = node_mapping[tgt]\n",
        "\n",
        "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "    def get_embeddings(self, x):\n",
        "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
        "        # Split features into their components\n",
        "        node_type_idx = x[:, 0]\n",
        "        operation_idx = x[:, 1]\n",
        "        register_idx1 = x[:, 2]\n",
        "        register_idx2 = x[:, 3]\n",
        "        register_idx3 = x[:, 4]\n",
        "        memory_pattern_idx = x[:, 5]\n",
        "        immediate_idx = x[:, 6]\n",
        "\n",
        "        # Get embeddings for each component\n",
        "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
        "        operation_emb = self.operation_embedding(operation_idx)\n",
        "\n",
        "        # Combine register embeddings (average them)\n",
        "        register_emb = (self.register_embedding(register_idx1) +\n",
        "                        self.register_embedding(register_idx2) +\n",
        "                        self.register_embedding(register_idx3)) / 3\n",
        "\n",
        "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
        "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        return torch.cat([\n",
        "            node_type_emb,\n",
        "            operation_emb,\n",
        "            register_emb,\n",
        "            memory_pattern_emb,\n",
        "            immediate_emb\n",
        "        ], dim=1)\n",
        "\n",
        "\"\"\"\n",
        "Graph Attention Network for assembly code analysis\n",
        "Args:\n",
        "    dataset: AssemblyGraphDataset instance\n",
        "    hidden_dim: Hidden dimension of GAT layers\n",
        "    output_dim: Output dimension of node embeddings\n",
        "    heads: Number of attention heads\n",
        "    dropout: Dropout rate\n",
        "\"\"\"\n",
        "class AssemblyGAT(nn.Module):\n",
        "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
        "        super(AssemblyGAT, self).__init__()\n",
        "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
        "        self.hash_dim = hash_dim\n",
        "\n",
        "    def forward(self, x, edge_index, hash_encoded, batch):\n",
        "        \"\"\"Forward pass through the GAT model\"\"\"\n",
        "        # First GAT layer with activation and dropout\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "# Function to parse digraph string (as in previous example)\n",
        "def parse_digraph_string(digraph_str):\n",
        "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
        "    # Simple parser for the provided format\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Regular expressions for node and edge definitions\n",
        "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
        "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
        "\n",
        "    # Extract nodes and edges\n",
        "    for line in digraph_str.strip().split('\\n'):\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip the Digraph G { and } lines\n",
        "        if line == 'Digraph G {' or line == '}':\n",
        "            continue\n",
        "\n",
        "        # Parse node definitions\n",
        "        node_match = node_pattern.search(line)\n",
        "        if node_match:\n",
        "            node_id = node_match.group(1)\n",
        "            label = node_match.group(2)\n",
        "            G.add_node(node_id, label=label)\n",
        "            continue\n",
        "\n",
        "        # Parse edge definitions\n",
        "        edge_match = edge_pattern.search(line)\n",
        "        if edge_match:\n",
        "            source = edge_match.group(1)\n",
        "            target = edge_match.group(2)\n",
        "            G.add_edge(source, target)\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "INiZZqldtSu-"
      },
      "outputs": [],
      "source": [
        "# Create empty list to store dataframes\n",
        "# init parameters to loop the file loading by batch\n",
        "files_meta_list = set(df_meta_train['name'].astype(str))\n",
        "random_seed = 42\n",
        "\n",
        "def split_dataloader(dataset, list_of_hash, batch_size, validation_split=0.5, shuffle=True):\n",
        "    random.seed(random_seed) #set random seed for random module.\n",
        "    torch.manual_seed(random_seed) #set random seed for torch.\n",
        "\n",
        "    validation_size = int(len(dataset) * validation_split)\n",
        "    train_size = len(dataset) - validation_size\n",
        "\n",
        "    indices = list(range(len(dataset)))\n",
        "    if shuffle:\n",
        "        random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    validation_indices = indices[train_size:]\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    validation_dataset = Subset(dataset, validation_indices)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # Shuffle is handled by the initial indices shuffling.\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    train_hash = [list_of_hash[i] for i in train_indices]\n",
        "    val_hash = [list_of_hash[i] for i in validation_indices]\n",
        "\n",
        "    return train_dataloader, validation_dataloader, train_hash, val_hash\n",
        "\n",
        "def process_batch(file_batch):\n",
        "    \"\"\"\n",
        "    Processes a batch of files.\n",
        "    Args:\n",
        "        file_batch (list): A list of file paths.\n",
        "    \"\"\"\n",
        "    graph_list_curr = []\n",
        "    file_with_err = 0\n",
        "\n",
        "    for full_path_file in file_batch:\n",
        "        try:\n",
        "            with open(full_path_file, 'r') as f:\n",
        "                hash_file = full_path_file.split('.jso')[0]\n",
        "                hash_file = hash_file.split(split_char)[-1]\n",
        "\n",
        "                #f = open(full_path_file, 'r')\n",
        "                digraph_str = f.read()\n",
        "\n",
        "                # test if file content has no error tag\n",
        "                if 'ERROR' in digraph_str:\n",
        "                    file_with_err += 1\n",
        "                    continue\n",
        "\n",
        "                G = parse_digraph_string(digraph_str)\n",
        "                graph_list_curr.append({\n",
        "                    'name': hash_file,\n",
        "                    'graph_input' : G\n",
        "                })\n",
        "                f.close()\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found: {full_path_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {full_path_file}: {e}\")\n",
        "\n",
        "    return graph_list_curr\n",
        "\n",
        "def get_metadata_from_hash(hash_list):\n",
        "    hash_meta_values = []\n",
        "    for hash_name in hash_list:\n",
        "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
        "        df_y_values.drop(columns=['name'], inplace=True)\n",
        "        if len(df_y_values) > 0 :\n",
        "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
        "        else:\n",
        "            print(f\"{hash_name} not found in metadata\")\n",
        "    return hash_meta_values\n",
        "\n",
        "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
        "    if os.path.exists(model_save_file):\n",
        "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
        "\n",
        "        model.train()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        count_batch = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "            # get the different components of the dataset\n",
        "            x = batch.x.to(device)\n",
        "            edge_index = batch.edge_index.to(device)\n",
        "            hash_encoded = batch.hash_encoded.to(device)\n",
        "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
        "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
        "            y_label = get_metadata_from_hash(list_hash_batch)\n",
        "            y_label = np.array(y_label)\n",
        "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
        "\n",
        "            bincount_list = torch.bincount(batch.batch).tolist()\n",
        "            if any(bincount_list): #checks if any values are not zero.\n",
        "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
        "                batch_p = batch_p.to(device)\n",
        "            else:\n",
        "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
        "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
        "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(x, edge_index, hash_encoded, batch_p)\n",
        "            count_batch += 1\n",
        "\n",
        "            if output.shape != y_label.shape:\n",
        "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
        "                break\n",
        "\n",
        "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # save model trained\n",
        "    torch.save(model.state_dict(), model_save_file)\n",
        "\n",
        "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
        "    if os.path.exists(model_save_file):\n",
        "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
        "\n",
        "    model.eval() #set the model to evaluation mode.\n",
        "    all_true_label = []\n",
        "    all_pred_label = []\n",
        "    hash_v = []\n",
        "\n",
        "    count_batch = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "            x = batch.x.to(device)\n",
        "            edge_index = batch.edge_index.to(device)\n",
        "            hash_encoded = batch.hash_encoded.to(device)\n",
        "\n",
        "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
        "            y_label = get_metadata_from_hash(list_hash_batch)\n",
        "            y_label = np.array(y_label)\n",
        "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
        "\n",
        "            bincount_list = torch.bincount(batch.batch).tolist()\n",
        "            if any(bincount_list): #checks if any values are not zero.\n",
        "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
        "                batch_p = batch_p.to(device)\n",
        "            else:\n",
        "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
        "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
        "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(x, edge_index, hash_encoded, batch_p)\n",
        "            count_batch += 1\n",
        "\n",
        "            if output.shape != y_label.shape:\n",
        "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
        "                break\n",
        "\n",
        "            predicted_labels_prob = torch.sigmoid(output)\n",
        "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
        "            all_true_label.append(y_label.cpu())\n",
        "            all_pred_label.append(prediction_labels.cpu())\n",
        "            #hash_v.append(list_hash_batch)\n",
        "\n",
        "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
        "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
        "\n",
        "    return pred_labels_tensor, true_labels_tensor\n",
        "\n",
        "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
        "\n",
        "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
        "    num_files = len(full_file_paths)\n",
        "    nb_batch = (num_files // batch_files_size) +1\n",
        "\n",
        "    hash_v = []\n",
        "    hash_t = []\n",
        "    all_true_label = []\n",
        "    all_pred_label = []\n",
        "\n",
        "    all_counts = {\n",
        "        'node_types': [],\n",
        "        'operations': [],\n",
        "        'registers': [],\n",
        "        'memory_patterns': [],\n",
        "        'immediates': [],\n",
        "        'unique_hashes': [],\n",
        "    }\n",
        "    print(f\"**** Tokenization Processing\")\n",
        "    for i in range(0, num_files, batch_files_size):\n",
        "        batch_files = full_file_paths[i:i + batch_files_size]\n",
        "\n",
        "        #print(f\"Tokenization Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
        "        batch_graph_list = process_batch(batch_files)\n",
        "        if len(batch_graph_list) == 0:\n",
        "            print(\"graph list = 0\")\n",
        "            break\n",
        "\n",
        "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list) # you will need to implement this\n",
        "        all_counts.update(tokens)\n",
        "\n",
        "    # Create a tokenizer from token counts\n",
        "    tokenizer.fit_from_counts(all_counts)\n",
        "\n",
        "    print(f\"**** Dataset Processing & Training\")\n",
        "    for i in range(0, num_files, batch_files_size):\n",
        "        batch_files = full_file_paths[i:i + batch_files_size]\n",
        "        print(f\"Dataset Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
        "\n",
        "        batch_graph_list = process_batch(batch_files)\n",
        "        if len(batch_graph_list) == 0:\n",
        "            print(\"graph list = 0\")\n",
        "            break\n",
        "\n",
        "        dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
        "        curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
        "\n",
        "        list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
        "\n",
        "        if mode == 'train':\n",
        "            train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=3)\n",
        "            hash_t.append(list_of_hash)\n",
        "        else:\n",
        "            pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
        "            all_true_label.append(true_labels.cpu())\n",
        "            all_pred_label.append(pred_labels.cpu())\n",
        "            hash_v.append(list_of_hash)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # save all labels as tensor to return if mode= 'validation'\n",
        "    if mode == 'validation':\n",
        "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
        "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
        "\n",
        "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
        "    elif mode == 'train':\n",
        "        return hash_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avU1v3vVtSu-",
        "outputId": "ea90c29a-07c0-4841-d92f-62415f5d899d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== batch: rep_0 ========== \n",
            "size 103.576\n",
            "size 15.984\n",
            "size 303.471\n",
            "size 11229.717\n",
            "size 3728.521\n",
            "size 439.94\n",
            "size 1114.102\n",
            "count\n",
            "Number of files with error: 1\n",
            "There are 5 files for train\n",
            "**** Tokenization Processing\n",
            "**** Dataset Processing & Training\n",
            "Dataset Processing batch 1/3: 2 files\n",
            "Dataset Processing batch 2/3: 2 files\n",
            "Dataset Processing batch 3/3: 1 files\n",
            "There are 1 files for validation\n",
            "**** Tokenization Processing\n",
            "**** Dataset Processing & Training\n",
            "Dataset Processing batch 1/1: 1 files\n",
            "**** F1 score: 96.69 \n",
            "\n",
            "========== batch: rep_500 ========== \n",
            "size 13.043\n",
            "size 1411.096\n",
            "size 253.194\n",
            "size 731.409\n",
            "size 3668.997\n",
            "size 823.316\n",
            "size 459.689\n",
            "count\n",
            "Number of files with error: 1\n",
            "There are 5 files for train\n",
            "**** Tokenization Processing\n",
            "**** Dataset Processing & Training\n",
            "Dataset Processing batch 1/3: 2 files\n",
            "Dataset Processing batch 2/3: 2 files\n",
            "Dataset Processing batch 3/3: 1 files\n",
            "There are 1 files for validation\n",
            "**** Tokenization Processing\n",
            "**** Dataset Processing & Training\n",
            "Dataset Processing batch 1/1: 1 files\n",
            "**** F1 score: 94.48 \n",
            "\n",
            "========== batch: rep_1000 ========== \n",
            "size 1127.573\n",
            "size 110.752\n",
            "size 9782.704\n",
            "size 6197.761\n",
            "size 123.78\n",
            "size 172.425\n",
            "size 2466.819\n",
            "count\n",
            "Number of files with error: 1\n",
            "There are 5 files for train\n",
            "**** Tokenization Processing\n",
            "**** Dataset Processing & Training\n",
            "Dataset Processing batch 1/3: 2 files\n",
            "Dataset Processing batch 2/3: 2 files\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[152]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=\u001b[32m0.1\u001b[39m, rep_batch=batch_curr)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m hash_t = \u001b[43mrun_files_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_files_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_files_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[32m     24\u001b[39m all_pred_label_tensor, all_true_label_tensor, hash_v = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode=\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mrun_files_in_batches\u001b[39m\u001b[34m(full_file_paths, batch_files_size, mode)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgraph list = 0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m dataset = \u001b[43mAssemblyGraphDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_graph_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feature_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_feature_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    230\u001b[39m list_of_hash = [graph[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m batch_graph_list]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 296\u001b[39m, in \u001b[36mAssemblyGraphDataset.__init__\u001b[39m\u001b[34m(self, graph_list, tokenizer, node_feature_dim)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.data_list = []\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.graph_list:\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# process graph info\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28mself\u001b[39m.data_list.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgraph_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 317\u001b[39m, in \u001b[36mAssemblyGraphDataset._process_graph\u001b[39m\u001b[34m(self, graph, hash_name)\u001b[39m\n\u001b[32m    315\u001b[39m     edge_index = torch.zeros((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m), dtype=torch.long)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     edge_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize_edge_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Endcode Hash name\u001b[39;00m\n\u001b[32m    320\u001b[39m encoded_hash = \u001b[38;5;28mself\u001b[39m.tokenizer.encode_hash(hash_name)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 348\u001b[39m, in \u001b[36mAssemblyGraphDataset.optimize_edge_index\u001b[39m\u001b[34m(self, edge_list, node_mapping)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (src, tgt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(edge_list):\n\u001b[32m    347\u001b[39m     src_indices[i] = node_mapping[src]\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     tgt_indices[i] = node_mapping[tgt]\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack((src_indices, tgt_indices), dim=\u001b[32m0\u001b[39m).contiguous()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "###############################################################\n",
        "# Main init and training sequence\n",
        "###############################################################\n",
        "# Batch params\n",
        "batch_data_size = 1\n",
        "batch_files_size = 2\n",
        "num_feature_dim = 15\n",
        "\n",
        "# Initialize model\n",
        "tokenizer = HierarchicalAssemblyTokenizer()\n",
        "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "for i in range(0, 22501, 500):\n",
        "  batch_curr = f'rep_{i}'\n",
        "  print(f\"========== batch: {batch_curr} ========== \")\n",
        "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch=batch_curr)\n",
        "\n",
        "  # train\n",
        "  hash_t = run_files_in_batches(list_train, batch_files_size=batch_files_size, mode='train')\n",
        "\n",
        "  # validate\n",
        "  all_pred_label_tensor, all_true_label_tensor, hash_v = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode='validation')\n",
        "  f1_macro = f1_score(all_true_label_tensor, all_pred_label_tensor, average='macro', zero_division=1)\n",
        "  print(f\"**** F1 score: {f1_macro*100:.2f} \\n\")\n",
        "\n",
        "  if not os.path.exists(filename_trained):\n",
        "      with open(filename_trained, 'w', newline='') as csvfile:\n",
        "          writer = csv.writer(csvfile)\n",
        "          writer.writerow([\"files_trained\"])\n",
        "          for item in hash_t:\n",
        "              for value in item:\n",
        "                  writer.writerow([value])\n",
        "  else:\n",
        "      with open(filename_trained, 'a', newline='') as csvfile:\n",
        "          writer = csv.writer(csvfile)\n",
        "          for item in hash_t:\n",
        "              for value in item:\n",
        "                  writer.writerow([value])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUGQiK-mtSu-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test to be done later, when full training is completed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\ndef load_test_files(test_path_dir, batch_files_size=50):\\n\\n    for root, _, files in os.walk(test_path_dir):\\n        print(f\"There are {len(files)} files in the test directory: {root}\")\\n\\n        full_file_paths = [os.path.join(root, file) for file in files]\\n        print(f\"**** Tokenization Processing\")\\n        batch_graph_list = process_batch(full_file_paths)\\n        tokenizer.fit(batch_graph_list)\\n\\n        print(f\"**** Dataset Processing \")\\n        test_dataset = AssemblyGraphDataset(batch_graph_list, tokenizer,  node_feature_dim=num_feature_dim)\\n        test_dataloader = DataLoader(test_dataset, batch_size=batch_data_size, shuffle=False)\\n        list_of_hash = [graph[\\'name\\'] for graph in batch_graph_list]\\n\\n    return test_dataloader, list_of_hash\\n\\n##############################################\\ntest_loader, list_of_hash = load_test_files(test_path_dir, batch_files_size=batch_files_size)\\nmodel.load_state_dict(torch.load(model_save_file, weights_only=True))\\nmodel.eval()\\n\\nwith torch.no_grad():\\n    columns = [\\'hash\\', \\'labels\\']\\n    all_pred_labels = pd.DataFrame(columns=columns)\\n\\n    count_batch = 0\\n    # loop on the batch from the dataloader\\n    for batch in test_loader:\\n        x = batch.x.to(device)\\n        edge_index = batch.edge_index.to(device)\\n        hash_encoded = batch.hash_encoded.to(device)\\n\\n        # Create batch vector for global pooling\\n        num_graphs = batch.num_graphs #obtain the number of graphs in the batch\\n        num_nodes_per_graph = torch.bincount(batch.batch).tolist() # get number of nodes per graph.\\n        if any(num_nodes_per_graph): #checks if any values are not zero.\\n            batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(torch.bincount(batch.batch).tolist())])\\n            batch_p = batch_p.to(device)\\n        else:\\n            # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\\n            print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\\n            batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\\n\\n        output = model(x, edge_index, hash_encoded, batch_p)\\n        count_batch += 1\\n\\n        predicted_labels_prob = torch.sigmoid(output)\\n        prediction_labels = (predicted_labels_prob > 0.5).int()\\n\\n        if len(list_of_hash) == 1:\\n            list_hash_batch = list_of_hash\\n        else:\\n            list_hash_batch = list_of_hash[count_batch*batch_data_size:(count_batch+1)*batch_data_size]\\n\\n        if len(list_hash_batch) != len(prediction_labels):\\n            print(\"Error: list of pred <> list of hash : \", len(list_hash_batch), len(prediction_labels))\\n            break\\n\\n        new_rows = []\\n        for i, hash_val in enumerate(list_hash_batch):\\n            label_list = prediction_labels[i].tolist()  # Convert tensor row to list\\n            int_label_list = [int(val) for val in label_list] #convert to int\\n            new_rows.append({\\'hash\\': hash_val, \\'labels\\': int_label_list})\\n\\n        all_pred_labels = pd.concat([all_pred_labels, pd.DataFrame(new_rows)], ignore_index=True)\\n\\nprint(f\"Test end: nb of graph analyzed: {len(list_of_hash)}, total pred labels: {len(all_pred_labels)} \")\\nall_pred_labels.head()\\n\\nall_pred_labels.to_csv(filename_test_results, index=False)\"\\n'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "###############################################################\n",
        "# Main Test sequence\n",
        "#\n",
        "# ==> to run later, when full training is completed\n",
        "#\n",
        "###############################################################\n",
        "\n",
        "print(\"test to be done later, when full training is completed\")\n",
        "\n",
        "\"\"\"\n",
        "def load_test_files(test_path_dir, batch_files_size=50):\n",
        "\n",
        "    for root, _, files in os.walk(test_path_dir):\n",
        "        print(f\"There are {len(files)} files in the test directory: {root}\")\n",
        "\n",
        "        full_file_paths = [os.path.join(root, file) for file in files]\n",
        "        print(f\"**** Tokenization Processing\")\n",
        "        batch_graph_list = process_batch(full_file_paths)\n",
        "        tokenizer.fit(batch_graph_list)\n",
        "\n",
        "        print(f\"**** Dataset Processing \")\n",
        "        test_dataset = AssemblyGraphDataset(batch_graph_list, tokenizer,  node_feature_dim=num_feature_dim)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=batch_data_size, shuffle=False)\n",
        "        list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
        "\n",
        "    return test_dataloader, list_of_hash\n",
        "\n",
        "##############################################\n",
        "test_loader, list_of_hash = load_test_files(test_path_dir, batch_files_size=batch_files_size)\n",
        "model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    columns = ['hash', 'labels']\n",
        "    all_pred_labels = pd.DataFrame(columns=columns)\n",
        "\n",
        "    count_batch = 0\n",
        "    # loop on the batch from the dataloader\n",
        "    for batch in test_loader:\n",
        "        x = batch.x.to(device)\n",
        "        edge_index = batch.edge_index.to(device)\n",
        "        hash_encoded = batch.hash_encoded.to(device)\n",
        "\n",
        "        # Create batch vector for global pooling\n",
        "        num_graphs = batch.num_graphs #obtain the number of graphs in the batch\n",
        "        num_nodes_per_graph = torch.bincount(batch.batch).tolist() # get number of nodes per graph.\n",
        "        if any(num_nodes_per_graph): #checks if any values are not zero.\n",
        "            batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(torch.bincount(batch.batch).tolist())])\n",
        "            batch_p = batch_p.to(device)\n",
        "        else:\n",
        "            # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
        "            print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
        "            batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
        "\n",
        "        output = model(x, edge_index, hash_encoded, batch_p)\n",
        "        count_batch += 1\n",
        "\n",
        "        predicted_labels_prob = torch.sigmoid(output)\n",
        "        prediction_labels = (predicted_labels_prob > 0.5).int()\n",
        "\n",
        "        if len(list_of_hash) == 1:\n",
        "            list_hash_batch = list_of_hash\n",
        "        else:\n",
        "            list_hash_batch = list_of_hash[count_batch*batch_data_size:(count_batch+1)*batch_data_size]\n",
        "\n",
        "        if len(list_hash_batch) != len(prediction_labels):\n",
        "            print(\"Error: list of pred <> list of hash : \", len(list_hash_batch), len(prediction_labels))\n",
        "            break\n",
        "\n",
        "        new_rows = []\n",
        "        for i, hash_val in enumerate(list_hash_batch):\n",
        "            label_list = prediction_labels[i].tolist()  # Convert tensor row to list\n",
        "            int_label_list = [int(val) for val in label_list] #convert to int\n",
        "            new_rows.append({'hash': hash_val, 'labels': int_label_list})\n",
        "\n",
        "        all_pred_labels = pd.concat([all_pred_labels, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "\n",
        "print(f\"Test end: nb of graph analyzed: {len(list_of_hash)}, total pred labels: {len(all_pred_labels)} \")\n",
        "all_pred_labels.head()\n",
        "\n",
        "all_pred_labels.to_csv(filename_test_results, index=False)\"\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".my-dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
