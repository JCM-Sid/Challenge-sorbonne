{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15777,
     "status": "ok",
     "timestamp": 1742720364859,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "vx1v6NiktSu7",
    "outputId": "da509e60-8c4e-49f1-8128-e720702bae6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n",
      "The current working directory is: c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\n"
     ]
    }
   ],
   "source": [
    "# Challenge Sorbonne - DST\n",
    "#\n",
    "#!pip install torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "#import rarfile\n",
    "import networkx as nx\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.optim import AdamW\n",
    "#!pip install torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is\", device)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "#env_system = os.environ\n",
    "#print(env_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25892,
     "status": "ok",
     "timestamp": 1742720504835,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "KaGZYw4ZtSu8",
    "outputId": "963f3e01-2c76-4511-da95-a91064c5a181"
   },
   "outputs": [],
   "source": [
    "# for Google Colab only\n",
    "if 'GOOGLE_COLAB' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 2051,
     "status": "ok",
     "timestamp": 1742720512835,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "eLIITiH84EOo",
    "outputId": "43e12de7-4147-4287-a704-7e0dffa83b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is: c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  9fbf213113ba0a18dc2642f83b1201541428fd7951d6a8...   \n",
       "1  1b35c9dbf3cd9ac60015aaa6cd451c898defa6dac1ff43...   \n",
       "2  bf8d307a136a936f7338c1f2eec773c4eb1c802cab77da...   \n",
       "3  1e51933903f0358c0b635f863368eb15a61cd3442bc5bf...   \n",
       "4  8a6503fe68d699f8a31531c157e9da931192cd7e3ec809...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                  0       1           0   \n",
       "1                                  0       0           0   \n",
       "2                                  0       0           0   \n",
       "3                                  0       0           0   \n",
       "4                                  0       0           0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                  0                   0                    0   \n",
       "1                  0                   0                    0   \n",
       "2                  0                   0                    0   \n",
       "3                  0                   0                    0   \n",
       "4                  0                   0                    0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0                0  ...       0     0     0                         0   \n",
       "1                0  ...       0     0     0                         0   \n",
       "2                0  ...       0     0     0                         0   \n",
       "3                0  ...       0     0     0                         0   \n",
       "4                0  ...       0     0     0                         0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                     0                    0                      0   \n",
       "1                     0                    0                      1   \n",
       "2                     0                    0                      0   \n",
       "3                     0                    0                      0   \n",
       "4                     0                    0                      0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0           0         0     0  \n",
       "1           0         0     0  \n",
       "2           0         0     0  \n",
       "3           0         0     0  \n",
       "4           0         0     0  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(f\"The current working directory is: {current_directory}\")\n",
    "\n",
    "# init du repertoire des données\n",
    "if current_directory == '/usr/users/msiac/msiac_10/Sorbonne' : #DCE\n",
    "    training_path_dir =   \"./training\"\n",
    "    test_path_dir = \"./test\"\n",
    "\n",
    "    train_meta_data = 'training_set_metadata.csv'\n",
    "    model_save_file =  'model_sorbonne_weights.pth'\n",
    "\n",
    "    split_char = '/'\n",
    "    filename_test_results = \"test_prediction.csv\"\n",
    "    filename_trained = \"list_hash_trained.csv\"\n",
    "    file_split_training = 'split_training-dce.csv'\n",
    "\n",
    "elif current_directory == '/content': # Google Colab\n",
    "    training_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set\"\n",
    "    test_path_dir = \"/content/drive/MyDrive/Sorbonne_Data_challenge/folder_test_set\"\n",
    "\n",
    "    train_meta_data =  '/content/drive/MyDrive/Sorbonne_Data_challenge/training_set_metadata.csv'\n",
    "\n",
    "    model_save_file =  '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/model_sorbonne_weights.pth'\n",
    "    file_split_training = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/split_training-colab.csv'\n",
    "    split_char = '/'\n",
    "\n",
    "    filename_test_results = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/test_prediction.csv'\n",
    "    filename_trained = '/content/drive/MyDrive/Colab Notebooks/Sorbonne-Challenge/list_hash_trained.csv'\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\Challenge-Sorbonne\": #PC1\n",
    "    split_char = '\\\\'\n",
    "    \n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\".\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  r'..\\..\\Data\\ChallengeSorbonne\\training_set_metadata.csv'\n",
    "    file_split_training = r'.\\split_training-pc.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_path_dir = r\"G:\\Mon Drive\\Colab Notebooks\\Sorbonne-Challenge\"\n",
    "    test_init_file = r\".\\test_set_to_predict.csv\"\n",
    "    filename_test_results = r\".\\test_prediction.csv\"\n",
    "    \n",
    "    model_save_file =  r'.\\model_sorbonne_weights.pth'\n",
    "    filename_test_results = r\".\\test_prediction.csv\"\n",
    "\n",
    "elif current_directory == r\"c:\\Users\\jch_m\\Documents Perso\\DevPython\\Challenge-sorbonne\": #PC2\n",
    "    split_char = '\\\\'\n",
    "    \n",
    "    #training_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\"\n",
    "    training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    #training_path_dir =   r\"..\\..\\Data\\ChallengeSorbonne\\training_s\"\n",
    "    filename_trained = r\".\\list_hash_trained.csv\"\n",
    "\n",
    "    train_meta_data =  'training_set_metadata.csv'\n",
    "    file_split_training = r'.\\split_training-pc.csv'\n",
    "\n",
    "    #test_path_dir =   r\"D:\\ChallengeDST\\folder_training_set\\folder_training_set\"\n",
    "    #test_path_dir = r\"..\\..\\Data\\ChallengeSorbonne\\training\"\n",
    "    test_path_dir = r\"G:\\.shortcut-targets-by-id\\19Ls71oZUG1aa1uTMu1laH6Nt3qK5gSZK\\Sorbonne_Data_challenge\\folder_test_set\"\n",
    "    test_init_file = \"test_set_to_predict.csv\"\n",
    "    filename_test_results = \"test_prediction.csv\"\n",
    "    model_save_file =  'model_sorbonne_weights.pth'\n",
    "else:\n",
    "    print(\"**** ERROR: init files names and directories not done - root directory and environment not identified\")\n",
    "\n",
    "# read CSV input and save in df\n",
    "df_meta_train = pd.read_csv(train_meta_data, sep=\";\")\n",
    "df_meta_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1742720516871,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "wCU7s5ZbtSu9",
    "outputId": "b6be6075-4089-4471-e8df-b5d10d46ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23102 entries, 0 to 23101\n",
      "Columns: 454 entries, name to yoda\n",
      "dtypes: int64(453), object(1)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meta_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1742720570710,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "bOuNPC0iZACE",
    "outputId": "b1fe1803-1abf-430b-cd54-b231adc20e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\list_hash_trained.csv\n",
      "taille : 5076\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>793a1bda32069f37ba201167d90e8c16c004008c4fd467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819d0b70a905ae5f8bef6c47423964359c2a90a168414f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>892e7b1463b98d6f5b41579c6b314de3a723af2c67d88f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       files_trained\n",
       "0                                      files_trained\n",
       "1  861f2c5f07c9e1c7d24c2e34eb47ff3129cd39a2227a25...\n",
       "2  793a1bda32069f37ba201167d90e8c16c004008c4fd467...\n",
       "3  819d0b70a905ae5f8bef6c47423964359c2a90a168414f...\n",
       "4  892e7b1463b98d6f5b41579c6b314de3a723af2c67d88f..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filename_trained)\n",
    "df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "print(\"taille :\",len(df_files_trained))\n",
    "df_files_trained.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1719,
     "status": "ok",
     "timestamp": 1742720581568,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "oRBCbVPaOKjz",
    "outputId": "1735a612-3320-4dc1-d85e-a6bb1e15ba58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep_0' 'rep_500' 'rep_1000']\n",
      "['D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04333849d7eb415a3986dc1afe0041723184b7f47aeeec5ffa07a0f9c222fee3.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\04390cd4165bf34f62658cea0fa16116037e391a86ed59d09493c9d141f92276.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043a5b4be0a8f0500e88a2ffc91d15bf4212fa12a3f07be04654b67adc41d21c.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043cf998d7059cacaf626f443181c67f91adeb2bc7088ca918e1aee94838d23d.json', 'D:\\\\ChallengeDST\\\\folder_training_set\\\\folder_training_set\\\\043e2a8241a6eb0aa66de40e35e75dce0f1d047a6d9e4c934b482f7c588f9c61.json']\n"
     ]
    }
   ],
   "source": [
    "# load & check traing split\n",
    "df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
    "full_file_list = df_training_split[df_training_split['batch'] == 'rep_500']['orign path'].tolist()\n",
    "print(df_training_split['batch'].unique()[:3])\n",
    "print(full_file_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1742721120476,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "_gU6S_PgtSu9"
   },
   "outputs": [],
   "source": [
    "def process_graph_directory(file_directory, val_size=0.1, rep_batch='rep_0'):\n",
    "    \"\"\"\n",
    "    Scan the training directory\n",
    "    get the hash name of the files, check and exclude hash file with error\n",
    "    retour a list of hash for training, validation and error\n",
    "    \"\"\"\n",
    "\n",
    "    list_train_hash, list_val_hash, list_err_hash  = [], [], []\n",
    "    file_with_err = 0\n",
    "    count_files = 0\n",
    "    max_file = 0\n",
    "\n",
    "    #file_split_training = r\"C:\\Users\\jch_m\\DocumentsPerso\\Centrale-IAC\\Cours\\Python-ML\\ProjectMaster\\ChallengeSorbonne\\split_training-colab.csv\"\n",
    "    df_files_trained = pd.read_csv(filename_trained, sep=\",\")\n",
    "    df_training_split = pd.read_csv(file_split_training, sep=\";\")\n",
    "    full_file_list = df_training_split[df_training_split['batch']== rep_batch]['orign path'].tolist()\n",
    "\n",
    "    for full_path_file in full_file_list:\n",
    "        # test if filename in the trained list\n",
    "        hash_name = full_path_file.split('.jso')[0]\n",
    "        hash_name = hash_name.split(split_char)[-1]\n",
    "\n",
    "        # file already trained\n",
    "        if hash_name in df_files_trained['files_trained'].values:\n",
    "            #print(\"trained\")\n",
    "            continue\n",
    "\n",
    "        # files not in metadata\n",
    "        if hash_name not in df_meta_train['name'].values:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"meta\", hash_name, full_path_file)\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(full_path_file):\n",
    "            file_size = os.path.getsize(full_path_file)\n",
    "            if file_size/1000 > 100_000: #file too big\n",
    "                list_err_hash.append(full_path_file)\n",
    "                print(\"size\", file_size/1000, full_path_file)\n",
    "                continue\n",
    "        else:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"not exist\", full_path_file)\n",
    "            continue\n",
    "\n",
    "        # check if nb of files is within the max_files allowed\n",
    "        if max_file != 0 and count_files > max_file:\n",
    "            list_err_hash.append(full_path_file)\n",
    "            print(\"count\")\n",
    "            break\n",
    "\n",
    "        if count_files % (val_size*100) == 0:\n",
    "            list_val_hash.append(full_path_file)\n",
    "        else:\n",
    "            list_train_hash.append(full_path_file)\n",
    "\n",
    "        count_files += 1\n",
    "\n",
    "    print(f\"Number of files with error: {len(list_err_hash)}\")\n",
    "    return list_train_hash, list_val_hash, list_err_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "84w6HNzrOKjz",
    "outputId": "2c637405-d05c-425b-932f-1baa52a9b0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\16e40e92dd11003a960e6944be261ef59e988599437c9e0a025d94a315abfe55.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\16ff5ba3a1cd45b8886f10f61cefa4140c09be357240a1b8233c94f8bd6758a1.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\172363c4c7b1688c62a6b0ba798dc7c5a80b119f5b9ffff7bae009a28cd3e6c8.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1730968b5903215d77bd1f2e114f3b9e9a6cbff9bd8d25ada46d2af663422e81.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1745d056b69ad2dd90822c062c6bfe67f2d2dee1abd3bc5c6ace0298d677244d.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\175489d3698f84221b308bfed110ea40fb5d002819d888dc94fe932239fc5b34.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\176cd962682da7c22b2dda004637798dea9c36acc12584898cfb7b645f73a2bf.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\178f1c6b3dd17a9d158e6f723dab6fa52d498c71d7a56723c0ffa1aa28d7a8e4.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\17a1003108b5f21486e2a81ddbdf75f27690b20d329d5907593760c82b5f3034.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\17aa28d14a897a94977d15f7a9ac0f91fde6a40f632ac5caa6bf87ace6b978fb.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\17be08889e62f4af8e39b201a84c7477680d74b10fea55ff3712ab30154f2d53.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\17de596b52bba7c0179beb56565f24e5c68b1887ee71d69f189987ba1eb85ed8.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\17ff05a35e6b1135e7b118f8eda2e26ed3058200e0f8f1253568eacb925cf1ef.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\181705c4f5d203b4290fa99eca79a3ece3e4609294bbaae474fa51a699722841.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\18267ba674c43afdac396f36c463a5d452b970d1d1587ff7deb9519361032a51.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1829bb0254493418b335a4dd576a634c169b2be751d0c70a51d2f8443568449f.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\18442b2f3d0e70411d301fed0829f0c603c4025576e631770bc910bf7a7c6783.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1864ca6e8d5de0e2f01817742d7e2629f5dadda4034b1257d6708955158ab856.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1888168282c34491ed3f7dd9f2348878650b2bea678a12ecdd35941f9f9b3cfe.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\189b2c1eee77be80ec244ef2217ebd75b189a9586bd17f7a8c02838b00bbd3ca.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\18b01ea11dd7cf3bc48d8cd59e1e71f0e75a9f456bd44e3f3fc5cabaae890f15.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\18c25c8809ed07beeee5d09e6c8bf749025082dc7c462e88a96f9a634e582dc7.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\18eaa658aa74d95a46b37a55a09ca64fcec3fd924787c7e1e32bdde14de556e5.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\191272e200345dcb0a7a8c8c975a8b07847f07b9d9f0c3af472fdb88092aee0b.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19302a0b0cca42c6a0c392e47243ddbc24376469ee3378d427ca50f141dcd42e.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19611543a3cde1b5d87c4f075f7813fc6763313dc9bfb23c2b574acbd96806bf.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19716bdf3c9b2b5f5b2a4d110b7e4acdb90631b3a1aa15320f898cb4007001f0.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19896a23d7b054625c2f6b1ee1551a0da68ad25cddbb24510a3b74578418e618.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\199cef8bc248c45979737b5919b8ec99f7935ead10e44475b420031171da33d1.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19c6446e668a9aa8c0cb39696adb5aac9e7bb6a133bd95f0ab0b61ed02f7217c.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19dd6cb5f1b36f21598db51256d4952d7620eb0e1ee0c3f24bb51ef8a07a5dfc.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19ee35f7026828c86cfd82ee26daa6223524d8fd751ba3813d43fc386c327942.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\19fc5662e9b4d923f5217c5e4da0747c29a7b835575dafd87cd949717325fd28.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a1a8a27fb9324e5e3b8cd49aa83395ca6a0d50cb0d9d5197df679bfd8a0d755.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a433981ab515f1cbca157bc98c076bfa19fd4d283a8b3ddb9abd3d7d7808a23.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a54e577b6287a5cf1eec7b6395a234253fdb3a9b8f6a2e7eda0f2efaf833ae3.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a5cec72b84f8debb1abee801cfc4e1f7db0644e1314a88b39a16e7e13bd89fa.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a76f0ec5c04f4cb59b61f838b68798fa75c0bf34250e8b7c7c83e62d42567ab.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1a96c1f68eda8a0436b450a449aac8c9599e1094264cd097a0949ff1f9dd7d8c.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1aa7951de51b14a6a7d4584e6386d8009120594c3e226523c399b19cb98e9530.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1abc05dc0180c7e5e6061386f9a316ba72e4bd4416439b9be534a7655f283d10.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1adb32b1d4e1260b8ee1e315bfebe334f4609984c0c2c875c3b172bcac6a941e.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1af0b783a27334279197a3efdea928320427dd6a114adb99862140333b48a79e.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b0970d58934255caf367186d5360b5a907364b9e36bc8fe9af3785f84615cf1.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b2976e9e185cce587fc09c892335d033f19ec186dfb8c0d6fd61a8bae73f40d.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b36e1864016d6cd5fa2f7f7624c3c42ed679d16e8f34f3993646b949dfb7230.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b5b8d4000174658b280d11ce9143d60ac3db44b9885f35545c3b6c9aeb5ac13.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b757e06cc05a64603ec15d1c7fbd9390fa59a814705d495104e5504f5975800.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1b8de1eeff5c1d068d1ddd3ac2bc482509467aecb9a5c178584a33c7c0780554.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1ba5e2353aed48cfa185738c51dc9e0f45d6a7956f277b937d557b1a96ecf783.json\n",
      "not exist D:\\ChallengeDST\\folder_training_set\\folder_training_set\\1bbca2359b121af3ee988e25add9f3b1ea7df59102b68676c7e785d58df1697a.json\n",
      "Number of files with error: 51\n",
      "0 0 51\n"
     ]
    }
   ],
   "source": [
    "list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch='rep_2500')\n",
    "print(len(list_val), len(list_train), len(list_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1742720599926,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "RLwlVa-5tSu9"
   },
   "outputs": [],
   "source": [
    "class HierarchicalAssemblyTokenizer:\n",
    "    def __init__(self):\n",
    "        # Tokenizer implementation from previous code\n",
    "        # Abbreviated for brevity but would be the full implementation\n",
    "        self.node_type_vocab = {}\n",
    "        self.operation_vocab = {}\n",
    "        self.register_vocab = {}\n",
    "        self.memory_pattern_vocab = {}\n",
    "        self.immediate_vocab = {}\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        # Regex patterns for parsing\n",
    "        self.patterns = {\n",
    "            'node_type': re.compile(r'([A-Z]+)\\s*:'),\n",
    "            'operation': re.compile(r':\\s*([a-z]+)'),\n",
    "            'registers': re.compile(r'(?:^|\\s+)([a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l)(?:$|\\s+|,|\\])'),\n",
    "            'memory_ref': re.compile(r'\\[(.*?)\\]'),\n",
    "            'immediate': re.compile(r'0x[0-9a-fA-F]+|\\d+')\n",
    "        }\n",
    "\n",
    "    def fit(self, graph_l, min_freq=1):\n",
    "        \"\"\"Build vocabularies from the digraph nodes\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        self.unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        # Extract features from node labels\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                # Extract node type (JCC, INST)\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                # Extract operation (xor, push, mov)\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                # Extract registers\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                # Extract memory reference patterns\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    # Convert memory reference to a pattern (e.g., \"reg + offset\")\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                # Extract immediate values\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            self.unique_hashes.append(item['name'])\n",
    "            self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "            for i, hash_val in enumerate(self.unique_hashes):\n",
    "                self.hash_to_id[hash_val] = i\n",
    "                self.id_to_hash[i] = hash_val\n",
    "\n",
    "        # Build vocabularies with frequency filtering\n",
    "        self._build_vocab(self.node_type_vocab, node_types, min_freq)\n",
    "        self._build_vocab(self.operation_vocab, operations, min_freq)\n",
    "        self._build_vocab(self.register_vocab, registers, min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, memory_patterns, min_freq)\n",
    "        self._build_vocab(self.immediate_vocab, immediates, min_freq)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_from_counts(self, all_counts, min_freq=3):\n",
    "        \"\"\"Fits the tokenizer from accumulated counts.\"\"\"\n",
    "        self.node_type_vocab.clear()\n",
    "        self.operation_vocab.clear()\n",
    "        self.register_vocab.clear()\n",
    "        self.memory_pattern_vocab.clear()\n",
    "        self.immediate_vocab.clear()\n",
    "\n",
    "        self._build_vocab(self.node_type_vocab, all_counts['node_types'], min_freq)\n",
    "        self._build_vocab(self.operation_vocab, all_counts['operations'], min_freq*2)\n",
    "        self._build_vocab(self.register_vocab, all_counts['registers'], min_freq)\n",
    "        self._build_vocab(self.memory_pattern_vocab, all_counts['memory_patterns'], min_freq*3)\n",
    "        self._build_vocab(self.immediate_vocab, all_counts['immediates'], min_freq*3)\n",
    "\n",
    "        self.unique_hashes = all_counts['unique_hashes']\n",
    "        self.unique_hashes = sorted(list(self.unique_hashes))\n",
    "        for i, hash_val in enumerate(self.unique_hashes):\n",
    "            self.hash_to_id[hash_val] = i\n",
    "            self.id_to_hash[i] = hash_val\n",
    "\n",
    "    def _build_vocab(self, vocab_dict, tokens, min_freq):\n",
    "        \"\"\"Build vocabulary with frequency filtering\"\"\"\n",
    "        counter = Counter(tokens)\n",
    "        # Add special UNK token\n",
    "        vocab_dict[self.UNK_TOKEN] = 0\n",
    "\n",
    "        # Add tokens that meet minimum frequency\n",
    "        idx = 1\n",
    "        for token, count in counter.most_common():\n",
    "            if count >= min_freq:\n",
    "                vocab_dict[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def _memory_to_pattern(self, mem_ref):\n",
    "        \"\"\"Convert memory reference to pattern\"\"\"\n",
    "        # Replace registers with REG placeholder\n",
    "        pattern = re.sub(r'[a-z]{2,3}x|[a-z]{2,3}i|[a-z]{1,3}h|[a-z]{1,3}l', 'REG', mem_ref)\n",
    "        # Replace immediate values with IMM placeholder\n",
    "        pattern = re.sub(r'0x[0-9a-fA-F]+|\\d+', 'IMM', pattern)\n",
    "        return pattern.strip()\n",
    "\n",
    "    def tokenize(self, node_label):\n",
    "        \"\"\"Tokenize a node label into hierarchical features\"\"\"\n",
    "        features = {\n",
    "            'node_type': self.UNK_TOKEN,\n",
    "            'operation': self.UNK_TOKEN,\n",
    "            'registers': [],\n",
    "            'memory_pattern': self.UNK_TOKEN,\n",
    "            'immediate': self.UNK_TOKEN\n",
    "        }\n",
    "\n",
    "        # Extract node type\n",
    "        node_type_match = self.patterns['node_type'].search(node_label)\n",
    "        if node_type_match:\n",
    "            node_type = node_type_match.group(1)\n",
    "            features['node_type'] = node_type if node_type in self.node_type_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract operation\n",
    "        op_match = self.patterns['operation'].search(node_label)\n",
    "        if op_match:\n",
    "            operation = op_match.group(1)\n",
    "            features['operation'] = operation if operation in self.operation_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract registers\n",
    "        reg_matches = self.patterns['registers'].findall(node_label)\n",
    "        features['registers'] = [reg if reg in self.register_vocab else self.UNK_TOKEN for reg in reg_matches]\n",
    "\n",
    "        # Extract memory reference\n",
    "        mem_matches = self.patterns['memory_ref'].findall(node_label)\n",
    "        if mem_matches:\n",
    "            pattern = self._memory_to_pattern(mem_matches[0])\n",
    "            features['memory_pattern'] = pattern if pattern in self.memory_pattern_vocab else self.UNK_TOKEN\n",
    "\n",
    "        # Extract immediate values\n",
    "        imm_matches = self.patterns['immediate'].findall(node_label)\n",
    "        if imm_matches:\n",
    "            imm = imm_matches[0]\n",
    "            features['immediate'] = imm if imm in self.immediate_vocab else self.UNK_TOKEN\n",
    "\n",
    "        return features\n",
    "\n",
    "    def encode_nodelabel(self, node_label):\n",
    "        \"\"\"Convert a node label to numeric feature vectors\"\"\"\n",
    "        features = self.tokenize(node_label)\n",
    "\n",
    "        # Encode each feature\n",
    "        node_type_idx = self.node_type_vocab.get(features['node_type'], self.node_type_vocab[self.UNK_TOKEN])\n",
    "        operation_idx = self.operation_vocab.get(features['operation'], self.operation_vocab[self.UNK_TOKEN])\n",
    "\n",
    "        # Encode registers (take up to 3, pad if fewer)\n",
    "        register_indices = []\n",
    "        for i in range(min(3, len(features['registers']))):\n",
    "            reg = features['registers'][i]\n",
    "            reg_idx = self.register_vocab.get(reg, self.register_vocab[self.UNK_TOKEN])\n",
    "            register_indices.append(reg_idx)\n",
    "\n",
    "        # Pad register indices if needed\n",
    "        while len(register_indices) < 3:\n",
    "            register_indices.append(0)  # 0 for padding\n",
    "\n",
    "        memory_pattern_idx = self.memory_pattern_vocab.get(\n",
    "            features['memory_pattern'],\n",
    "            self.memory_pattern_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        immediate_idx = self.immediate_vocab.get(\n",
    "            features['immediate'],\n",
    "            self.immediate_vocab[self.UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Combine all indices into a feature vector\n",
    "        encoded = np.array([\n",
    "            node_type_idx,\n",
    "            operation_idx,\n",
    "            register_indices[0],\n",
    "            register_indices[1],\n",
    "            register_indices[2],\n",
    "            memory_pattern_idx,\n",
    "            immediate_idx\n",
    "        ], dtype=np.int64)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_graph(self, digraph):\n",
    "        \"\"\"Convert an entire digraph to node feature vectors\"\"\"\n",
    "        node_features = {}\n",
    "\n",
    "        for node_id in digraph.nodes():\n",
    "            label = digraph.nodes[node_id].get('label', '')\n",
    "            node_features[node_id] = self.encode_nodelabel(label)\n",
    "\n",
    "        return node_features\n",
    "\n",
    "    def encode_hash(self, hash_file):\n",
    "        return self.hash_to_id.get(hash_file, -1) # Return -1 if hash is not found\n",
    "\n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Return the size of each vocabulary\"\"\"\n",
    "        return {\n",
    "            'node_type': len(self.node_type_vocab),\n",
    "            'operation': len(self.operation_vocab),\n",
    "            'register': len(self.register_vocab),\n",
    "            'memory_pattern': len(self.memory_pattern_vocab),\n",
    "            'immediate': len(self.immediate_vocab)\n",
    "        }\n",
    "\n",
    "    def get_tokens_and_counts(self, graph_l):\n",
    "        \"\"\"Extracts tokens and counts from graph_l.\"\"\"\n",
    "        node_types = []\n",
    "        operations = []\n",
    "        registers = []\n",
    "        memory_patterns = []\n",
    "        immediates = []\n",
    "        unique_hashes = []\n",
    "        self.hash_to_id = {}\n",
    "        self.id_to_hash = {}\n",
    "\n",
    "        for item in graph_l:\n",
    "            graph = item['graph_input']\n",
    "            for node_id, node_attr in graph.nodes(data=True):\n",
    "                label = node_attr.get('label', '')\n",
    "\n",
    "                node_type_match = self.patterns['node_type'].search(label)\n",
    "                if node_type_match:\n",
    "                    node_types.append(node_type_match.group(1))\n",
    "\n",
    "                op_match = self.patterns['operation'].search(label)\n",
    "                if op_match:\n",
    "                    operations.append(op_match.group(1))\n",
    "\n",
    "                reg_matches = self.patterns['registers'].findall(label)\n",
    "                registers.extend(reg_matches)\n",
    "\n",
    "                mem_matches = self.patterns['memory_ref'].findall(label)\n",
    "                for mem in mem_matches:\n",
    "                    pattern = self._memory_to_pattern(mem)\n",
    "                    memory_patterns.append(pattern)\n",
    "\n",
    "                imm_matches = self.patterns['immediate'].findall(label)\n",
    "                immediates.extend(imm_matches)\n",
    "\n",
    "            unique_hashes.append(item['name'])\n",
    "\n",
    "        return {\n",
    "            'node_types': node_types,\n",
    "            'operations': operations,\n",
    "            'registers': registers,\n",
    "            'memory_patterns': memory_patterns,\n",
    "            'immediates': immediates,\n",
    "            'unique_hashes': unique_hashes,\n",
    "        }\n",
    "\n",
    "class AssemblyGraphDataset:\n",
    "    def __init__(self, graph_list, tokenizer, node_feature_dim=60):\n",
    "        \"\"\"\n",
    "        Prepares a dataset of assembly graphs for GAT model training\n",
    "        Args:\n",
    "            graph_list: List of NetworkX digraphs representing assembly code\n",
    "            tokenizer: HierarchicalAssemblyTokenizer instance\n",
    "            node_feature_dim: Dimension of node embeddings\n",
    "        \"\"\"\n",
    "        self.graph_list = graph_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        # Initialize embedding layers for each feature type\n",
    "        num_feature_type = 5\n",
    "        vocab_sizes = tokenizer.get_vocab_sizes()\n",
    "        self.node_type_embedding = nn.Embedding(vocab_sizes['node_type'], node_feature_dim // num_feature_type)\n",
    "        self.operation_embedding = nn.Embedding(vocab_sizes['operation'], node_feature_dim // num_feature_type)\n",
    "        self.register_embedding = nn.Embedding(vocab_sizes['register'], node_feature_dim // num_feature_type)\n",
    "        self.memory_pattern_embedding = nn.Embedding(vocab_sizes['memory_pattern'], node_feature_dim // num_feature_type)\n",
    "        self.immediate_embedding = nn.Embedding(vocab_sizes['immediate'], node_feature_dim // num_feature_type)\n",
    "\n",
    "        # Process graphs into PyTorch Geometric Data objects\n",
    "        self.data_list = []\n",
    "        for graph in self.graph_list:\n",
    "            # process graph info\n",
    "            self.data_list.append(self._process_graph(graph['graph_input'],graph['name']))\n",
    "\n",
    "    def _process_graph(self, graph, hash_name):\n",
    "        \"\"\"Convert a NetworkX graph to a PyTorch Geometric Data object\"\"\"\n",
    "        # Create a node ID mapping for consecutive IDs\n",
    "        node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "        # Get node features\n",
    "        node_features_dict = self.tokenizer.encode_graph(graph)\n",
    "\n",
    "        # Convert to tensor-friendly format\n",
    "        x = torch.zeros((len(graph.nodes()), 7), dtype=torch.long)\n",
    "        for node_id, features in node_features_dict.items():\n",
    "            x[node_mapping[node_id]] = torch.tensor(features)\n",
    "\n",
    "        # Create edge index\n",
    "        edge_list = list(graph.edges())\n",
    "        if not edge_list:\n",
    "            # Handle case with no edges\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = self.optimize_edge_index(edge_list, node_mapping)\n",
    "\n",
    "        # Endcode Hash name\n",
    "        encoded_hash = self.tokenizer.encode_hash(hash_name)\n",
    "        encoded_hash_tensor = torch.tensor(encoded_hash, dtype=torch.long)\n",
    "\n",
    "        # Get embeddings before creating Data object\n",
    "        x = self.get_embeddings(x)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x, edge_index=edge_index, hash_encoded=encoded_hash_tensor)\n",
    "        return data\n",
    "\n",
    "    def optimize_edge_index(self, edge_list, node_mapping):\n",
    "        \"\"\"\n",
    "        Optimizes the creation of edge_index tensor for graph representation.\n",
    "        Args:\n",
    "            edge_list (list of tuples): List of edge tuples (src, tgt).\n",
    "            node_mapping (dict): Dictionary mapping node IDs to integer indices.\n",
    "        Returns:\n",
    "            torch.Tensor: Optimized edge_index tensor.\n",
    "        \"\"\"\n",
    "        if not edge_list:\n",
    "            return torch.zeros((2, 0), dtype=torch.long)\n",
    "\n",
    "        num_edges = len(edge_list)\n",
    "        src_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "        tgt_indices = torch.empty(num_edges, dtype=torch.long)\n",
    "\n",
    "        for i, (src, tgt) in enumerate(edge_list):\n",
    "            src_indices[i] = node_mapping[src]\n",
    "            tgt_indices[i] = node_mapping[tgt]\n",
    "\n",
    "        return torch.stack((src_indices, tgt_indices), dim=0).contiguous()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Transform tokenized features into embeddings\"\"\"\n",
    "        # Split features into their components\n",
    "        node_type_idx = x[:, 0]\n",
    "        operation_idx = x[:, 1]\n",
    "        register_idx1 = x[:, 2]\n",
    "        register_idx2 = x[:, 3]\n",
    "        register_idx3 = x[:, 4]\n",
    "        memory_pattern_idx = x[:, 5]\n",
    "        immediate_idx = x[:, 6]\n",
    "\n",
    "        # Get embeddings for each component\n",
    "        node_type_emb = self.node_type_embedding(node_type_idx)\n",
    "        operation_emb = self.operation_embedding(operation_idx)\n",
    "\n",
    "        # Combine register embeddings (average them)\n",
    "        register_emb = (self.register_embedding(register_idx1) +\n",
    "                        self.register_embedding(register_idx2) +\n",
    "                        self.register_embedding(register_idx3)) / 3\n",
    "\n",
    "        memory_pattern_emb = self.memory_pattern_embedding(memory_pattern_idx)\n",
    "        immediate_emb = self.immediate_embedding(immediate_idx)\n",
    "\n",
    "        # Concatenate all embeddings\n",
    "        return torch.cat([\n",
    "            node_type_emb,\n",
    "            operation_emb,\n",
    "            register_emb,\n",
    "            memory_pattern_emb,\n",
    "            immediate_emb\n",
    "        ], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Graph Attention Network for assembly code analysis\n",
    "Args:\n",
    "    dataset: AssemblyGraphDataset instance\n",
    "    hidden_dim: Hidden dimension of GAT layers\n",
    "    output_dim: Output dimension of node embeddings\n",
    "    heads: Number of attention heads\n",
    "    dropout: Dropout rate\n",
    "\"\"\"\n",
    "class AssemblyGAT(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim=64, output_dim=453, heads=8, dropout=0.6, hash_dim=512):\n",
    "        super(AssemblyGAT, self).__init__()\n",
    "        self.conv1 = GATConv(node_feature_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        self.hash_linear = nn.Linear(hash_dim, node_feature_dim) # Linear layer for hash encoding, adjusted dim\n",
    "        self.hash_dim = hash_dim\n",
    "\n",
    "    def forward(self, x, edge_index, hash_encoded, batch):\n",
    "        \"\"\"Forward pass through the GAT model\"\"\"\n",
    "        # First GAT layer with activation and dropout\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# Function to parse digraph string (as in previous example)\n",
    "def parse_digraph_string(digraph_str):\n",
    "    \"\"\"Parse the digraph string to create a NetworkX DiGraph\"\"\"\n",
    "    # Simple parser for the provided format\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Regular expressions for node and edge definitions\n",
    "    node_pattern = re.compile(r'\"([^\"]+)\"\\s*\\[label\\s*=\\s*\"([^\"]+)\"\\]')\n",
    "    edge_pattern = re.compile(r'\"([^\"]+)\"\\s*->\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # Extract nodes and edges\n",
    "    for line in digraph_str.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip the Digraph G { and } lines\n",
    "        if line == 'Digraph G {' or line == '}':\n",
    "            continue\n",
    "\n",
    "        # Parse node definitions\n",
    "        node_match = node_pattern.search(line)\n",
    "        if node_match:\n",
    "            node_id = node_match.group(1)\n",
    "            label = node_match.group(2)\n",
    "            G.add_node(node_id, label=label)\n",
    "            continue\n",
    "\n",
    "        # Parse edge definitions\n",
    "        edge_match = edge_pattern.search(line)\n",
    "        if edge_match:\n",
    "            source = edge_match.group(1)\n",
    "            target = edge_match.group(2)\n",
    "            G.add_edge(source, target)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742721142838,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "INiZZqldtSu-"
   },
   "outputs": [],
   "source": [
    "# Create empty list to store dataframes\n",
    "# init parameters to loop the file loading by batch\n",
    "files_meta_list = set(df_meta_train['name'].astype(str))\n",
    "random_seed = 42\n",
    "\n",
    "def split_dataloader(dataset, list_of_hash, batch_size, validation_split=0.5, shuffle=True):\n",
    "    random.seed(random_seed) #set random seed for random module.\n",
    "    torch.manual_seed(random_seed) #set random seed for torch.\n",
    "\n",
    "    validation_size = int(len(dataset) * validation_split)\n",
    "    train_size = len(dataset) - validation_size\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    validation_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    validation_dataset = Subset(dataset, validation_indices)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # Shuffle is handled by the initial indices shuffling.\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_hash = [list_of_hash[i] for i in train_indices]\n",
    "    val_hash = [list_of_hash[i] for i in validation_indices]\n",
    "\n",
    "    return train_dataloader, validation_dataloader, train_hash, val_hash\n",
    "\n",
    "def process_batch(file_batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of files.\n",
    "    Args:\n",
    "        file_batch (list): A list of file paths.\n",
    "    \"\"\"\n",
    "    graph_list_curr = []\n",
    "    file_with_err = 0\n",
    "\n",
    "    for full_path_file in file_batch:\n",
    "        try:\n",
    "            with open(full_path_file, 'r') as f:\n",
    "                hash_file = full_path_file.split('.jso')[0]\n",
    "                hash_file = hash_file.split(split_char)[-1]\n",
    "\n",
    "                #f = open(full_path_file, 'r')\n",
    "                digraph_str = f.read()\n",
    "\n",
    "                # test if file content has no error tag\n",
    "                if 'ERROR' in digraph_str:\n",
    "                    file_with_err += 1\n",
    "                    continue\n",
    "\n",
    "                G = parse_digraph_string(digraph_str)\n",
    "                graph_list_curr.append({\n",
    "                    'name': hash_file,\n",
    "                    'graph_input' : G\n",
    "                })\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found: {full_path_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {full_path_file}: {e}\")\n",
    "\n",
    "    return graph_list_curr\n",
    "\n",
    "def get_metadata_from_hash(hash_list):\n",
    "    hash_meta_values = []\n",
    "    for hash_name in hash_list:\n",
    "        df_y_values = df_meta_train[df_meta_train['name'] == hash_name].copy()\n",
    "        df_y_values.drop(columns=['name'], inplace=True)\n",
    "        if len(df_y_values) > 0 :\n",
    "             hash_meta_values.append(df_y_values.values[0]) # Get the first row's values\n",
    "        else:\n",
    "            print(f\"{hash_name} not found in metadata\")\n",
    "    return hash_meta_values\n",
    "\n",
    "def train_model(train_dataloader, train_hash, batch_size, epochs):\n",
    "    if os.path.exists(model_save_file):\n",
    "      if torch.cuda.is_available():\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "      else:\n",
    "          model.load_state_dict(torch.load(model_save_file, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        count_batch = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # get the different components of the dataset\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "            #print(f\"batch: {count_batch}  == x: {x.shape} edge_index: {edge_index.shape} \")\n",
    "            list_hash_batch = train_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(output, y_label.float())\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        #print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # save model trained\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "def validate_model_perf(validation_dataloader, val_hash, batch_size):\n",
    "    if os.path.exists(model_save_file):\n",
    "        model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "\n",
    "    model.eval() #set the model to evaluation mode.\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "    hash_v = []\n",
    "\n",
    "    count_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            list_hash_batch = val_hash[count_batch*batch_size:(count_batch+1)*batch_size]\n",
    "            y_label = get_metadata_from_hash(list_hash_batch)\n",
    "            y_label = np.array(y_label)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float32).to(device)\n",
    "\n",
    "            bincount_list = torch.bincount(batch.batch).tolist()\n",
    "            if any(bincount_list): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(bincount_list)])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            count_batch += 1\n",
    "\n",
    "            if output.shape != y_label.shape:\n",
    "                print(f\"Warning: output shape {output.shape} and y_label shape {y_label.shape} do not match.\")\n",
    "                break\n",
    "\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "            all_true_label.append(y_label.cpu())\n",
    "            all_pred_label.append(prediction_labels.cpu())\n",
    "            #hash_v.append(list_hash_batch)\n",
    "\n",
    "    true_labels_tensor = torch.cat(all_true_label, dim=0) # Concatenate along dimension 0\n",
    "    pred_labels_tensor = torch.cat(all_pred_label, dim=0) # Concatenate along dimension 0\n",
    "\n",
    "    return pred_labels_tensor, true_labels_tensor\n",
    "\n",
    "def run_files_in_batches(full_file_paths, batch_files_size=50, mode='train'):\n",
    "\n",
    "    print(f\"There are {len(full_file_paths)} files for {mode}\")\n",
    "    num_files = len(full_file_paths)\n",
    "    nb_batch = (num_files // batch_files_size) +1\n",
    "\n",
    "    hash_v = []\n",
    "    hash_t = []\n",
    "    all_true_label = []\n",
    "    all_pred_label = []\n",
    "\n",
    "    all_counts = {\n",
    "        'node_types': [],\n",
    "        'operations': [],\n",
    "        'registers': [],\n",
    "        'memory_patterns': [],\n",
    "        'immediates': [],\n",
    "        'unique_hashes': [],\n",
    "    }\n",
    "    print(f\"**** Tokenization Processing\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "\n",
    "        #print(f\"Tokenization Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if len(batch_graph_list) == 0:\n",
    "            print(\"graph list = 0\")\n",
    "            break\n",
    "\n",
    "        tokens = tokenizer.get_tokens_and_counts(batch_graph_list) # you will need to implement this\n",
    "        all_counts.update(tokens)\n",
    "\n",
    "    # Create a tokenizer from token counts\n",
    "    tokenizer.fit_from_counts(all_counts)\n",
    "\n",
    "    print(f\"**** Dataset Processing & Training\")\n",
    "    for i in range(0, num_files, batch_files_size):\n",
    "        batch_files = full_file_paths[i:i + batch_files_size]\n",
    "        print(f\"Dataset Processing batch {i // batch_files_size + 1}/{nb_batch}: {len(batch_files)} files\")\n",
    "\n",
    "        batch_graph_list = process_batch(batch_files)\n",
    "        if len(batch_graph_list) == 0:\n",
    "            print(\"graph list = 0\")\n",
    "            break\n",
    "\n",
    "        dataset = AssemblyGraphDataset(batch_graph_list, tokenizer, node_feature_dim=num_feature_dim)\n",
    "        curr_dataloader = DataLoader(dataset, batch_size=batch_data_size, shuffle=False)\n",
    "\n",
    "        list_of_hash = [graph['name'] for graph in batch_graph_list]\n",
    "\n",
    "        if mode == 'train':\n",
    "            train_model(curr_dataloader, list_of_hash, batch_data_size, epochs=3)\n",
    "            hash_t.append(list_of_hash)\n",
    "        else:\n",
    "            pred_labels, true_labels = validate_model_perf(curr_dataloader, list_of_hash, batch_data_size)\n",
    "            all_true_label.append(true_labels.cpu())\n",
    "            all_pred_label.append(pred_labels.cpu())\n",
    "            hash_v.append(list_of_hash)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # save all labels as tensor to return if mode= 'validation'\n",
    "    if mode == 'validation':\n",
    "        all_true_label_tensor = torch.cat(all_true_label).cpu().numpy()\n",
    "        all_pred_label_tensor = torch.cat(all_pred_label).cpu().numpy()\n",
    "\n",
    "        return all_pred_label_tensor, all_true_label_tensor, hash_v\n",
    "    elif mode == 'train':\n",
    "        return hash_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 56657317,
     "status": "error",
     "timestamp": 1742777808389,
     "user": {
      "displayName": "jean-charles Monceau",
      "userId": "11688387581777078781"
     },
     "user_tz": -60
    },
    "id": "avU1v3vVtSu-",
    "outputId": "6adaf197-d907-4ed1-d0b2-fa72b7a9b5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch: rep_0 ========== \n",
      "Number of files with error: 0\n",
      "There are 45 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/23: 2 files\n",
      "Dataset Processing batch 2/23: 2 files\n",
      "Dataset Processing batch 3/23: 2 files\n",
      "Dataset Processing batch 4/23: 2 files\n",
      "Dataset Processing batch 5/23: 2 files\n",
      "Dataset Processing batch 6/23: 2 files\n",
      "Dataset Processing batch 7/23: 2 files\n",
      "Dataset Processing batch 8/23: 2 files\n",
      "Dataset Processing batch 9/23: 2 files\n",
      "Dataset Processing batch 10/23: 2 files\n",
      "Dataset Processing batch 11/23: 2 files\n",
      "Dataset Processing batch 12/23: 2 files\n",
      "Dataset Processing batch 13/23: 2 files\n",
      "Dataset Processing batch 14/23: 2 files\n",
      "Dataset Processing batch 15/23: 2 files\n",
      "Dataset Processing batch 16/23: 2 files\n",
      "Dataset Processing batch 17/23: 2 files\n",
      "Dataset Processing batch 18/23: 2 files\n",
      "Dataset Processing batch 19/23: 2 files\n",
      "Dataset Processing batch 20/23: 2 files\n",
      "Dataset Processing batch 21/23: 2 files\n",
      "Dataset Processing batch 22/23: 2 files\n",
      "Dataset Processing batch 23/23: 1 files\n",
      "There are 5 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/3: 2 files\n",
      "Dataset Processing batch 2/3: 2 files\n",
      "Dataset Processing batch 3/3: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 0.98 \n",
      "\n",
      "========== batch: rep_500 ========== \n",
      "Number of files with error: 0\n",
      "There are 45 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/23: 2 files\n",
      "Dataset Processing batch 2/23: 2 files\n",
      "Dataset Processing batch 3/23: 2 files\n",
      "Dataset Processing batch 4/23: 2 files\n",
      "Dataset Processing batch 5/23: 2 files\n",
      "Dataset Processing batch 6/23: 2 files\n",
      "Dataset Processing batch 7/23: 2 files\n",
      "Dataset Processing batch 8/23: 2 files\n",
      "Dataset Processing batch 9/23: 2 files\n",
      "Dataset Processing batch 10/23: 2 files\n",
      "Dataset Processing batch 11/23: 2 files\n",
      "Dataset Processing batch 12/23: 2 files\n",
      "Dataset Processing batch 13/23: 2 files\n",
      "Dataset Processing batch 14/23: 2 files\n",
      "Dataset Processing batch 15/23: 2 files\n",
      "Dataset Processing batch 16/23: 2 files\n",
      "Dataset Processing batch 17/23: 2 files\n",
      "Dataset Processing batch 18/23: 2 files\n",
      "Dataset Processing batch 19/23: 2 files\n",
      "Dataset Processing batch 20/23: 2 files\n",
      "Dataset Processing batch 21/23: 2 files\n",
      "Dataset Processing batch 22/23: 2 files\n",
      "Dataset Processing batch 23/23: 1 files\n",
      "There are 5 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/3: 2 files\n",
      "Dataset Processing batch 2/3: 2 files\n",
      "Dataset Processing batch 3/3: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 1.45 \n",
      "\n",
      "========== batch: rep_1000 ========== \n",
      "Number of files with error: 0\n",
      "There are 45 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/23: 2 files\n",
      "Dataset Processing batch 2/23: 2 files\n",
      "Dataset Processing batch 3/23: 2 files\n",
      "Dataset Processing batch 4/23: 2 files\n",
      "Dataset Processing batch 5/23: 2 files\n",
      "Dataset Processing batch 6/23: 2 files\n",
      "Dataset Processing batch 7/23: 2 files\n",
      "Dataset Processing batch 8/23: 2 files\n",
      "Dataset Processing batch 9/23: 2 files\n",
      "Dataset Processing batch 10/23: 2 files\n",
      "Dataset Processing batch 11/23: 2 files\n",
      "Dataset Processing batch 12/23: 2 files\n",
      "Dataset Processing batch 13/23: 2 files\n",
      "Dataset Processing batch 14/23: 2 files\n",
      "Dataset Processing batch 15/23: 2 files\n",
      "Dataset Processing batch 16/23: 2 files\n",
      "Dataset Processing batch 17/23: 2 files\n",
      "Dataset Processing batch 18/23: 2 files\n",
      "Dataset Processing batch 19/23: 2 files\n",
      "Dataset Processing batch 20/23: 2 files\n",
      "Dataset Processing batch 21/23: 2 files\n",
      "Dataset Processing batch 22/23: 2 files\n",
      "Dataset Processing batch 23/23: 1 files\n",
      "There are 5 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/3: 2 files\n",
      "Dataset Processing batch 2/3: 2 files\n",
      "Dataset Processing batch 3/3: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 1.85 \n",
      "\n",
      "========== batch: rep_1500 ========== \n",
      "Number of files with error: 0\n",
      "There are 441 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/221: 2 files\n",
      "Dataset Processing batch 2/221: 2 files\n",
      "Dataset Processing batch 3/221: 2 files\n",
      "Dataset Processing batch 4/221: 2 files\n",
      "Dataset Processing batch 5/221: 2 files\n",
      "Dataset Processing batch 6/221: 2 files\n",
      "Dataset Processing batch 7/221: 2 files\n",
      "Dataset Processing batch 8/221: 2 files\n",
      "Dataset Processing batch 9/221: 2 files\n",
      "Dataset Processing batch 10/221: 2 files\n",
      "Dataset Processing batch 11/221: 2 files\n",
      "Dataset Processing batch 12/221: 2 files\n",
      "Dataset Processing batch 13/221: 2 files\n",
      "Dataset Processing batch 14/221: 2 files\n",
      "Dataset Processing batch 15/221: 2 files\n",
      "Dataset Processing batch 16/221: 2 files\n",
      "Dataset Processing batch 17/221: 2 files\n",
      "Dataset Processing batch 18/221: 2 files\n",
      "Dataset Processing batch 19/221: 2 files\n",
      "Dataset Processing batch 20/221: 2 files\n",
      "Dataset Processing batch 21/221: 2 files\n",
      "Dataset Processing batch 22/221: 2 files\n",
      "Dataset Processing batch 23/221: 2 files\n",
      "Dataset Processing batch 24/221: 2 files\n",
      "Dataset Processing batch 25/221: 2 files\n",
      "Dataset Processing batch 26/221: 2 files\n",
      "Dataset Processing batch 27/221: 2 files\n",
      "Dataset Processing batch 28/221: 2 files\n",
      "Dataset Processing batch 29/221: 2 files\n",
      "Dataset Processing batch 30/221: 2 files\n",
      "Dataset Processing batch 31/221: 2 files\n",
      "Dataset Processing batch 32/221: 2 files\n",
      "Dataset Processing batch 33/221: 2 files\n",
      "Dataset Processing batch 34/221: 2 files\n",
      "Dataset Processing batch 35/221: 2 files\n",
      "Dataset Processing batch 36/221: 2 files\n",
      "Dataset Processing batch 37/221: 2 files\n",
      "Dataset Processing batch 38/221: 2 files\n",
      "Dataset Processing batch 39/221: 2 files\n",
      "Dataset Processing batch 40/221: 2 files\n",
      "Dataset Processing batch 41/221: 2 files\n",
      "Dataset Processing batch 42/221: 2 files\n",
      "Dataset Processing batch 43/221: 2 files\n",
      "Dataset Processing batch 44/221: 2 files\n",
      "Dataset Processing batch 45/221: 2 files\n",
      "Dataset Processing batch 46/221: 2 files\n",
      "Dataset Processing batch 47/221: 2 files\n",
      "Dataset Processing batch 48/221: 2 files\n",
      "Dataset Processing batch 49/221: 2 files\n",
      "Dataset Processing batch 50/221: 2 files\n",
      "Dataset Processing batch 51/221: 2 files\n",
      "Dataset Processing batch 52/221: 2 files\n",
      "Dataset Processing batch 53/221: 2 files\n",
      "Dataset Processing batch 54/221: 2 files\n",
      "Dataset Processing batch 55/221: 2 files\n",
      "Dataset Processing batch 56/221: 2 files\n",
      "Dataset Processing batch 57/221: 2 files\n",
      "Dataset Processing batch 58/221: 2 files\n",
      "Dataset Processing batch 59/221: 2 files\n",
      "Dataset Processing batch 60/221: 2 files\n",
      "Dataset Processing batch 61/221: 2 files\n",
      "Dataset Processing batch 62/221: 2 files\n",
      "Dataset Processing batch 63/221: 2 files\n",
      "Dataset Processing batch 64/221: 2 files\n",
      "Dataset Processing batch 65/221: 2 files\n",
      "Dataset Processing batch 66/221: 2 files\n",
      "Dataset Processing batch 67/221: 2 files\n",
      "Dataset Processing batch 68/221: 2 files\n",
      "Dataset Processing batch 69/221: 2 files\n",
      "Dataset Processing batch 70/221: 2 files\n",
      "Dataset Processing batch 71/221: 2 files\n",
      "Dataset Processing batch 72/221: 2 files\n",
      "Dataset Processing batch 73/221: 2 files\n",
      "Dataset Processing batch 74/221: 2 files\n",
      "Dataset Processing batch 75/221: 2 files\n",
      "Dataset Processing batch 76/221: 2 files\n",
      "Dataset Processing batch 77/221: 2 files\n",
      "Dataset Processing batch 78/221: 2 files\n",
      "Dataset Processing batch 79/221: 2 files\n",
      "Dataset Processing batch 80/221: 2 files\n",
      "Dataset Processing batch 81/221: 2 files\n",
      "Dataset Processing batch 82/221: 2 files\n",
      "Dataset Processing batch 83/221: 2 files\n",
      "Dataset Processing batch 84/221: 2 files\n",
      "Dataset Processing batch 85/221: 2 files\n",
      "Dataset Processing batch 86/221: 2 files\n",
      "Dataset Processing batch 87/221: 2 files\n",
      "Dataset Processing batch 88/221: 2 files\n",
      "Dataset Processing batch 89/221: 2 files\n",
      "Dataset Processing batch 90/221: 2 files\n",
      "Dataset Processing batch 91/221: 2 files\n",
      "Dataset Processing batch 92/221: 2 files\n",
      "Dataset Processing batch 93/221: 2 files\n",
      "Dataset Processing batch 94/221: 2 files\n",
      "Dataset Processing batch 95/221: 2 files\n",
      "Dataset Processing batch 96/221: 2 files\n",
      "Dataset Processing batch 97/221: 2 files\n",
      "Dataset Processing batch 98/221: 2 files\n",
      "Dataset Processing batch 99/221: 2 files\n",
      "Dataset Processing batch 100/221: 2 files\n",
      "Dataset Processing batch 101/221: 2 files\n",
      "Dataset Processing batch 102/221: 2 files\n",
      "Dataset Processing batch 103/221: 2 files\n",
      "Dataset Processing batch 104/221: 2 files\n",
      "Dataset Processing batch 105/221: 2 files\n",
      "Dataset Processing batch 106/221: 2 files\n",
      "Dataset Processing batch 107/221: 2 files\n",
      "Dataset Processing batch 108/221: 2 files\n",
      "Dataset Processing batch 109/221: 2 files\n",
      "Dataset Processing batch 110/221: 2 files\n",
      "Dataset Processing batch 111/221: 2 files\n",
      "Dataset Processing batch 112/221: 2 files\n",
      "Dataset Processing batch 113/221: 2 files\n",
      "Dataset Processing batch 114/221: 2 files\n",
      "Dataset Processing batch 115/221: 2 files\n",
      "Dataset Processing batch 116/221: 2 files\n",
      "Dataset Processing batch 117/221: 2 files\n",
      "Dataset Processing batch 118/221: 2 files\n",
      "Dataset Processing batch 119/221: 2 files\n",
      "Dataset Processing batch 120/221: 2 files\n",
      "Dataset Processing batch 121/221: 2 files\n",
      "Dataset Processing batch 122/221: 2 files\n",
      "Dataset Processing batch 123/221: 2 files\n",
      "Dataset Processing batch 124/221: 2 files\n",
      "Dataset Processing batch 125/221: 2 files\n",
      "Dataset Processing batch 126/221: 2 files\n",
      "Dataset Processing batch 127/221: 2 files\n",
      "Dataset Processing batch 128/221: 2 files\n",
      "Dataset Processing batch 129/221: 2 files\n",
      "Dataset Processing batch 130/221: 2 files\n",
      "Dataset Processing batch 131/221: 2 files\n",
      "Dataset Processing batch 132/221: 2 files\n",
      "Dataset Processing batch 133/221: 2 files\n",
      "Dataset Processing batch 134/221: 2 files\n",
      "Dataset Processing batch 135/221: 2 files\n",
      "Dataset Processing batch 136/221: 2 files\n",
      "Dataset Processing batch 137/221: 2 files\n",
      "Dataset Processing batch 138/221: 2 files\n",
      "Dataset Processing batch 139/221: 2 files\n",
      "Dataset Processing batch 140/221: 2 files\n",
      "Dataset Processing batch 141/221: 2 files\n",
      "Dataset Processing batch 142/221: 2 files\n",
      "Dataset Processing batch 143/221: 2 files\n",
      "Dataset Processing batch 144/221: 2 files\n",
      "Dataset Processing batch 145/221: 2 files\n",
      "Dataset Processing batch 146/221: 2 files\n",
      "Dataset Processing batch 147/221: 2 files\n",
      "Dataset Processing batch 148/221: 2 files\n",
      "Dataset Processing batch 149/221: 2 files\n",
      "Dataset Processing batch 150/221: 2 files\n",
      "Dataset Processing batch 151/221: 2 files\n",
      "Dataset Processing batch 152/221: 2 files\n",
      "Dataset Processing batch 153/221: 2 files\n",
      "Dataset Processing batch 154/221: 2 files\n",
      "Dataset Processing batch 155/221: 2 files\n",
      "Dataset Processing batch 156/221: 2 files\n",
      "Dataset Processing batch 157/221: 2 files\n",
      "Dataset Processing batch 158/221: 2 files\n",
      "Dataset Processing batch 159/221: 2 files\n",
      "Dataset Processing batch 160/221: 2 files\n",
      "Dataset Processing batch 161/221: 2 files\n",
      "Dataset Processing batch 162/221: 2 files\n",
      "Dataset Processing batch 163/221: 2 files\n",
      "Dataset Processing batch 164/221: 2 files\n",
      "Dataset Processing batch 165/221: 2 files\n",
      "Dataset Processing batch 166/221: 2 files\n",
      "Dataset Processing batch 167/221: 2 files\n",
      "Dataset Processing batch 168/221: 2 files\n",
      "Dataset Processing batch 169/221: 2 files\n",
      "Dataset Processing batch 170/221: 2 files\n",
      "Dataset Processing batch 171/221: 2 files\n",
      "Dataset Processing batch 172/221: 2 files\n",
      "Dataset Processing batch 173/221: 2 files\n",
      "Dataset Processing batch 174/221: 2 files\n",
      "Dataset Processing batch 175/221: 2 files\n",
      "Dataset Processing batch 176/221: 2 files\n",
      "Dataset Processing batch 177/221: 2 files\n",
      "Dataset Processing batch 178/221: 2 files\n",
      "Dataset Processing batch 179/221: 2 files\n",
      "Dataset Processing batch 180/221: 2 files\n",
      "Dataset Processing batch 181/221: 2 files\n",
      "Dataset Processing batch 182/221: 2 files\n",
      "Dataset Processing batch 183/221: 2 files\n",
      "Dataset Processing batch 184/221: 2 files\n",
      "Dataset Processing batch 185/221: 2 files\n",
      "Dataset Processing batch 186/221: 2 files\n",
      "Dataset Processing batch 187/221: 2 files\n",
      "Dataset Processing batch 188/221: 2 files\n",
      "Dataset Processing batch 189/221: 2 files\n",
      "Dataset Processing batch 190/221: 2 files\n",
      "Dataset Processing batch 191/221: 2 files\n",
      "Dataset Processing batch 192/221: 2 files\n",
      "Dataset Processing batch 193/221: 2 files\n",
      "Dataset Processing batch 194/221: 2 files\n",
      "Dataset Processing batch 195/221: 2 files\n",
      "Dataset Processing batch 196/221: 2 files\n",
      "Dataset Processing batch 197/221: 2 files\n",
      "Dataset Processing batch 198/221: 2 files\n",
      "Dataset Processing batch 199/221: 2 files\n",
      "Dataset Processing batch 200/221: 2 files\n",
      "Dataset Processing batch 201/221: 2 files\n",
      "Dataset Processing batch 202/221: 2 files\n",
      "Dataset Processing batch 203/221: 2 files\n",
      "Dataset Processing batch 204/221: 2 files\n",
      "Dataset Processing batch 205/221: 2 files\n",
      "Dataset Processing batch 206/221: 2 files\n",
      "Dataset Processing batch 207/221: 2 files\n",
      "Dataset Processing batch 208/221: 2 files\n",
      "Dataset Processing batch 209/221: 2 files\n",
      "Dataset Processing batch 210/221: 2 files\n",
      "Dataset Processing batch 211/221: 2 files\n",
      "Dataset Processing batch 212/221: 2 files\n",
      "Dataset Processing batch 213/221: 2 files\n",
      "Dataset Processing batch 214/221: 2 files\n",
      "Dataset Processing batch 215/221: 2 files\n",
      "Dataset Processing batch 216/221: 2 files\n",
      "Dataset Processing batch 217/221: 2 files\n",
      "Dataset Processing batch 218/221: 2 files\n",
      "Dataset Processing batch 219/221: 2 files\n",
      "Dataset Processing batch 220/221: 2 files\n",
      "Dataset Processing batch 221/221: 1 files\n",
      "There are 49 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/25: 2 files\n",
      "Dataset Processing batch 2/25: 2 files\n",
      "Dataset Processing batch 3/25: 2 files\n",
      "Dataset Processing batch 4/25: 2 files\n",
      "Dataset Processing batch 5/25: 2 files\n",
      "Dataset Processing batch 6/25: 2 files\n",
      "Dataset Processing batch 7/25: 2 files\n",
      "Dataset Processing batch 8/25: 2 files\n",
      "Dataset Processing batch 9/25: 2 files\n",
      "Dataset Processing batch 10/25: 2 files\n",
      "Dataset Processing batch 11/25: 2 files\n",
      "Dataset Processing batch 12/25: 2 files\n",
      "Dataset Processing batch 13/25: 2 files\n",
      "Dataset Processing batch 14/25: 2 files\n",
      "Dataset Processing batch 15/25: 2 files\n",
      "Dataset Processing batch 16/25: 2 files\n",
      "Dataset Processing batch 17/25: 2 files\n",
      "Dataset Processing batch 18/25: 2 files\n",
      "Dataset Processing batch 19/25: 2 files\n",
      "Dataset Processing batch 20/25: 2 files\n",
      "Dataset Processing batch 21/25: 2 files\n",
      "Dataset Processing batch 22/25: 2 files\n",
      "Dataset Processing batch 23/25: 2 files\n",
      "Dataset Processing batch 24/25: 2 files\n",
      "Dataset Processing batch 25/25: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 2.37 \n",
      "\n",
      "========== batch: rep_2000 ========== \n",
      "Number of files with error: 0\n",
      "There are 441 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/221: 2 files\n",
      "Dataset Processing batch 2/221: 2 files\n",
      "Dataset Processing batch 3/221: 2 files\n",
      "Dataset Processing batch 4/221: 2 files\n",
      "Dataset Processing batch 5/221: 2 files\n",
      "Dataset Processing batch 6/221: 2 files\n",
      "Dataset Processing batch 7/221: 2 files\n",
      "Dataset Processing batch 8/221: 2 files\n",
      "Dataset Processing batch 9/221: 2 files\n",
      "Dataset Processing batch 10/221: 2 files\n",
      "Dataset Processing batch 11/221: 2 files\n",
      "Dataset Processing batch 12/221: 2 files\n",
      "Dataset Processing batch 13/221: 2 files\n",
      "Dataset Processing batch 14/221: 2 files\n",
      "Dataset Processing batch 15/221: 2 files\n",
      "Dataset Processing batch 16/221: 2 files\n",
      "Dataset Processing batch 17/221: 2 files\n",
      "Dataset Processing batch 18/221: 2 files\n",
      "Dataset Processing batch 19/221: 2 files\n",
      "Dataset Processing batch 20/221: 2 files\n",
      "Dataset Processing batch 21/221: 2 files\n",
      "Dataset Processing batch 22/221: 2 files\n",
      "Dataset Processing batch 23/221: 2 files\n",
      "Dataset Processing batch 24/221: 2 files\n",
      "Dataset Processing batch 25/221: 2 files\n",
      "Dataset Processing batch 26/221: 2 files\n",
      "Dataset Processing batch 27/221: 2 files\n",
      "Dataset Processing batch 28/221: 2 files\n",
      "Dataset Processing batch 29/221: 2 files\n",
      "Dataset Processing batch 30/221: 2 files\n",
      "Dataset Processing batch 31/221: 2 files\n",
      "Dataset Processing batch 32/221: 2 files\n",
      "Dataset Processing batch 33/221: 2 files\n",
      "Dataset Processing batch 34/221: 2 files\n",
      "Dataset Processing batch 35/221: 2 files\n",
      "Dataset Processing batch 36/221: 2 files\n",
      "Dataset Processing batch 37/221: 2 files\n",
      "Dataset Processing batch 38/221: 2 files\n",
      "Dataset Processing batch 39/221: 2 files\n",
      "Dataset Processing batch 40/221: 2 files\n",
      "Dataset Processing batch 41/221: 2 files\n",
      "Dataset Processing batch 42/221: 2 files\n",
      "Dataset Processing batch 43/221: 2 files\n",
      "Dataset Processing batch 44/221: 2 files\n",
      "Dataset Processing batch 45/221: 2 files\n",
      "Dataset Processing batch 46/221: 2 files\n",
      "Dataset Processing batch 47/221: 2 files\n",
      "Dataset Processing batch 48/221: 2 files\n",
      "Dataset Processing batch 49/221: 2 files\n",
      "Dataset Processing batch 50/221: 2 files\n",
      "Dataset Processing batch 51/221: 2 files\n",
      "Dataset Processing batch 52/221: 2 files\n",
      "Dataset Processing batch 53/221: 2 files\n",
      "Dataset Processing batch 54/221: 2 files\n",
      "Dataset Processing batch 55/221: 2 files\n",
      "Dataset Processing batch 56/221: 2 files\n",
      "Dataset Processing batch 57/221: 2 files\n",
      "Dataset Processing batch 58/221: 2 files\n",
      "Dataset Processing batch 59/221: 2 files\n",
      "Dataset Processing batch 60/221: 2 files\n",
      "Dataset Processing batch 61/221: 2 files\n",
      "Dataset Processing batch 62/221: 2 files\n",
      "Dataset Processing batch 63/221: 2 files\n",
      "Dataset Processing batch 64/221: 2 files\n",
      "Dataset Processing batch 65/221: 2 files\n",
      "Dataset Processing batch 66/221: 2 files\n",
      "Dataset Processing batch 67/221: 2 files\n",
      "Dataset Processing batch 68/221: 2 files\n",
      "Dataset Processing batch 69/221: 2 files\n",
      "Dataset Processing batch 70/221: 2 files\n",
      "Dataset Processing batch 71/221: 2 files\n",
      "Dataset Processing batch 72/221: 2 files\n",
      "Dataset Processing batch 73/221: 2 files\n",
      "Dataset Processing batch 74/221: 2 files\n",
      "Dataset Processing batch 75/221: 2 files\n",
      "Dataset Processing batch 76/221: 2 files\n",
      "Dataset Processing batch 77/221: 2 files\n",
      "Dataset Processing batch 78/221: 2 files\n",
      "Dataset Processing batch 79/221: 2 files\n",
      "Dataset Processing batch 80/221: 2 files\n",
      "Dataset Processing batch 81/221: 2 files\n",
      "Dataset Processing batch 82/221: 2 files\n",
      "Dataset Processing batch 83/221: 2 files\n",
      "Dataset Processing batch 84/221: 2 files\n",
      "Dataset Processing batch 85/221: 2 files\n",
      "Dataset Processing batch 86/221: 2 files\n",
      "Dataset Processing batch 87/221: 2 files\n",
      "Dataset Processing batch 88/221: 2 files\n",
      "Dataset Processing batch 89/221: 2 files\n",
      "Dataset Processing batch 90/221: 2 files\n",
      "Dataset Processing batch 91/221: 2 files\n",
      "Dataset Processing batch 92/221: 2 files\n",
      "Dataset Processing batch 93/221: 2 files\n",
      "Dataset Processing batch 94/221: 2 files\n",
      "Dataset Processing batch 95/221: 2 files\n",
      "Dataset Processing batch 96/221: 2 files\n",
      "Dataset Processing batch 97/221: 2 files\n",
      "Dataset Processing batch 98/221: 2 files\n",
      "Dataset Processing batch 99/221: 2 files\n",
      "Dataset Processing batch 100/221: 2 files\n",
      "Dataset Processing batch 101/221: 2 files\n",
      "Dataset Processing batch 102/221: 2 files\n",
      "Dataset Processing batch 103/221: 2 files\n",
      "Dataset Processing batch 104/221: 2 files\n",
      "Dataset Processing batch 105/221: 2 files\n",
      "Dataset Processing batch 106/221: 2 files\n",
      "Dataset Processing batch 107/221: 2 files\n",
      "Dataset Processing batch 108/221: 2 files\n",
      "Dataset Processing batch 109/221: 2 files\n",
      "Dataset Processing batch 110/221: 2 files\n",
      "Dataset Processing batch 111/221: 2 files\n",
      "Dataset Processing batch 112/221: 2 files\n",
      "Dataset Processing batch 113/221: 2 files\n",
      "Dataset Processing batch 114/221: 2 files\n",
      "Dataset Processing batch 115/221: 2 files\n",
      "Dataset Processing batch 116/221: 2 files\n",
      "Dataset Processing batch 117/221: 2 files\n",
      "Dataset Processing batch 118/221: 2 files\n",
      "Dataset Processing batch 119/221: 2 files\n",
      "Dataset Processing batch 120/221: 2 files\n",
      "Dataset Processing batch 121/221: 2 files\n",
      "Dataset Processing batch 122/221: 2 files\n",
      "Dataset Processing batch 123/221: 2 files\n",
      "Dataset Processing batch 124/221: 2 files\n",
      "Dataset Processing batch 125/221: 2 files\n",
      "Dataset Processing batch 126/221: 2 files\n",
      "Dataset Processing batch 127/221: 2 files\n",
      "Dataset Processing batch 128/221: 2 files\n",
      "Dataset Processing batch 129/221: 2 files\n",
      "Dataset Processing batch 130/221: 2 files\n",
      "Dataset Processing batch 131/221: 2 files\n",
      "Dataset Processing batch 132/221: 2 files\n",
      "Dataset Processing batch 133/221: 2 files\n",
      "Dataset Processing batch 134/221: 2 files\n",
      "Dataset Processing batch 135/221: 2 files\n",
      "Dataset Processing batch 136/221: 2 files\n",
      "Dataset Processing batch 137/221: 2 files\n",
      "Dataset Processing batch 138/221: 2 files\n",
      "Dataset Processing batch 139/221: 2 files\n",
      "Dataset Processing batch 140/221: 2 files\n",
      "Dataset Processing batch 141/221: 2 files\n",
      "Dataset Processing batch 142/221: 2 files\n",
      "Dataset Processing batch 143/221: 2 files\n",
      "Dataset Processing batch 144/221: 2 files\n",
      "Dataset Processing batch 145/221: 2 files\n",
      "Dataset Processing batch 146/221: 2 files\n",
      "Dataset Processing batch 147/221: 2 files\n",
      "Dataset Processing batch 148/221: 2 files\n",
      "Dataset Processing batch 149/221: 2 files\n",
      "Dataset Processing batch 150/221: 2 files\n",
      "Dataset Processing batch 151/221: 2 files\n",
      "Dataset Processing batch 152/221: 2 files\n",
      "Dataset Processing batch 153/221: 2 files\n",
      "Dataset Processing batch 154/221: 2 files\n",
      "Dataset Processing batch 155/221: 2 files\n",
      "Dataset Processing batch 156/221: 2 files\n",
      "Dataset Processing batch 157/221: 2 files\n",
      "Dataset Processing batch 158/221: 2 files\n",
      "Dataset Processing batch 159/221: 2 files\n",
      "Dataset Processing batch 160/221: 2 files\n",
      "Dataset Processing batch 161/221: 2 files\n",
      "Dataset Processing batch 162/221: 2 files\n",
      "Dataset Processing batch 163/221: 2 files\n",
      "Dataset Processing batch 164/221: 2 files\n",
      "Dataset Processing batch 165/221: 2 files\n",
      "Dataset Processing batch 166/221: 2 files\n",
      "Dataset Processing batch 167/221: 2 files\n",
      "Dataset Processing batch 168/221: 2 files\n",
      "Dataset Processing batch 169/221: 2 files\n",
      "Dataset Processing batch 170/221: 2 files\n",
      "Dataset Processing batch 171/221: 2 files\n",
      "Dataset Processing batch 172/221: 2 files\n",
      "Dataset Processing batch 173/221: 2 files\n",
      "Dataset Processing batch 174/221: 2 files\n",
      "Dataset Processing batch 175/221: 2 files\n",
      "Dataset Processing batch 176/221: 2 files\n",
      "Dataset Processing batch 177/221: 2 files\n",
      "Dataset Processing batch 178/221: 2 files\n",
      "Dataset Processing batch 179/221: 2 files\n",
      "Dataset Processing batch 180/221: 2 files\n",
      "Dataset Processing batch 181/221: 2 files\n",
      "Dataset Processing batch 182/221: 2 files\n",
      "Dataset Processing batch 183/221: 2 files\n",
      "Dataset Processing batch 184/221: 2 files\n",
      "Dataset Processing batch 185/221: 2 files\n",
      "Dataset Processing batch 186/221: 2 files\n",
      "Dataset Processing batch 187/221: 2 files\n",
      "Dataset Processing batch 188/221: 2 files\n",
      "Dataset Processing batch 189/221: 2 files\n",
      "Dataset Processing batch 190/221: 2 files\n",
      "Dataset Processing batch 191/221: 2 files\n",
      "Dataset Processing batch 192/221: 2 files\n",
      "Dataset Processing batch 193/221: 2 files\n",
      "Dataset Processing batch 194/221: 2 files\n",
      "Dataset Processing batch 195/221: 2 files\n",
      "Dataset Processing batch 196/221: 2 files\n",
      "Dataset Processing batch 197/221: 2 files\n",
      "Dataset Processing batch 198/221: 2 files\n",
      "Dataset Processing batch 199/221: 2 files\n",
      "Dataset Processing batch 200/221: 2 files\n",
      "Dataset Processing batch 201/221: 2 files\n",
      "Dataset Processing batch 202/221: 2 files\n",
      "Dataset Processing batch 203/221: 2 files\n",
      "Dataset Processing batch 204/221: 2 files\n",
      "Dataset Processing batch 205/221: 2 files\n",
      "Dataset Processing batch 206/221: 2 files\n",
      "Dataset Processing batch 207/221: 2 files\n",
      "Dataset Processing batch 208/221: 2 files\n",
      "Dataset Processing batch 209/221: 2 files\n",
      "Dataset Processing batch 210/221: 2 files\n",
      "Dataset Processing batch 211/221: 2 files\n",
      "Dataset Processing batch 212/221: 2 files\n",
      "Dataset Processing batch 213/221: 2 files\n",
      "Dataset Processing batch 214/221: 2 files\n",
      "Dataset Processing batch 215/221: 2 files\n",
      "Dataset Processing batch 216/221: 2 files\n",
      "Dataset Processing batch 217/221: 2 files\n",
      "Dataset Processing batch 218/221: 2 files\n",
      "Dataset Processing batch 219/221: 2 files\n",
      "Dataset Processing batch 220/221: 2 files\n",
      "Dataset Processing batch 221/221: 1 files\n",
      "There are 49 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/25: 2 files\n",
      "Dataset Processing batch 2/25: 2 files\n",
      "Dataset Processing batch 3/25: 2 files\n",
      "Dataset Processing batch 4/25: 2 files\n",
      "Dataset Processing batch 5/25: 2 files\n",
      "Dataset Processing batch 6/25: 2 files\n",
      "Dataset Processing batch 7/25: 2 files\n",
      "Dataset Processing batch 8/25: 2 files\n",
      "Dataset Processing batch 9/25: 2 files\n",
      "Dataset Processing batch 10/25: 2 files\n",
      "Dataset Processing batch 11/25: 2 files\n",
      "Dataset Processing batch 12/25: 2 files\n",
      "Dataset Processing batch 13/25: 2 files\n",
      "Dataset Processing batch 14/25: 2 files\n",
      "Dataset Processing batch 15/25: 2 files\n",
      "Dataset Processing batch 16/25: 2 files\n",
      "Dataset Processing batch 17/25: 2 files\n",
      "Dataset Processing batch 18/25: 2 files\n",
      "Dataset Processing batch 19/25: 2 files\n",
      "Dataset Processing batch 20/25: 2 files\n",
      "Dataset Processing batch 21/25: 2 files\n",
      "Dataset Processing batch 22/25: 2 files\n",
      "Dataset Processing batch 23/25: 2 files\n",
      "Dataset Processing batch 24/25: 2 files\n",
      "Dataset Processing batch 25/25: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 4.63 \n",
      "\n",
      "========== batch: rep_2500 ========== \n",
      "Number of files with error: 0\n",
      "There are 446 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/224: 2 files\n",
      "Dataset Processing batch 2/224: 2 files\n",
      "Dataset Processing batch 3/224: 2 files\n",
      "Dataset Processing batch 4/224: 2 files\n",
      "Dataset Processing batch 5/224: 2 files\n",
      "Dataset Processing batch 6/224: 2 files\n",
      "Dataset Processing batch 7/224: 2 files\n",
      "Dataset Processing batch 8/224: 2 files\n",
      "Dataset Processing batch 9/224: 2 files\n",
      "Dataset Processing batch 10/224: 2 files\n",
      "Dataset Processing batch 11/224: 2 files\n",
      "Dataset Processing batch 12/224: 2 files\n",
      "Dataset Processing batch 13/224: 2 files\n",
      "Dataset Processing batch 14/224: 2 files\n",
      "Dataset Processing batch 15/224: 2 files\n",
      "Dataset Processing batch 16/224: 2 files\n",
      "Dataset Processing batch 17/224: 2 files\n",
      "Dataset Processing batch 18/224: 2 files\n",
      "Dataset Processing batch 19/224: 2 files\n",
      "Dataset Processing batch 20/224: 2 files\n",
      "Dataset Processing batch 21/224: 2 files\n",
      "Dataset Processing batch 22/224: 2 files\n",
      "Dataset Processing batch 23/224: 2 files\n",
      "Dataset Processing batch 24/224: 2 files\n",
      "Dataset Processing batch 25/224: 2 files\n",
      "Dataset Processing batch 26/224: 2 files\n",
      "Dataset Processing batch 27/224: 2 files\n",
      "Dataset Processing batch 28/224: 2 files\n",
      "Dataset Processing batch 29/224: 2 files\n",
      "Dataset Processing batch 30/224: 2 files\n",
      "Dataset Processing batch 31/224: 2 files\n",
      "Dataset Processing batch 32/224: 2 files\n",
      "Dataset Processing batch 33/224: 2 files\n",
      "Dataset Processing batch 34/224: 2 files\n",
      "Dataset Processing batch 35/224: 2 files\n",
      "Dataset Processing batch 36/224: 2 files\n",
      "Dataset Processing batch 37/224: 2 files\n",
      "Dataset Processing batch 38/224: 2 files\n",
      "Dataset Processing batch 39/224: 2 files\n",
      "Dataset Processing batch 40/224: 2 files\n",
      "Dataset Processing batch 41/224: 2 files\n",
      "Dataset Processing batch 42/224: 2 files\n",
      "Dataset Processing batch 43/224: 2 files\n",
      "Dataset Processing batch 44/224: 2 files\n",
      "Dataset Processing batch 45/224: 2 files\n",
      "Dataset Processing batch 46/224: 2 files\n",
      "Dataset Processing batch 47/224: 2 files\n",
      "Dataset Processing batch 48/224: 2 files\n",
      "Dataset Processing batch 49/224: 2 files\n",
      "Dataset Processing batch 50/224: 2 files\n",
      "Dataset Processing batch 51/224: 2 files\n",
      "Dataset Processing batch 52/224: 2 files\n",
      "Dataset Processing batch 53/224: 2 files\n",
      "Dataset Processing batch 54/224: 2 files\n",
      "Dataset Processing batch 55/224: 2 files\n",
      "Dataset Processing batch 56/224: 2 files\n",
      "Dataset Processing batch 57/224: 2 files\n",
      "Dataset Processing batch 58/224: 2 files\n",
      "Dataset Processing batch 59/224: 2 files\n",
      "Dataset Processing batch 60/224: 2 files\n",
      "Dataset Processing batch 61/224: 2 files\n",
      "Dataset Processing batch 62/224: 2 files\n",
      "Dataset Processing batch 63/224: 2 files\n",
      "Dataset Processing batch 64/224: 2 files\n",
      "Dataset Processing batch 65/224: 2 files\n",
      "Dataset Processing batch 66/224: 2 files\n",
      "Dataset Processing batch 67/224: 2 files\n",
      "Dataset Processing batch 68/224: 2 files\n",
      "Dataset Processing batch 69/224: 2 files\n",
      "Dataset Processing batch 70/224: 2 files\n",
      "Dataset Processing batch 71/224: 2 files\n",
      "Dataset Processing batch 72/224: 2 files\n",
      "Dataset Processing batch 73/224: 2 files\n",
      "Dataset Processing batch 74/224: 2 files\n",
      "Dataset Processing batch 75/224: 2 files\n",
      "Dataset Processing batch 76/224: 2 files\n",
      "Dataset Processing batch 77/224: 2 files\n",
      "Dataset Processing batch 78/224: 2 files\n",
      "Dataset Processing batch 79/224: 2 files\n",
      "Dataset Processing batch 80/224: 2 files\n",
      "Dataset Processing batch 81/224: 2 files\n",
      "Dataset Processing batch 82/224: 2 files\n",
      "Dataset Processing batch 83/224: 2 files\n",
      "Dataset Processing batch 84/224: 2 files\n",
      "Dataset Processing batch 85/224: 2 files\n",
      "Dataset Processing batch 86/224: 2 files\n",
      "Dataset Processing batch 87/224: 2 files\n",
      "Dataset Processing batch 88/224: 2 files\n",
      "Dataset Processing batch 89/224: 2 files\n",
      "Dataset Processing batch 90/224: 2 files\n",
      "Dataset Processing batch 91/224: 2 files\n",
      "Dataset Processing batch 92/224: 2 files\n",
      "Dataset Processing batch 93/224: 2 files\n",
      "Dataset Processing batch 94/224: 2 files\n",
      "Dataset Processing batch 95/224: 2 files\n",
      "Dataset Processing batch 96/224: 2 files\n",
      "Dataset Processing batch 97/224: 2 files\n",
      "Dataset Processing batch 98/224: 2 files\n",
      "Dataset Processing batch 99/224: 2 files\n",
      "Dataset Processing batch 100/224: 2 files\n",
      "Dataset Processing batch 101/224: 2 files\n",
      "Dataset Processing batch 102/224: 2 files\n",
      "Dataset Processing batch 103/224: 2 files\n",
      "Dataset Processing batch 104/224: 2 files\n",
      "Dataset Processing batch 105/224: 2 files\n",
      "Dataset Processing batch 106/224: 2 files\n",
      "Dataset Processing batch 107/224: 2 files\n",
      "Dataset Processing batch 108/224: 2 files\n",
      "Dataset Processing batch 109/224: 2 files\n",
      "Dataset Processing batch 110/224: 2 files\n",
      "Dataset Processing batch 111/224: 2 files\n",
      "Dataset Processing batch 112/224: 2 files\n",
      "Dataset Processing batch 113/224: 2 files\n",
      "Dataset Processing batch 114/224: 2 files\n",
      "Dataset Processing batch 115/224: 2 files\n",
      "Dataset Processing batch 116/224: 2 files\n",
      "Dataset Processing batch 117/224: 2 files\n",
      "Dataset Processing batch 118/224: 2 files\n",
      "Dataset Processing batch 119/224: 2 files\n",
      "Dataset Processing batch 120/224: 2 files\n",
      "Dataset Processing batch 121/224: 2 files\n",
      "Dataset Processing batch 122/224: 2 files\n",
      "Dataset Processing batch 123/224: 2 files\n",
      "Dataset Processing batch 124/224: 2 files\n",
      "Dataset Processing batch 125/224: 2 files\n",
      "Dataset Processing batch 126/224: 2 files\n",
      "Dataset Processing batch 127/224: 2 files\n",
      "Dataset Processing batch 128/224: 2 files\n",
      "Dataset Processing batch 129/224: 2 files\n",
      "Dataset Processing batch 130/224: 2 files\n",
      "Dataset Processing batch 131/224: 2 files\n",
      "Dataset Processing batch 132/224: 2 files\n",
      "Dataset Processing batch 133/224: 2 files\n",
      "Dataset Processing batch 134/224: 2 files\n",
      "Dataset Processing batch 135/224: 2 files\n",
      "Dataset Processing batch 136/224: 2 files\n",
      "Dataset Processing batch 137/224: 2 files\n",
      "Dataset Processing batch 138/224: 2 files\n",
      "Dataset Processing batch 139/224: 2 files\n",
      "Dataset Processing batch 140/224: 2 files\n",
      "Dataset Processing batch 141/224: 2 files\n",
      "Dataset Processing batch 142/224: 2 files\n",
      "Dataset Processing batch 143/224: 2 files\n",
      "Dataset Processing batch 144/224: 2 files\n",
      "Dataset Processing batch 145/224: 2 files\n",
      "Dataset Processing batch 146/224: 2 files\n",
      "Dataset Processing batch 147/224: 2 files\n",
      "Dataset Processing batch 148/224: 2 files\n",
      "Dataset Processing batch 149/224: 2 files\n",
      "Dataset Processing batch 150/224: 2 files\n",
      "Dataset Processing batch 151/224: 2 files\n",
      "Dataset Processing batch 152/224: 2 files\n",
      "Dataset Processing batch 153/224: 2 files\n",
      "Dataset Processing batch 154/224: 2 files\n",
      "Dataset Processing batch 155/224: 2 files\n",
      "Dataset Processing batch 156/224: 2 files\n",
      "Dataset Processing batch 157/224: 2 files\n",
      "Dataset Processing batch 158/224: 2 files\n",
      "Dataset Processing batch 159/224: 2 files\n",
      "Dataset Processing batch 160/224: 2 files\n",
      "Dataset Processing batch 161/224: 2 files\n",
      "Dataset Processing batch 162/224: 2 files\n",
      "Dataset Processing batch 163/224: 2 files\n",
      "Dataset Processing batch 164/224: 2 files\n",
      "Dataset Processing batch 165/224: 2 files\n",
      "Dataset Processing batch 166/224: 2 files\n",
      "Dataset Processing batch 167/224: 2 files\n",
      "Dataset Processing batch 168/224: 2 files\n",
      "Dataset Processing batch 169/224: 2 files\n",
      "Dataset Processing batch 170/224: 2 files\n",
      "Dataset Processing batch 171/224: 2 files\n",
      "Dataset Processing batch 172/224: 2 files\n",
      "Dataset Processing batch 173/224: 2 files\n",
      "Dataset Processing batch 174/224: 2 files\n",
      "Dataset Processing batch 175/224: 2 files\n",
      "Dataset Processing batch 176/224: 2 files\n",
      "Dataset Processing batch 177/224: 2 files\n",
      "Dataset Processing batch 178/224: 2 files\n",
      "Dataset Processing batch 179/224: 2 files\n",
      "Dataset Processing batch 180/224: 2 files\n",
      "Dataset Processing batch 181/224: 2 files\n",
      "Dataset Processing batch 182/224: 2 files\n",
      "Dataset Processing batch 183/224: 2 files\n",
      "Dataset Processing batch 184/224: 2 files\n",
      "Dataset Processing batch 185/224: 2 files\n",
      "Dataset Processing batch 186/224: 2 files\n",
      "Dataset Processing batch 187/224: 2 files\n",
      "Dataset Processing batch 188/224: 2 files\n",
      "Dataset Processing batch 189/224: 2 files\n",
      "Dataset Processing batch 190/224: 2 files\n",
      "Dataset Processing batch 191/224: 2 files\n",
      "Dataset Processing batch 192/224: 2 files\n",
      "Dataset Processing batch 193/224: 2 files\n",
      "Dataset Processing batch 194/224: 2 files\n",
      "Dataset Processing batch 195/224: 2 files\n",
      "Dataset Processing batch 196/224: 2 files\n",
      "Dataset Processing batch 197/224: 2 files\n",
      "Dataset Processing batch 198/224: 2 files\n",
      "Dataset Processing batch 199/224: 2 files\n",
      "Dataset Processing batch 200/224: 2 files\n",
      "Dataset Processing batch 201/224: 2 files\n",
      "Dataset Processing batch 202/224: 2 files\n",
      "Dataset Processing batch 203/224: 2 files\n",
      "Dataset Processing batch 204/224: 2 files\n",
      "Dataset Processing batch 205/224: 2 files\n",
      "Dataset Processing batch 206/224: 2 files\n",
      "Dataset Processing batch 207/224: 2 files\n",
      "Dataset Processing batch 208/224: 2 files\n",
      "Dataset Processing batch 209/224: 2 files\n",
      "Dataset Processing batch 210/224: 2 files\n",
      "Dataset Processing batch 211/224: 2 files\n",
      "Dataset Processing batch 212/224: 2 files\n",
      "Dataset Processing batch 213/224: 2 files\n",
      "Dataset Processing batch 214/224: 2 files\n",
      "Dataset Processing batch 215/224: 2 files\n",
      "Dataset Processing batch 216/224: 2 files\n",
      "Dataset Processing batch 217/224: 2 files\n",
      "Dataset Processing batch 218/224: 2 files\n",
      "Dataset Processing batch 219/224: 2 files\n",
      "Dataset Processing batch 220/224: 2 files\n",
      "Dataset Processing batch 221/224: 2 files\n",
      "Dataset Processing batch 222/224: 2 files\n",
      "Dataset Processing batch 223/224: 2 files\n",
      "There are 50 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/26: 2 files\n",
      "Dataset Processing batch 2/26: 2 files\n",
      "Dataset Processing batch 3/26: 2 files\n",
      "Dataset Processing batch 4/26: 2 files\n",
      "Dataset Processing batch 5/26: 2 files\n",
      "Dataset Processing batch 6/26: 2 files\n",
      "Dataset Processing batch 7/26: 2 files\n",
      "Dataset Processing batch 8/26: 2 files\n",
      "Dataset Processing batch 9/26: 2 files\n",
      "Dataset Processing batch 10/26: 2 files\n",
      "Dataset Processing batch 11/26: 2 files\n",
      "Dataset Processing batch 12/26: 2 files\n",
      "Dataset Processing batch 13/26: 2 files\n",
      "Dataset Processing batch 14/26: 2 files\n",
      "Dataset Processing batch 15/26: 2 files\n",
      "Dataset Processing batch 16/26: 2 files\n",
      "Dataset Processing batch 17/26: 2 files\n",
      "Dataset Processing batch 18/26: 2 files\n",
      "Dataset Processing batch 19/26: 2 files\n",
      "Dataset Processing batch 20/26: 2 files\n",
      "Dataset Processing batch 21/26: 2 files\n",
      "Dataset Processing batch 22/26: 2 files\n",
      "Dataset Processing batch 23/26: 2 files\n",
      "Dataset Processing batch 24/26: 2 files\n",
      "Dataset Processing batch 25/26: 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 1.88 \n",
      "\n",
      "========== batch: rep_3000 ========== \n",
      "Number of files with error: 0\n",
      "There are 447 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/224: 2 files\n",
      "Dataset Processing batch 2/224: 2 files\n",
      "Dataset Processing batch 3/224: 2 files\n",
      "Dataset Processing batch 4/224: 2 files\n",
      "Dataset Processing batch 5/224: 2 files\n",
      "Dataset Processing batch 6/224: 2 files\n",
      "Dataset Processing batch 7/224: 2 files\n",
      "Dataset Processing batch 8/224: 2 files\n",
      "Dataset Processing batch 9/224: 2 files\n",
      "Dataset Processing batch 10/224: 2 files\n",
      "Dataset Processing batch 11/224: 2 files\n",
      "Dataset Processing batch 12/224: 2 files\n",
      "Dataset Processing batch 13/224: 2 files\n",
      "Dataset Processing batch 14/224: 2 files\n",
      "Dataset Processing batch 15/224: 2 files\n",
      "Dataset Processing batch 16/224: 2 files\n",
      "Dataset Processing batch 17/224: 2 files\n",
      "Dataset Processing batch 18/224: 2 files\n",
      "Dataset Processing batch 19/224: 2 files\n",
      "Dataset Processing batch 20/224: 2 files\n",
      "Dataset Processing batch 21/224: 2 files\n",
      "Dataset Processing batch 22/224: 2 files\n",
      "Dataset Processing batch 23/224: 2 files\n",
      "Dataset Processing batch 24/224: 2 files\n",
      "Dataset Processing batch 25/224: 2 files\n",
      "Dataset Processing batch 26/224: 2 files\n",
      "Dataset Processing batch 27/224: 2 files\n",
      "Dataset Processing batch 28/224: 2 files\n",
      "Dataset Processing batch 29/224: 2 files\n",
      "Dataset Processing batch 30/224: 2 files\n",
      "Dataset Processing batch 31/224: 2 files\n",
      "Dataset Processing batch 32/224: 2 files\n",
      "Dataset Processing batch 33/224: 2 files\n",
      "Dataset Processing batch 34/224: 2 files\n",
      "Dataset Processing batch 35/224: 2 files\n",
      "Dataset Processing batch 36/224: 2 files\n",
      "Dataset Processing batch 37/224: 2 files\n",
      "Dataset Processing batch 38/224: 2 files\n",
      "Dataset Processing batch 39/224: 2 files\n",
      "Dataset Processing batch 40/224: 2 files\n",
      "Dataset Processing batch 41/224: 2 files\n",
      "Dataset Processing batch 42/224: 2 files\n",
      "Dataset Processing batch 43/224: 2 files\n",
      "Dataset Processing batch 44/224: 2 files\n",
      "Dataset Processing batch 45/224: 2 files\n",
      "Dataset Processing batch 46/224: 2 files\n",
      "Dataset Processing batch 47/224: 2 files\n",
      "Dataset Processing batch 48/224: 2 files\n",
      "Dataset Processing batch 49/224: 2 files\n",
      "Dataset Processing batch 50/224: 2 files\n",
      "Dataset Processing batch 51/224: 2 files\n",
      "Dataset Processing batch 52/224: 2 files\n",
      "Dataset Processing batch 53/224: 2 files\n",
      "Dataset Processing batch 54/224: 2 files\n",
      "Dataset Processing batch 55/224: 2 files\n",
      "Dataset Processing batch 56/224: 2 files\n",
      "Dataset Processing batch 57/224: 2 files\n",
      "Dataset Processing batch 58/224: 2 files\n",
      "Dataset Processing batch 59/224: 2 files\n",
      "Dataset Processing batch 60/224: 2 files\n",
      "Dataset Processing batch 61/224: 2 files\n",
      "Dataset Processing batch 62/224: 2 files\n",
      "Dataset Processing batch 63/224: 2 files\n",
      "Dataset Processing batch 64/224: 2 files\n",
      "Dataset Processing batch 65/224: 2 files\n",
      "Dataset Processing batch 66/224: 2 files\n",
      "Dataset Processing batch 67/224: 2 files\n",
      "Dataset Processing batch 68/224: 2 files\n",
      "Dataset Processing batch 69/224: 2 files\n",
      "Dataset Processing batch 70/224: 2 files\n",
      "Dataset Processing batch 71/224: 2 files\n",
      "Dataset Processing batch 72/224: 2 files\n",
      "Dataset Processing batch 73/224: 2 files\n",
      "Dataset Processing batch 74/224: 2 files\n",
      "Dataset Processing batch 75/224: 2 files\n",
      "Dataset Processing batch 76/224: 2 files\n",
      "Dataset Processing batch 77/224: 2 files\n",
      "Dataset Processing batch 78/224: 2 files\n",
      "Dataset Processing batch 79/224: 2 files\n",
      "Dataset Processing batch 80/224: 2 files\n",
      "Dataset Processing batch 81/224: 2 files\n",
      "Dataset Processing batch 82/224: 2 files\n",
      "Dataset Processing batch 83/224: 2 files\n",
      "Dataset Processing batch 84/224: 2 files\n",
      "Dataset Processing batch 85/224: 2 files\n",
      "Dataset Processing batch 86/224: 2 files\n",
      "Dataset Processing batch 87/224: 2 files\n",
      "Dataset Processing batch 88/224: 2 files\n",
      "Dataset Processing batch 89/224: 2 files\n",
      "Dataset Processing batch 90/224: 2 files\n",
      "Dataset Processing batch 91/224: 2 files\n",
      "Dataset Processing batch 92/224: 2 files\n",
      "Dataset Processing batch 93/224: 2 files\n",
      "Dataset Processing batch 94/224: 2 files\n",
      "Dataset Processing batch 95/224: 2 files\n",
      "Dataset Processing batch 96/224: 2 files\n",
      "Dataset Processing batch 97/224: 2 files\n",
      "Dataset Processing batch 98/224: 2 files\n",
      "Dataset Processing batch 99/224: 2 files\n",
      "Dataset Processing batch 100/224: 2 files\n",
      "Dataset Processing batch 101/224: 2 files\n",
      "Dataset Processing batch 102/224: 2 files\n",
      "Dataset Processing batch 103/224: 2 files\n",
      "Dataset Processing batch 104/224: 2 files\n",
      "Dataset Processing batch 105/224: 2 files\n",
      "Dataset Processing batch 106/224: 2 files\n",
      "Dataset Processing batch 107/224: 2 files\n",
      "Dataset Processing batch 108/224: 2 files\n",
      "Dataset Processing batch 109/224: 2 files\n",
      "Dataset Processing batch 110/224: 2 files\n",
      "Dataset Processing batch 111/224: 2 files\n",
      "Dataset Processing batch 112/224: 2 files\n",
      "Dataset Processing batch 113/224: 2 files\n",
      "Dataset Processing batch 114/224: 2 files\n",
      "Dataset Processing batch 115/224: 2 files\n",
      "Dataset Processing batch 116/224: 2 files\n",
      "Dataset Processing batch 117/224: 2 files\n",
      "Dataset Processing batch 118/224: 2 files\n",
      "Dataset Processing batch 119/224: 2 files\n",
      "Dataset Processing batch 120/224: 2 files\n",
      "Dataset Processing batch 121/224: 2 files\n",
      "Dataset Processing batch 122/224: 2 files\n",
      "Dataset Processing batch 123/224: 2 files\n",
      "Dataset Processing batch 124/224: 2 files\n",
      "Dataset Processing batch 125/224: 2 files\n",
      "Dataset Processing batch 126/224: 2 files\n",
      "Dataset Processing batch 127/224: 2 files\n",
      "Dataset Processing batch 128/224: 2 files\n",
      "Dataset Processing batch 129/224: 2 files\n",
      "Dataset Processing batch 130/224: 2 files\n",
      "Dataset Processing batch 131/224: 2 files\n",
      "Dataset Processing batch 132/224: 2 files\n",
      "Dataset Processing batch 133/224: 2 files\n",
      "Dataset Processing batch 134/224: 2 files\n",
      "Dataset Processing batch 135/224: 2 files\n",
      "Dataset Processing batch 136/224: 2 files\n",
      "Dataset Processing batch 137/224: 2 files\n",
      "Dataset Processing batch 138/224: 2 files\n",
      "Dataset Processing batch 139/224: 2 files\n",
      "Dataset Processing batch 140/224: 2 files\n",
      "Dataset Processing batch 141/224: 2 files\n",
      "Dataset Processing batch 142/224: 2 files\n",
      "Dataset Processing batch 143/224: 2 files\n",
      "Dataset Processing batch 144/224: 2 files\n",
      "Dataset Processing batch 145/224: 2 files\n",
      "Dataset Processing batch 146/224: 2 files\n",
      "Dataset Processing batch 147/224: 2 files\n",
      "Dataset Processing batch 148/224: 2 files\n",
      "Dataset Processing batch 149/224: 2 files\n",
      "Dataset Processing batch 150/224: 2 files\n",
      "Dataset Processing batch 151/224: 2 files\n",
      "Dataset Processing batch 152/224: 2 files\n",
      "Dataset Processing batch 153/224: 2 files\n",
      "Dataset Processing batch 154/224: 2 files\n",
      "Dataset Processing batch 155/224: 2 files\n",
      "Dataset Processing batch 156/224: 2 files\n",
      "Dataset Processing batch 157/224: 2 files\n",
      "Dataset Processing batch 158/224: 2 files\n",
      "Dataset Processing batch 159/224: 2 files\n",
      "Dataset Processing batch 160/224: 2 files\n",
      "Dataset Processing batch 161/224: 2 files\n",
      "Dataset Processing batch 162/224: 2 files\n",
      "Dataset Processing batch 163/224: 2 files\n",
      "Dataset Processing batch 164/224: 2 files\n",
      "Dataset Processing batch 165/224: 2 files\n",
      "Dataset Processing batch 166/224: 2 files\n",
      "Dataset Processing batch 167/224: 2 files\n",
      "Dataset Processing batch 168/224: 2 files\n",
      "Dataset Processing batch 169/224: 2 files\n",
      "Dataset Processing batch 170/224: 2 files\n",
      "Dataset Processing batch 171/224: 2 files\n",
      "Dataset Processing batch 172/224: 2 files\n",
      "Dataset Processing batch 173/224: 2 files\n",
      "Dataset Processing batch 174/224: 2 files\n",
      "Dataset Processing batch 175/224: 2 files\n",
      "Dataset Processing batch 176/224: 2 files\n",
      "Dataset Processing batch 177/224: 2 files\n",
      "Dataset Processing batch 178/224: 2 files\n",
      "Dataset Processing batch 179/224: 2 files\n",
      "Dataset Processing batch 180/224: 2 files\n",
      "Dataset Processing batch 181/224: 2 files\n",
      "Dataset Processing batch 182/224: 2 files\n",
      "Dataset Processing batch 183/224: 2 files\n",
      "Dataset Processing batch 184/224: 2 files\n",
      "Dataset Processing batch 185/224: 2 files\n",
      "Dataset Processing batch 186/224: 2 files\n",
      "Dataset Processing batch 187/224: 2 files\n",
      "Dataset Processing batch 188/224: 2 files\n",
      "Dataset Processing batch 189/224: 2 files\n",
      "Dataset Processing batch 190/224: 2 files\n",
      "Dataset Processing batch 191/224: 2 files\n",
      "Dataset Processing batch 192/224: 2 files\n",
      "Dataset Processing batch 193/224: 2 files\n",
      "Dataset Processing batch 194/224: 2 files\n",
      "Dataset Processing batch 195/224: 2 files\n",
      "Dataset Processing batch 196/224: 2 files\n",
      "Dataset Processing batch 197/224: 2 files\n",
      "Dataset Processing batch 198/224: 2 files\n",
      "Dataset Processing batch 199/224: 2 files\n",
      "Dataset Processing batch 200/224: 2 files\n",
      "Dataset Processing batch 201/224: 2 files\n",
      "Dataset Processing batch 202/224: 2 files\n",
      "Dataset Processing batch 203/224: 2 files\n",
      "Dataset Processing batch 204/224: 2 files\n",
      "Dataset Processing batch 205/224: 2 files\n",
      "Dataset Processing batch 206/224: 2 files\n",
      "Dataset Processing batch 207/224: 2 files\n",
      "Dataset Processing batch 208/224: 2 files\n",
      "Dataset Processing batch 209/224: 2 files\n",
      "Dataset Processing batch 210/224: 2 files\n",
      "Dataset Processing batch 211/224: 2 files\n",
      "Dataset Processing batch 212/224: 2 files\n",
      "Dataset Processing batch 213/224: 2 files\n",
      "Dataset Processing batch 214/224: 2 files\n",
      "Dataset Processing batch 215/224: 2 files\n",
      "Dataset Processing batch 216/224: 2 files\n",
      "Dataset Processing batch 217/224: 2 files\n",
      "Dataset Processing batch 218/224: 2 files\n",
      "Dataset Processing batch 219/224: 2 files\n",
      "Dataset Processing batch 220/224: 2 files\n",
      "Dataset Processing batch 221/224: 2 files\n",
      "Dataset Processing batch 222/224: 2 files\n",
      "Dataset Processing batch 223/224: 2 files\n",
      "Dataset Processing batch 224/224: 1 files\n",
      "There are 50 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/26: 2 files\n",
      "Dataset Processing batch 2/26: 2 files\n",
      "Dataset Processing batch 3/26: 2 files\n",
      "Dataset Processing batch 4/26: 2 files\n",
      "Dataset Processing batch 5/26: 2 files\n",
      "Dataset Processing batch 6/26: 2 files\n",
      "Dataset Processing batch 7/26: 2 files\n",
      "Dataset Processing batch 8/26: 2 files\n",
      "Dataset Processing batch 9/26: 2 files\n",
      "Dataset Processing batch 10/26: 2 files\n",
      "Dataset Processing batch 11/26: 2 files\n",
      "Dataset Processing batch 12/26: 2 files\n",
      "Dataset Processing batch 13/26: 2 files\n",
      "Dataset Processing batch 14/26: 2 files\n",
      "Dataset Processing batch 15/26: 2 files\n",
      "Dataset Processing batch 16/26: 2 files\n",
      "Dataset Processing batch 17/26: 2 files\n",
      "Dataset Processing batch 18/26: 2 files\n",
      "Dataset Processing batch 19/26: 2 files\n",
      "Dataset Processing batch 20/26: 2 files\n",
      "Dataset Processing batch 21/26: 2 files\n",
      "Dataset Processing batch 22/26: 2 files\n",
      "Dataset Processing batch 23/26: 2 files\n",
      "Dataset Processing batch 24/26: 2 files\n",
      "Dataset Processing batch 25/26: 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 1.57 \n",
      "\n",
      "========== batch: rep_3500 ========== \n",
      "Number of files with error: 0\n",
      "There are 447 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/224: 2 files\n",
      "Dataset Processing batch 2/224: 2 files\n",
      "Dataset Processing batch 3/224: 2 files\n",
      "Dataset Processing batch 4/224: 2 files\n",
      "Dataset Processing batch 5/224: 2 files\n",
      "Dataset Processing batch 6/224: 2 files\n",
      "Dataset Processing batch 7/224: 2 files\n",
      "Dataset Processing batch 8/224: 2 files\n",
      "Dataset Processing batch 9/224: 2 files\n",
      "Dataset Processing batch 10/224: 2 files\n",
      "Dataset Processing batch 11/224: 2 files\n",
      "Dataset Processing batch 12/224: 2 files\n",
      "Dataset Processing batch 13/224: 2 files\n",
      "Dataset Processing batch 14/224: 2 files\n",
      "Dataset Processing batch 15/224: 2 files\n",
      "Dataset Processing batch 16/224: 2 files\n",
      "Dataset Processing batch 17/224: 2 files\n",
      "Dataset Processing batch 18/224: 2 files\n",
      "Dataset Processing batch 19/224: 2 files\n",
      "Dataset Processing batch 20/224: 2 files\n",
      "Dataset Processing batch 21/224: 2 files\n",
      "Dataset Processing batch 22/224: 2 files\n",
      "Dataset Processing batch 23/224: 2 files\n",
      "Dataset Processing batch 24/224: 2 files\n",
      "Dataset Processing batch 25/224: 2 files\n",
      "Dataset Processing batch 26/224: 2 files\n",
      "Dataset Processing batch 27/224: 2 files\n",
      "Dataset Processing batch 28/224: 2 files\n",
      "Dataset Processing batch 29/224: 2 files\n",
      "Dataset Processing batch 30/224: 2 files\n",
      "Dataset Processing batch 31/224: 2 files\n",
      "Dataset Processing batch 32/224: 2 files\n",
      "Dataset Processing batch 33/224: 2 files\n",
      "Dataset Processing batch 34/224: 2 files\n",
      "Dataset Processing batch 35/224: 2 files\n",
      "Dataset Processing batch 36/224: 2 files\n",
      "Dataset Processing batch 37/224: 2 files\n",
      "Dataset Processing batch 38/224: 2 files\n",
      "Dataset Processing batch 39/224: 2 files\n",
      "Dataset Processing batch 40/224: 2 files\n",
      "Dataset Processing batch 41/224: 2 files\n",
      "Dataset Processing batch 42/224: 2 files\n",
      "Dataset Processing batch 43/224: 2 files\n",
      "Dataset Processing batch 44/224: 2 files\n",
      "Dataset Processing batch 45/224: 2 files\n",
      "Dataset Processing batch 46/224: 2 files\n",
      "Dataset Processing batch 47/224: 2 files\n",
      "Dataset Processing batch 48/224: 2 files\n",
      "Dataset Processing batch 49/224: 2 files\n",
      "Dataset Processing batch 50/224: 2 files\n",
      "Dataset Processing batch 51/224: 2 files\n",
      "Dataset Processing batch 52/224: 2 files\n",
      "Dataset Processing batch 53/224: 2 files\n",
      "Dataset Processing batch 54/224: 2 files\n",
      "Dataset Processing batch 55/224: 2 files\n",
      "Dataset Processing batch 56/224: 2 files\n",
      "Dataset Processing batch 57/224: 2 files\n",
      "Dataset Processing batch 58/224: 2 files\n",
      "Dataset Processing batch 59/224: 2 files\n",
      "Dataset Processing batch 60/224: 2 files\n",
      "Dataset Processing batch 61/224: 2 files\n",
      "Dataset Processing batch 62/224: 2 files\n",
      "Dataset Processing batch 63/224: 2 files\n",
      "Dataset Processing batch 64/224: 2 files\n",
      "Dataset Processing batch 65/224: 2 files\n",
      "Dataset Processing batch 66/224: 2 files\n",
      "Dataset Processing batch 67/224: 2 files\n",
      "Dataset Processing batch 68/224: 2 files\n",
      "Dataset Processing batch 69/224: 2 files\n",
      "Dataset Processing batch 70/224: 2 files\n",
      "Dataset Processing batch 71/224: 2 files\n",
      "Dataset Processing batch 72/224: 2 files\n",
      "Dataset Processing batch 73/224: 2 files\n",
      "Dataset Processing batch 74/224: 2 files\n",
      "Dataset Processing batch 75/224: 2 files\n",
      "Dataset Processing batch 76/224: 2 files\n",
      "Dataset Processing batch 77/224: 2 files\n",
      "Dataset Processing batch 78/224: 2 files\n",
      "Dataset Processing batch 79/224: 2 files\n",
      "Dataset Processing batch 80/224: 2 files\n",
      "Dataset Processing batch 81/224: 2 files\n",
      "Dataset Processing batch 82/224: 2 files\n",
      "Dataset Processing batch 83/224: 2 files\n",
      "Dataset Processing batch 84/224: 2 files\n",
      "Dataset Processing batch 85/224: 2 files\n",
      "Dataset Processing batch 86/224: 2 files\n",
      "Dataset Processing batch 87/224: 2 files\n",
      "Dataset Processing batch 88/224: 2 files\n",
      "Dataset Processing batch 89/224: 2 files\n",
      "Dataset Processing batch 90/224: 2 files\n",
      "Dataset Processing batch 91/224: 2 files\n",
      "Dataset Processing batch 92/224: 2 files\n",
      "Dataset Processing batch 93/224: 2 files\n",
      "Dataset Processing batch 94/224: 2 files\n",
      "Dataset Processing batch 95/224: 2 files\n",
      "Dataset Processing batch 96/224: 2 files\n",
      "Dataset Processing batch 97/224: 2 files\n",
      "Dataset Processing batch 98/224: 2 files\n",
      "Dataset Processing batch 99/224: 2 files\n",
      "Dataset Processing batch 100/224: 2 files\n",
      "Dataset Processing batch 101/224: 2 files\n",
      "Dataset Processing batch 102/224: 2 files\n",
      "Dataset Processing batch 103/224: 2 files\n",
      "Dataset Processing batch 104/224: 2 files\n",
      "Dataset Processing batch 105/224: 2 files\n",
      "Dataset Processing batch 106/224: 2 files\n",
      "Dataset Processing batch 107/224: 2 files\n",
      "Dataset Processing batch 108/224: 2 files\n",
      "Dataset Processing batch 109/224: 2 files\n",
      "Dataset Processing batch 110/224: 2 files\n",
      "Dataset Processing batch 111/224: 2 files\n",
      "Dataset Processing batch 112/224: 2 files\n",
      "Dataset Processing batch 113/224: 2 files\n",
      "Dataset Processing batch 114/224: 2 files\n",
      "Dataset Processing batch 115/224: 2 files\n",
      "Dataset Processing batch 116/224: 2 files\n",
      "Dataset Processing batch 117/224: 2 files\n",
      "Dataset Processing batch 118/224: 2 files\n",
      "Dataset Processing batch 119/224: 2 files\n",
      "Dataset Processing batch 120/224: 2 files\n",
      "Dataset Processing batch 121/224: 2 files\n",
      "Dataset Processing batch 122/224: 2 files\n",
      "Dataset Processing batch 123/224: 2 files\n",
      "Dataset Processing batch 124/224: 2 files\n",
      "Dataset Processing batch 125/224: 2 files\n",
      "Dataset Processing batch 126/224: 2 files\n",
      "Dataset Processing batch 127/224: 2 files\n",
      "Dataset Processing batch 128/224: 2 files\n",
      "Dataset Processing batch 129/224: 2 files\n",
      "Dataset Processing batch 130/224: 2 files\n",
      "Dataset Processing batch 131/224: 2 files\n",
      "Dataset Processing batch 132/224: 2 files\n",
      "Dataset Processing batch 133/224: 2 files\n",
      "Dataset Processing batch 134/224: 2 files\n",
      "Dataset Processing batch 135/224: 2 files\n",
      "Dataset Processing batch 136/224: 2 files\n",
      "Dataset Processing batch 137/224: 2 files\n",
      "Dataset Processing batch 138/224: 2 files\n",
      "Dataset Processing batch 139/224: 2 files\n",
      "Dataset Processing batch 140/224: 2 files\n",
      "Dataset Processing batch 141/224: 2 files\n",
      "Dataset Processing batch 142/224: 2 files\n",
      "Dataset Processing batch 143/224: 2 files\n",
      "Dataset Processing batch 144/224: 2 files\n",
      "Dataset Processing batch 145/224: 2 files\n",
      "Dataset Processing batch 146/224: 2 files\n",
      "Dataset Processing batch 147/224: 2 files\n",
      "Dataset Processing batch 148/224: 2 files\n",
      "Dataset Processing batch 149/224: 2 files\n",
      "Dataset Processing batch 150/224: 2 files\n",
      "Dataset Processing batch 151/224: 2 files\n",
      "Dataset Processing batch 152/224: 2 files\n",
      "Dataset Processing batch 153/224: 2 files\n",
      "Dataset Processing batch 154/224: 2 files\n",
      "Dataset Processing batch 155/224: 2 files\n",
      "Dataset Processing batch 156/224: 2 files\n",
      "Dataset Processing batch 157/224: 2 files\n",
      "Dataset Processing batch 158/224: 2 files\n",
      "Dataset Processing batch 159/224: 2 files\n",
      "Dataset Processing batch 160/224: 2 files\n",
      "Dataset Processing batch 161/224: 2 files\n",
      "Dataset Processing batch 162/224: 2 files\n",
      "Dataset Processing batch 163/224: 2 files\n",
      "Dataset Processing batch 164/224: 2 files\n",
      "Dataset Processing batch 165/224: 2 files\n",
      "Dataset Processing batch 166/224: 2 files\n",
      "Dataset Processing batch 167/224: 2 files\n",
      "Dataset Processing batch 168/224: 2 files\n",
      "Dataset Processing batch 169/224: 2 files\n",
      "Dataset Processing batch 170/224: 2 files\n",
      "Dataset Processing batch 171/224: 2 files\n",
      "Dataset Processing batch 172/224: 2 files\n",
      "Dataset Processing batch 173/224: 2 files\n",
      "Dataset Processing batch 174/224: 2 files\n",
      "Dataset Processing batch 175/224: 2 files\n",
      "Dataset Processing batch 176/224: 2 files\n",
      "Dataset Processing batch 177/224: 2 files\n",
      "Dataset Processing batch 178/224: 2 files\n",
      "Dataset Processing batch 179/224: 2 files\n",
      "Dataset Processing batch 180/224: 2 files\n",
      "Dataset Processing batch 181/224: 2 files\n",
      "Dataset Processing batch 182/224: 2 files\n",
      "Dataset Processing batch 183/224: 2 files\n",
      "Dataset Processing batch 184/224: 2 files\n",
      "Dataset Processing batch 185/224: 2 files\n",
      "Dataset Processing batch 186/224: 2 files\n",
      "Dataset Processing batch 187/224: 2 files\n",
      "Dataset Processing batch 188/224: 2 files\n",
      "Dataset Processing batch 189/224: 2 files\n",
      "Dataset Processing batch 190/224: 2 files\n",
      "Dataset Processing batch 191/224: 2 files\n",
      "Dataset Processing batch 192/224: 2 files\n",
      "Dataset Processing batch 193/224: 2 files\n",
      "Dataset Processing batch 194/224: 2 files\n",
      "Dataset Processing batch 195/224: 2 files\n",
      "Dataset Processing batch 196/224: 2 files\n",
      "Dataset Processing batch 197/224: 2 files\n",
      "Dataset Processing batch 198/224: 2 files\n",
      "Dataset Processing batch 199/224: 2 files\n",
      "Dataset Processing batch 200/224: 2 files\n",
      "Dataset Processing batch 201/224: 2 files\n",
      "Dataset Processing batch 202/224: 2 files\n",
      "Dataset Processing batch 203/224: 2 files\n",
      "Dataset Processing batch 204/224: 2 files\n",
      "Dataset Processing batch 205/224: 2 files\n",
      "Dataset Processing batch 206/224: 2 files\n",
      "Dataset Processing batch 207/224: 2 files\n",
      "Dataset Processing batch 208/224: 2 files\n",
      "Dataset Processing batch 209/224: 2 files\n",
      "Dataset Processing batch 210/224: 2 files\n",
      "Dataset Processing batch 211/224: 2 files\n",
      "Dataset Processing batch 212/224: 2 files\n",
      "Dataset Processing batch 213/224: 2 files\n",
      "Dataset Processing batch 214/224: 2 files\n",
      "Dataset Processing batch 215/224: 2 files\n",
      "Dataset Processing batch 216/224: 2 files\n",
      "Dataset Processing batch 217/224: 2 files\n",
      "Dataset Processing batch 218/224: 2 files\n",
      "Dataset Processing batch 219/224: 2 files\n",
      "Dataset Processing batch 220/224: 2 files\n",
      "Dataset Processing batch 221/224: 2 files\n",
      "Dataset Processing batch 222/224: 2 files\n",
      "Dataset Processing batch 223/224: 2 files\n",
      "Dataset Processing batch 224/224: 1 files\n",
      "There are 50 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/26: 2 files\n",
      "Dataset Processing batch 2/26: 2 files\n",
      "Dataset Processing batch 3/26: 2 files\n",
      "Dataset Processing batch 4/26: 2 files\n",
      "Dataset Processing batch 5/26: 2 files\n",
      "Dataset Processing batch 6/26: 2 files\n",
      "Dataset Processing batch 7/26: 2 files\n",
      "Dataset Processing batch 8/26: 2 files\n",
      "Dataset Processing batch 9/26: 2 files\n",
      "Dataset Processing batch 10/26: 2 files\n",
      "Dataset Processing batch 11/26: 2 files\n",
      "Dataset Processing batch 12/26: 2 files\n",
      "Dataset Processing batch 13/26: 2 files\n",
      "Dataset Processing batch 14/26: 2 files\n",
      "Dataset Processing batch 15/26: 2 files\n",
      "Dataset Processing batch 16/26: 2 files\n",
      "Dataset Processing batch 17/26: 2 files\n",
      "Dataset Processing batch 18/26: 2 files\n",
      "Dataset Processing batch 19/26: 2 files\n",
      "Dataset Processing batch 20/26: 2 files\n",
      "Dataset Processing batch 21/26: 2 files\n",
      "Dataset Processing batch 22/26: 2 files\n",
      "Dataset Processing batch 23/26: 2 files\n",
      "Dataset Processing batch 24/26: 2 files\n",
      "Dataset Processing batch 25/26: 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 3.67 \n",
      "\n",
      "========== batch: rep_4000 ========== \n",
      "size 101371.113 /content/drive/MyDrive/Sorbonne_Data_challenge/folder_training_set/26768b0bdbb3a9003b41c9e358db0e9ce4e5c660f7954c0ba5e3d649cc43ff53.json\n",
      "Number of files with error: 1\n",
      "There are 444 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/223: 2 files\n",
      "Dataset Processing batch 2/223: 2 files\n",
      "Dataset Processing batch 3/223: 2 files\n",
      "Dataset Processing batch 4/223: 2 files\n",
      "Dataset Processing batch 5/223: 2 files\n",
      "Dataset Processing batch 6/223: 2 files\n",
      "Dataset Processing batch 7/223: 2 files\n",
      "Dataset Processing batch 8/223: 2 files\n",
      "Dataset Processing batch 9/223: 2 files\n",
      "Dataset Processing batch 10/223: 2 files\n",
      "Dataset Processing batch 11/223: 2 files\n",
      "Dataset Processing batch 12/223: 2 files\n",
      "Dataset Processing batch 13/223: 2 files\n",
      "Dataset Processing batch 14/223: 2 files\n",
      "Dataset Processing batch 15/223: 2 files\n",
      "Dataset Processing batch 16/223: 2 files\n",
      "Dataset Processing batch 17/223: 2 files\n",
      "Dataset Processing batch 18/223: 2 files\n",
      "Dataset Processing batch 19/223: 2 files\n",
      "Dataset Processing batch 20/223: 2 files\n",
      "Dataset Processing batch 21/223: 2 files\n",
      "Dataset Processing batch 22/223: 2 files\n",
      "Dataset Processing batch 23/223: 2 files\n",
      "Dataset Processing batch 24/223: 2 files\n",
      "Dataset Processing batch 25/223: 2 files\n",
      "Dataset Processing batch 26/223: 2 files\n",
      "Dataset Processing batch 27/223: 2 files\n",
      "Dataset Processing batch 28/223: 2 files\n",
      "Dataset Processing batch 29/223: 2 files\n",
      "Dataset Processing batch 30/223: 2 files\n",
      "Dataset Processing batch 31/223: 2 files\n",
      "Dataset Processing batch 32/223: 2 files\n",
      "Dataset Processing batch 33/223: 2 files\n",
      "Dataset Processing batch 34/223: 2 files\n",
      "Dataset Processing batch 35/223: 2 files\n",
      "Dataset Processing batch 36/223: 2 files\n",
      "Dataset Processing batch 37/223: 2 files\n",
      "Dataset Processing batch 38/223: 2 files\n",
      "Dataset Processing batch 39/223: 2 files\n",
      "Dataset Processing batch 40/223: 2 files\n",
      "Dataset Processing batch 41/223: 2 files\n",
      "Dataset Processing batch 42/223: 2 files\n",
      "Dataset Processing batch 43/223: 2 files\n",
      "Dataset Processing batch 44/223: 2 files\n",
      "Dataset Processing batch 45/223: 2 files\n",
      "Dataset Processing batch 46/223: 2 files\n",
      "Dataset Processing batch 47/223: 2 files\n",
      "Dataset Processing batch 48/223: 2 files\n",
      "Dataset Processing batch 49/223: 2 files\n",
      "Dataset Processing batch 50/223: 2 files\n",
      "Dataset Processing batch 51/223: 2 files\n",
      "Dataset Processing batch 52/223: 2 files\n",
      "Dataset Processing batch 53/223: 2 files\n",
      "Dataset Processing batch 54/223: 2 files\n",
      "Dataset Processing batch 55/223: 2 files\n",
      "Dataset Processing batch 56/223: 2 files\n",
      "Dataset Processing batch 57/223: 2 files\n",
      "Dataset Processing batch 58/223: 2 files\n",
      "Dataset Processing batch 59/223: 2 files\n",
      "Dataset Processing batch 60/223: 2 files\n",
      "Dataset Processing batch 61/223: 2 files\n",
      "Dataset Processing batch 62/223: 2 files\n",
      "Dataset Processing batch 63/223: 2 files\n",
      "Dataset Processing batch 64/223: 2 files\n",
      "Dataset Processing batch 65/223: 2 files\n",
      "Dataset Processing batch 66/223: 2 files\n",
      "Dataset Processing batch 67/223: 2 files\n",
      "Dataset Processing batch 68/223: 2 files\n",
      "Dataset Processing batch 69/223: 2 files\n",
      "Dataset Processing batch 70/223: 2 files\n",
      "Dataset Processing batch 71/223: 2 files\n",
      "Dataset Processing batch 72/223: 2 files\n",
      "Dataset Processing batch 73/223: 2 files\n",
      "Dataset Processing batch 74/223: 2 files\n",
      "Dataset Processing batch 75/223: 2 files\n",
      "Dataset Processing batch 76/223: 2 files\n",
      "Dataset Processing batch 77/223: 2 files\n",
      "Dataset Processing batch 78/223: 2 files\n",
      "Dataset Processing batch 79/223: 2 files\n",
      "Dataset Processing batch 80/223: 2 files\n",
      "Dataset Processing batch 81/223: 2 files\n",
      "Dataset Processing batch 82/223: 2 files\n",
      "Dataset Processing batch 83/223: 2 files\n",
      "Dataset Processing batch 84/223: 2 files\n",
      "Dataset Processing batch 85/223: 2 files\n",
      "Dataset Processing batch 86/223: 2 files\n",
      "Dataset Processing batch 87/223: 2 files\n",
      "Dataset Processing batch 88/223: 2 files\n",
      "Dataset Processing batch 89/223: 2 files\n",
      "Dataset Processing batch 90/223: 2 files\n",
      "Dataset Processing batch 91/223: 2 files\n",
      "Dataset Processing batch 92/223: 2 files\n",
      "Dataset Processing batch 93/223: 2 files\n",
      "Dataset Processing batch 94/223: 2 files\n",
      "Dataset Processing batch 95/223: 2 files\n",
      "Dataset Processing batch 96/223: 2 files\n",
      "Dataset Processing batch 97/223: 2 files\n",
      "Dataset Processing batch 98/223: 2 files\n",
      "Dataset Processing batch 99/223: 2 files\n",
      "Dataset Processing batch 100/223: 2 files\n",
      "Dataset Processing batch 101/223: 2 files\n",
      "Dataset Processing batch 102/223: 2 files\n",
      "Dataset Processing batch 103/223: 2 files\n",
      "Dataset Processing batch 104/223: 2 files\n",
      "Dataset Processing batch 105/223: 2 files\n",
      "Dataset Processing batch 106/223: 2 files\n",
      "Dataset Processing batch 107/223: 2 files\n",
      "Dataset Processing batch 108/223: 2 files\n",
      "Dataset Processing batch 109/223: 2 files\n",
      "Dataset Processing batch 110/223: 2 files\n",
      "Dataset Processing batch 111/223: 2 files\n",
      "Dataset Processing batch 112/223: 2 files\n",
      "Dataset Processing batch 113/223: 2 files\n",
      "Dataset Processing batch 114/223: 2 files\n",
      "Dataset Processing batch 115/223: 2 files\n",
      "Dataset Processing batch 116/223: 2 files\n",
      "Dataset Processing batch 117/223: 2 files\n",
      "Dataset Processing batch 118/223: 2 files\n",
      "Dataset Processing batch 119/223: 2 files\n",
      "Dataset Processing batch 120/223: 2 files\n",
      "Dataset Processing batch 121/223: 2 files\n",
      "Dataset Processing batch 122/223: 2 files\n",
      "Dataset Processing batch 123/223: 2 files\n",
      "Dataset Processing batch 124/223: 2 files\n",
      "Dataset Processing batch 125/223: 2 files\n",
      "Dataset Processing batch 126/223: 2 files\n",
      "Dataset Processing batch 127/223: 2 files\n",
      "Dataset Processing batch 128/223: 2 files\n",
      "Dataset Processing batch 129/223: 2 files\n",
      "Dataset Processing batch 130/223: 2 files\n",
      "Dataset Processing batch 131/223: 2 files\n",
      "Dataset Processing batch 132/223: 2 files\n",
      "Dataset Processing batch 133/223: 2 files\n",
      "Dataset Processing batch 134/223: 2 files\n",
      "Dataset Processing batch 135/223: 2 files\n",
      "Dataset Processing batch 136/223: 2 files\n",
      "Dataset Processing batch 137/223: 2 files\n",
      "Dataset Processing batch 138/223: 2 files\n",
      "Dataset Processing batch 139/223: 2 files\n",
      "Dataset Processing batch 140/223: 2 files\n",
      "Dataset Processing batch 141/223: 2 files\n",
      "Dataset Processing batch 142/223: 2 files\n",
      "Dataset Processing batch 143/223: 2 files\n",
      "Dataset Processing batch 144/223: 2 files\n",
      "Dataset Processing batch 145/223: 2 files\n",
      "Dataset Processing batch 146/223: 2 files\n",
      "Dataset Processing batch 147/223: 2 files\n",
      "Dataset Processing batch 148/223: 2 files\n",
      "Dataset Processing batch 149/223: 2 files\n",
      "Dataset Processing batch 150/223: 2 files\n",
      "Dataset Processing batch 151/223: 2 files\n",
      "Dataset Processing batch 152/223: 2 files\n",
      "Dataset Processing batch 153/223: 2 files\n",
      "Dataset Processing batch 154/223: 2 files\n",
      "Dataset Processing batch 155/223: 2 files\n",
      "Dataset Processing batch 156/223: 2 files\n",
      "Dataset Processing batch 157/223: 2 files\n",
      "Dataset Processing batch 158/223: 2 files\n",
      "Dataset Processing batch 159/223: 2 files\n",
      "Dataset Processing batch 160/223: 2 files\n",
      "Dataset Processing batch 161/223: 2 files\n",
      "Dataset Processing batch 162/223: 2 files\n",
      "Dataset Processing batch 163/223: 2 files\n",
      "Dataset Processing batch 164/223: 2 files\n",
      "Dataset Processing batch 165/223: 2 files\n",
      "Dataset Processing batch 166/223: 2 files\n",
      "Dataset Processing batch 167/223: 2 files\n",
      "Dataset Processing batch 168/223: 2 files\n",
      "Dataset Processing batch 169/223: 2 files\n",
      "Dataset Processing batch 170/223: 2 files\n",
      "Dataset Processing batch 171/223: 2 files\n",
      "Dataset Processing batch 172/223: 2 files\n",
      "Dataset Processing batch 173/223: 2 files\n",
      "Dataset Processing batch 174/223: 2 files\n",
      "Dataset Processing batch 175/223: 2 files\n",
      "Dataset Processing batch 176/223: 2 files\n",
      "Dataset Processing batch 177/223: 2 files\n",
      "Dataset Processing batch 178/223: 2 files\n",
      "Dataset Processing batch 179/223: 2 files\n",
      "Dataset Processing batch 180/223: 2 files\n",
      "Dataset Processing batch 181/223: 2 files\n",
      "Dataset Processing batch 182/223: 2 files\n",
      "Dataset Processing batch 183/223: 2 files\n",
      "Dataset Processing batch 184/223: 2 files\n",
      "Dataset Processing batch 185/223: 2 files\n",
      "Dataset Processing batch 186/223: 2 files\n",
      "Dataset Processing batch 187/223: 2 files\n",
      "Dataset Processing batch 188/223: 2 files\n",
      "Dataset Processing batch 189/223: 2 files\n",
      "Dataset Processing batch 190/223: 2 files\n",
      "Dataset Processing batch 191/223: 2 files\n",
      "Dataset Processing batch 192/223: 2 files\n",
      "Dataset Processing batch 193/223: 2 files\n",
      "Dataset Processing batch 194/223: 2 files\n",
      "Dataset Processing batch 195/223: 2 files\n",
      "Dataset Processing batch 196/223: 2 files\n",
      "Dataset Processing batch 197/223: 2 files\n",
      "Dataset Processing batch 198/223: 2 files\n",
      "Dataset Processing batch 199/223: 2 files\n",
      "Dataset Processing batch 200/223: 2 files\n",
      "Dataset Processing batch 201/223: 2 files\n",
      "Dataset Processing batch 202/223: 2 files\n",
      "Dataset Processing batch 203/223: 2 files\n",
      "Dataset Processing batch 204/223: 2 files\n",
      "Dataset Processing batch 205/223: 2 files\n",
      "Dataset Processing batch 206/223: 2 files\n",
      "Dataset Processing batch 207/223: 2 files\n",
      "Dataset Processing batch 208/223: 2 files\n",
      "Dataset Processing batch 209/223: 2 files\n",
      "Dataset Processing batch 210/223: 2 files\n",
      "Dataset Processing batch 211/223: 2 files\n",
      "Dataset Processing batch 212/223: 2 files\n",
      "Dataset Processing batch 213/223: 2 files\n",
      "Dataset Processing batch 214/223: 2 files\n",
      "Dataset Processing batch 215/223: 2 files\n",
      "Dataset Processing batch 216/223: 2 files\n",
      "Dataset Processing batch 217/223: 2 files\n",
      "Dataset Processing batch 218/223: 2 files\n",
      "Dataset Processing batch 219/223: 2 files\n",
      "Dataset Processing batch 220/223: 2 files\n",
      "Dataset Processing batch 221/223: 2 files\n",
      "Dataset Processing batch 222/223: 2 files\n",
      "There are 50 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/26: 2 files\n",
      "Dataset Processing batch 2/26: 2 files\n",
      "Dataset Processing batch 3/26: 2 files\n",
      "Dataset Processing batch 4/26: 2 files\n",
      "Dataset Processing batch 5/26: 2 files\n",
      "Dataset Processing batch 6/26: 2 files\n",
      "Dataset Processing batch 7/26: 2 files\n",
      "Dataset Processing batch 8/26: 2 files\n",
      "Dataset Processing batch 9/26: 2 files\n",
      "Dataset Processing batch 10/26: 2 files\n",
      "Dataset Processing batch 11/26: 2 files\n",
      "Dataset Processing batch 12/26: 2 files\n",
      "Dataset Processing batch 13/26: 2 files\n",
      "Dataset Processing batch 14/26: 2 files\n",
      "Dataset Processing batch 15/26: 2 files\n",
      "Dataset Processing batch 16/26: 2 files\n",
      "Dataset Processing batch 17/26: 2 files\n",
      "Dataset Processing batch 18/26: 2 files\n",
      "Dataset Processing batch 19/26: 2 files\n",
      "Dataset Processing batch 20/26: 2 files\n",
      "Dataset Processing batch 21/26: 2 files\n",
      "Dataset Processing batch 22/26: 2 files\n",
      "Dataset Processing batch 23/26: 2 files\n",
      "Dataset Processing batch 24/26: 2 files\n",
      "Dataset Processing batch 25/26: 2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 1.98 \n",
      "\n",
      "========== batch: rep_4500 ========== \n",
      "Number of files with error: 0\n",
      "There are 888 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/445: 2 files\n",
      "Dataset Processing batch 2/445: 2 files\n",
      "Dataset Processing batch 3/445: 2 files\n",
      "Dataset Processing batch 4/445: 2 files\n",
      "Dataset Processing batch 5/445: 2 files\n",
      "Dataset Processing batch 6/445: 2 files\n",
      "Dataset Processing batch 7/445: 2 files\n",
      "Dataset Processing batch 8/445: 2 files\n",
      "Dataset Processing batch 9/445: 2 files\n",
      "Dataset Processing batch 10/445: 2 files\n",
      "Dataset Processing batch 11/445: 2 files\n",
      "Dataset Processing batch 12/445: 2 files\n",
      "Dataset Processing batch 13/445: 2 files\n",
      "Dataset Processing batch 14/445: 2 files\n",
      "Dataset Processing batch 15/445: 2 files\n",
      "Dataset Processing batch 16/445: 2 files\n",
      "Dataset Processing batch 17/445: 2 files\n",
      "Dataset Processing batch 18/445: 2 files\n",
      "Dataset Processing batch 19/445: 2 files\n",
      "Dataset Processing batch 20/445: 2 files\n",
      "Dataset Processing batch 21/445: 2 files\n",
      "Dataset Processing batch 22/445: 2 files\n",
      "Dataset Processing batch 23/445: 2 files\n",
      "Dataset Processing batch 24/445: 2 files\n",
      "Dataset Processing batch 25/445: 2 files\n",
      "Dataset Processing batch 26/445: 2 files\n",
      "Dataset Processing batch 27/445: 2 files\n",
      "Dataset Processing batch 28/445: 2 files\n",
      "Dataset Processing batch 29/445: 2 files\n",
      "Dataset Processing batch 30/445: 2 files\n",
      "Dataset Processing batch 31/445: 2 files\n",
      "Dataset Processing batch 32/445: 2 files\n",
      "Dataset Processing batch 33/445: 2 files\n",
      "Dataset Processing batch 34/445: 2 files\n",
      "Dataset Processing batch 35/445: 2 files\n",
      "Dataset Processing batch 36/445: 2 files\n",
      "Dataset Processing batch 37/445: 2 files\n",
      "Dataset Processing batch 38/445: 2 files\n",
      "Dataset Processing batch 39/445: 2 files\n",
      "Dataset Processing batch 40/445: 2 files\n",
      "Dataset Processing batch 41/445: 2 files\n",
      "Dataset Processing batch 42/445: 2 files\n",
      "Dataset Processing batch 43/445: 2 files\n",
      "Dataset Processing batch 44/445: 2 files\n",
      "Dataset Processing batch 45/445: 2 files\n",
      "Dataset Processing batch 46/445: 2 files\n",
      "Dataset Processing batch 47/445: 2 files\n",
      "Dataset Processing batch 48/445: 2 files\n",
      "Dataset Processing batch 49/445: 2 files\n",
      "Dataset Processing batch 50/445: 2 files\n",
      "Dataset Processing batch 51/445: 2 files\n",
      "Dataset Processing batch 52/445: 2 files\n",
      "Dataset Processing batch 53/445: 2 files\n",
      "Dataset Processing batch 54/445: 2 files\n",
      "Dataset Processing batch 55/445: 2 files\n",
      "Dataset Processing batch 56/445: 2 files\n",
      "Dataset Processing batch 57/445: 2 files\n",
      "Dataset Processing batch 58/445: 2 files\n",
      "Dataset Processing batch 59/445: 2 files\n",
      "Dataset Processing batch 60/445: 2 files\n",
      "Dataset Processing batch 61/445: 2 files\n",
      "Dataset Processing batch 62/445: 2 files\n",
      "Dataset Processing batch 63/445: 2 files\n",
      "Dataset Processing batch 64/445: 2 files\n",
      "Dataset Processing batch 65/445: 2 files\n",
      "Dataset Processing batch 66/445: 2 files\n",
      "Dataset Processing batch 67/445: 2 files\n",
      "Dataset Processing batch 68/445: 2 files\n",
      "Dataset Processing batch 69/445: 2 files\n",
      "Dataset Processing batch 70/445: 2 files\n",
      "Dataset Processing batch 71/445: 2 files\n",
      "Dataset Processing batch 72/445: 2 files\n",
      "Dataset Processing batch 73/445: 2 files\n",
      "Dataset Processing batch 74/445: 2 files\n",
      "Dataset Processing batch 75/445: 2 files\n",
      "Dataset Processing batch 76/445: 2 files\n",
      "Dataset Processing batch 77/445: 2 files\n",
      "Dataset Processing batch 78/445: 2 files\n",
      "Dataset Processing batch 79/445: 2 files\n",
      "Dataset Processing batch 80/445: 2 files\n",
      "Dataset Processing batch 81/445: 2 files\n",
      "Dataset Processing batch 82/445: 2 files\n",
      "Dataset Processing batch 83/445: 2 files\n",
      "Dataset Processing batch 84/445: 2 files\n",
      "Dataset Processing batch 85/445: 2 files\n",
      "Dataset Processing batch 86/445: 2 files\n",
      "Dataset Processing batch 87/445: 2 files\n",
      "Dataset Processing batch 88/445: 2 files\n",
      "Dataset Processing batch 89/445: 2 files\n",
      "Dataset Processing batch 90/445: 2 files\n",
      "Dataset Processing batch 91/445: 2 files\n",
      "Dataset Processing batch 92/445: 2 files\n",
      "Dataset Processing batch 93/445: 2 files\n",
      "Dataset Processing batch 94/445: 2 files\n",
      "Dataset Processing batch 95/445: 2 files\n",
      "Dataset Processing batch 96/445: 2 files\n",
      "Dataset Processing batch 97/445: 2 files\n",
      "Dataset Processing batch 98/445: 2 files\n",
      "Dataset Processing batch 99/445: 2 files\n",
      "Dataset Processing batch 100/445: 2 files\n",
      "Dataset Processing batch 101/445: 2 files\n",
      "Dataset Processing batch 102/445: 2 files\n",
      "Dataset Processing batch 103/445: 2 files\n",
      "Dataset Processing batch 104/445: 2 files\n",
      "Dataset Processing batch 105/445: 2 files\n",
      "Dataset Processing batch 106/445: 2 files\n",
      "Dataset Processing batch 107/445: 2 files\n",
      "Dataset Processing batch 108/445: 2 files\n",
      "Dataset Processing batch 109/445: 2 files\n",
      "Dataset Processing batch 110/445: 2 files\n",
      "Dataset Processing batch 111/445: 2 files\n",
      "Dataset Processing batch 112/445: 2 files\n",
      "Dataset Processing batch 113/445: 2 files\n",
      "Dataset Processing batch 114/445: 2 files\n",
      "Dataset Processing batch 115/445: 2 files\n",
      "Dataset Processing batch 116/445: 2 files\n",
      "Dataset Processing batch 117/445: 2 files\n",
      "Dataset Processing batch 118/445: 2 files\n",
      "Dataset Processing batch 119/445: 2 files\n",
      "Dataset Processing batch 120/445: 2 files\n",
      "Dataset Processing batch 121/445: 2 files\n",
      "Dataset Processing batch 122/445: 2 files\n",
      "Dataset Processing batch 123/445: 2 files\n",
      "Dataset Processing batch 124/445: 2 files\n",
      "Dataset Processing batch 125/445: 2 files\n",
      "Dataset Processing batch 126/445: 2 files\n",
      "Dataset Processing batch 127/445: 2 files\n",
      "Dataset Processing batch 128/445: 2 files\n",
      "Dataset Processing batch 129/445: 2 files\n",
      "Dataset Processing batch 130/445: 2 files\n",
      "Dataset Processing batch 131/445: 2 files\n",
      "Dataset Processing batch 132/445: 2 files\n",
      "Dataset Processing batch 133/445: 2 files\n",
      "Dataset Processing batch 134/445: 2 files\n",
      "Dataset Processing batch 135/445: 2 files\n",
      "Dataset Processing batch 136/445: 2 files\n",
      "Dataset Processing batch 137/445: 2 files\n",
      "Dataset Processing batch 138/445: 2 files\n",
      "Dataset Processing batch 139/445: 2 files\n",
      "Dataset Processing batch 140/445: 2 files\n",
      "Dataset Processing batch 141/445: 2 files\n",
      "Dataset Processing batch 142/445: 2 files\n",
      "Dataset Processing batch 143/445: 2 files\n",
      "Dataset Processing batch 144/445: 2 files\n",
      "Dataset Processing batch 145/445: 2 files\n",
      "Dataset Processing batch 146/445: 2 files\n",
      "Dataset Processing batch 147/445: 2 files\n",
      "Dataset Processing batch 148/445: 2 files\n",
      "Dataset Processing batch 149/445: 2 files\n",
      "Dataset Processing batch 150/445: 2 files\n",
      "Dataset Processing batch 151/445: 2 files\n",
      "Dataset Processing batch 152/445: 2 files\n",
      "Dataset Processing batch 153/445: 2 files\n",
      "Dataset Processing batch 154/445: 2 files\n",
      "Dataset Processing batch 155/445: 2 files\n",
      "Dataset Processing batch 156/445: 2 files\n",
      "Dataset Processing batch 157/445: 2 files\n",
      "Dataset Processing batch 158/445: 2 files\n",
      "Dataset Processing batch 159/445: 2 files\n",
      "Dataset Processing batch 160/445: 2 files\n",
      "Dataset Processing batch 161/445: 2 files\n",
      "Dataset Processing batch 162/445: 2 files\n",
      "Dataset Processing batch 163/445: 2 files\n",
      "Dataset Processing batch 164/445: 2 files\n",
      "Dataset Processing batch 165/445: 2 files\n",
      "Dataset Processing batch 166/445: 2 files\n",
      "Dataset Processing batch 167/445: 2 files\n",
      "Dataset Processing batch 168/445: 2 files\n",
      "Dataset Processing batch 169/445: 2 files\n",
      "Dataset Processing batch 170/445: 2 files\n",
      "Dataset Processing batch 171/445: 2 files\n",
      "Dataset Processing batch 172/445: 2 files\n",
      "Dataset Processing batch 173/445: 2 files\n",
      "Dataset Processing batch 174/445: 2 files\n",
      "Dataset Processing batch 175/445: 2 files\n",
      "Dataset Processing batch 176/445: 2 files\n",
      "Dataset Processing batch 177/445: 2 files\n",
      "Dataset Processing batch 178/445: 2 files\n",
      "Dataset Processing batch 179/445: 2 files\n",
      "Dataset Processing batch 180/445: 2 files\n",
      "Dataset Processing batch 181/445: 2 files\n",
      "Dataset Processing batch 182/445: 2 files\n",
      "Dataset Processing batch 183/445: 2 files\n",
      "Dataset Processing batch 184/445: 2 files\n",
      "Dataset Processing batch 185/445: 2 files\n",
      "Dataset Processing batch 186/445: 2 files\n",
      "Dataset Processing batch 187/445: 2 files\n",
      "Dataset Processing batch 188/445: 2 files\n",
      "Dataset Processing batch 189/445: 2 files\n",
      "Dataset Processing batch 190/445: 2 files\n",
      "Dataset Processing batch 191/445: 2 files\n",
      "Dataset Processing batch 192/445: 2 files\n",
      "Dataset Processing batch 193/445: 2 files\n",
      "Dataset Processing batch 194/445: 2 files\n",
      "Dataset Processing batch 195/445: 2 files\n",
      "Dataset Processing batch 196/445: 2 files\n",
      "Dataset Processing batch 197/445: 2 files\n",
      "Dataset Processing batch 198/445: 2 files\n",
      "Dataset Processing batch 199/445: 2 files\n",
      "Dataset Processing batch 200/445: 2 files\n",
      "Dataset Processing batch 201/445: 2 files\n",
      "Dataset Processing batch 202/445: 2 files\n",
      "Dataset Processing batch 203/445: 2 files\n",
      "Dataset Processing batch 204/445: 2 files\n",
      "Dataset Processing batch 205/445: 2 files\n",
      "Dataset Processing batch 206/445: 2 files\n",
      "Dataset Processing batch 207/445: 2 files\n",
      "Dataset Processing batch 208/445: 2 files\n",
      "Dataset Processing batch 209/445: 2 files\n",
      "Dataset Processing batch 210/445: 2 files\n",
      "Dataset Processing batch 211/445: 2 files\n",
      "Dataset Processing batch 212/445: 2 files\n",
      "Dataset Processing batch 213/445: 2 files\n",
      "Dataset Processing batch 214/445: 2 files\n",
      "Dataset Processing batch 215/445: 2 files\n",
      "Dataset Processing batch 216/445: 2 files\n",
      "Dataset Processing batch 217/445: 2 files\n",
      "Dataset Processing batch 218/445: 2 files\n",
      "Dataset Processing batch 219/445: 2 files\n",
      "Dataset Processing batch 220/445: 2 files\n",
      "Dataset Processing batch 221/445: 2 files\n",
      "Dataset Processing batch 222/445: 2 files\n",
      "Dataset Processing batch 223/445: 2 files\n",
      "Dataset Processing batch 224/445: 2 files\n",
      "Dataset Processing batch 225/445: 2 files\n",
      "Dataset Processing batch 226/445: 2 files\n",
      "Dataset Processing batch 227/445: 2 files\n",
      "Dataset Processing batch 228/445: 2 files\n",
      "Dataset Processing batch 229/445: 2 files\n",
      "Dataset Processing batch 230/445: 2 files\n",
      "Dataset Processing batch 231/445: 2 files\n",
      "Dataset Processing batch 232/445: 2 files\n",
      "Dataset Processing batch 233/445: 2 files\n",
      "Dataset Processing batch 234/445: 2 files\n",
      "Dataset Processing batch 235/445: 2 files\n",
      "Dataset Processing batch 236/445: 2 files\n",
      "Dataset Processing batch 237/445: 2 files\n",
      "Dataset Processing batch 238/445: 2 files\n",
      "Dataset Processing batch 239/445: 2 files\n",
      "Dataset Processing batch 240/445: 2 files\n",
      "Dataset Processing batch 241/445: 2 files\n",
      "Dataset Processing batch 242/445: 2 files\n",
      "Dataset Processing batch 243/445: 2 files\n",
      "Dataset Processing batch 244/445: 2 files\n",
      "Dataset Processing batch 245/445: 2 files\n",
      "Dataset Processing batch 246/445: 2 files\n",
      "Dataset Processing batch 247/445: 2 files\n",
      "Dataset Processing batch 248/445: 2 files\n",
      "Dataset Processing batch 249/445: 2 files\n",
      "Dataset Processing batch 250/445: 2 files\n",
      "Dataset Processing batch 251/445: 2 files\n",
      "Dataset Processing batch 252/445: 2 files\n",
      "Dataset Processing batch 253/445: 2 files\n",
      "Dataset Processing batch 254/445: 2 files\n",
      "Dataset Processing batch 255/445: 2 files\n",
      "Dataset Processing batch 256/445: 2 files\n",
      "Dataset Processing batch 257/445: 2 files\n",
      "Dataset Processing batch 258/445: 2 files\n",
      "Dataset Processing batch 259/445: 2 files\n",
      "Dataset Processing batch 260/445: 2 files\n",
      "Dataset Processing batch 261/445: 2 files\n",
      "Dataset Processing batch 262/445: 2 files\n",
      "Dataset Processing batch 263/445: 2 files\n",
      "Dataset Processing batch 264/445: 2 files\n",
      "Dataset Processing batch 265/445: 2 files\n",
      "Dataset Processing batch 266/445: 2 files\n",
      "Dataset Processing batch 267/445: 2 files\n",
      "Dataset Processing batch 268/445: 2 files\n",
      "Dataset Processing batch 269/445: 2 files\n",
      "Dataset Processing batch 270/445: 2 files\n",
      "Dataset Processing batch 271/445: 2 files\n",
      "Dataset Processing batch 272/445: 2 files\n",
      "Dataset Processing batch 273/445: 2 files\n",
      "Dataset Processing batch 274/445: 2 files\n",
      "Dataset Processing batch 275/445: 2 files\n",
      "Dataset Processing batch 276/445: 2 files\n",
      "Dataset Processing batch 277/445: 2 files\n",
      "Dataset Processing batch 278/445: 2 files\n",
      "Dataset Processing batch 279/445: 2 files\n",
      "Dataset Processing batch 280/445: 2 files\n",
      "Dataset Processing batch 281/445: 2 files\n",
      "Dataset Processing batch 282/445: 2 files\n",
      "Dataset Processing batch 283/445: 2 files\n",
      "Dataset Processing batch 284/445: 2 files\n",
      "Dataset Processing batch 285/445: 2 files\n",
      "Dataset Processing batch 286/445: 2 files\n",
      "Dataset Processing batch 287/445: 2 files\n",
      "Dataset Processing batch 288/445: 2 files\n",
      "Dataset Processing batch 289/445: 2 files\n",
      "Dataset Processing batch 290/445: 2 files\n",
      "Dataset Processing batch 291/445: 2 files\n",
      "Dataset Processing batch 292/445: 2 files\n",
      "Dataset Processing batch 293/445: 2 files\n",
      "Dataset Processing batch 294/445: 2 files\n",
      "Dataset Processing batch 295/445: 2 files\n",
      "Dataset Processing batch 296/445: 2 files\n",
      "Dataset Processing batch 297/445: 2 files\n",
      "Dataset Processing batch 298/445: 2 files\n",
      "Dataset Processing batch 299/445: 2 files\n",
      "Dataset Processing batch 300/445: 2 files\n",
      "Dataset Processing batch 301/445: 2 files\n",
      "Dataset Processing batch 302/445: 2 files\n",
      "Dataset Processing batch 303/445: 2 files\n",
      "Dataset Processing batch 304/445: 2 files\n",
      "Dataset Processing batch 305/445: 2 files\n",
      "Dataset Processing batch 306/445: 2 files\n",
      "Dataset Processing batch 307/445: 2 files\n",
      "Dataset Processing batch 308/445: 2 files\n",
      "Dataset Processing batch 309/445: 2 files\n",
      "Dataset Processing batch 310/445: 2 files\n",
      "Dataset Processing batch 311/445: 2 files\n",
      "Dataset Processing batch 312/445: 2 files\n",
      "Dataset Processing batch 313/445: 2 files\n",
      "Dataset Processing batch 314/445: 2 files\n",
      "Dataset Processing batch 315/445: 2 files\n",
      "Dataset Processing batch 316/445: 2 files\n",
      "Dataset Processing batch 317/445: 2 files\n",
      "Dataset Processing batch 318/445: 2 files\n",
      "Dataset Processing batch 319/445: 2 files\n",
      "Dataset Processing batch 320/445: 2 files\n",
      "Dataset Processing batch 321/445: 2 files\n",
      "Dataset Processing batch 322/445: 2 files\n",
      "Dataset Processing batch 323/445: 2 files\n",
      "Dataset Processing batch 324/445: 2 files\n",
      "Dataset Processing batch 325/445: 2 files\n",
      "Dataset Processing batch 326/445: 2 files\n",
      "Dataset Processing batch 327/445: 2 files\n",
      "Dataset Processing batch 328/445: 2 files\n",
      "Dataset Processing batch 329/445: 2 files\n",
      "Dataset Processing batch 330/445: 2 files\n",
      "Dataset Processing batch 331/445: 2 files\n",
      "Dataset Processing batch 332/445: 2 files\n",
      "Dataset Processing batch 333/445: 2 files\n",
      "Dataset Processing batch 334/445: 2 files\n",
      "Dataset Processing batch 335/445: 2 files\n",
      "Dataset Processing batch 336/445: 2 files\n",
      "Dataset Processing batch 337/445: 2 files\n",
      "Dataset Processing batch 338/445: 2 files\n",
      "Dataset Processing batch 339/445: 2 files\n",
      "Dataset Processing batch 340/445: 2 files\n",
      "Dataset Processing batch 341/445: 2 files\n",
      "Dataset Processing batch 342/445: 2 files\n",
      "Dataset Processing batch 343/445: 2 files\n",
      "Dataset Processing batch 344/445: 2 files\n",
      "Dataset Processing batch 345/445: 2 files\n",
      "Dataset Processing batch 346/445: 2 files\n",
      "Dataset Processing batch 347/445: 2 files\n",
      "Dataset Processing batch 348/445: 2 files\n",
      "Dataset Processing batch 349/445: 2 files\n",
      "Dataset Processing batch 350/445: 2 files\n",
      "Dataset Processing batch 351/445: 2 files\n",
      "Dataset Processing batch 352/445: 2 files\n",
      "Dataset Processing batch 353/445: 2 files\n",
      "Dataset Processing batch 354/445: 2 files\n",
      "Dataset Processing batch 355/445: 2 files\n",
      "Dataset Processing batch 356/445: 2 files\n",
      "Dataset Processing batch 357/445: 2 files\n",
      "Dataset Processing batch 358/445: 2 files\n",
      "Dataset Processing batch 359/445: 2 files\n",
      "Dataset Processing batch 360/445: 2 files\n",
      "Dataset Processing batch 361/445: 2 files\n",
      "Dataset Processing batch 362/445: 2 files\n",
      "Dataset Processing batch 363/445: 2 files\n",
      "Dataset Processing batch 364/445: 2 files\n",
      "Dataset Processing batch 365/445: 2 files\n",
      "Dataset Processing batch 366/445: 2 files\n",
      "Dataset Processing batch 367/445: 2 files\n",
      "Dataset Processing batch 368/445: 2 files\n",
      "Dataset Processing batch 369/445: 2 files\n",
      "Dataset Processing batch 370/445: 2 files\n",
      "Dataset Processing batch 371/445: 2 files\n",
      "Dataset Processing batch 372/445: 2 files\n",
      "Dataset Processing batch 373/445: 2 files\n",
      "Dataset Processing batch 374/445: 2 files\n",
      "Dataset Processing batch 375/445: 2 files\n",
      "Dataset Processing batch 376/445: 2 files\n",
      "Dataset Processing batch 377/445: 2 files\n",
      "Dataset Processing batch 378/445: 2 files\n",
      "Dataset Processing batch 379/445: 2 files\n",
      "Dataset Processing batch 380/445: 2 files\n",
      "Dataset Processing batch 381/445: 2 files\n",
      "Dataset Processing batch 382/445: 2 files\n",
      "Dataset Processing batch 383/445: 2 files\n",
      "Dataset Processing batch 384/445: 2 files\n",
      "Dataset Processing batch 385/445: 2 files\n",
      "Dataset Processing batch 386/445: 2 files\n",
      "Dataset Processing batch 387/445: 2 files\n",
      "Dataset Processing batch 388/445: 2 files\n",
      "Dataset Processing batch 389/445: 2 files\n",
      "Dataset Processing batch 390/445: 2 files\n",
      "Dataset Processing batch 391/445: 2 files\n",
      "Dataset Processing batch 392/445: 2 files\n",
      "Dataset Processing batch 393/445: 2 files\n",
      "Dataset Processing batch 394/445: 2 files\n",
      "Dataset Processing batch 395/445: 2 files\n",
      "Dataset Processing batch 396/445: 2 files\n",
      "Dataset Processing batch 397/445: 2 files\n",
      "Dataset Processing batch 398/445: 2 files\n",
      "Dataset Processing batch 399/445: 2 files\n",
      "Dataset Processing batch 400/445: 2 files\n",
      "Dataset Processing batch 401/445: 2 files\n",
      "Dataset Processing batch 402/445: 2 files\n",
      "Dataset Processing batch 403/445: 2 files\n",
      "Dataset Processing batch 404/445: 2 files\n",
      "Dataset Processing batch 405/445: 2 files\n",
      "Dataset Processing batch 406/445: 2 files\n",
      "Dataset Processing batch 407/445: 2 files\n",
      "Dataset Processing batch 408/445: 2 files\n",
      "Dataset Processing batch 409/445: 2 files\n",
      "Dataset Processing batch 410/445: 2 files\n",
      "Dataset Processing batch 411/445: 2 files\n",
      "Dataset Processing batch 412/445: 2 files\n",
      "Dataset Processing batch 413/445: 2 files\n",
      "Dataset Processing batch 414/445: 2 files\n",
      "Dataset Processing batch 415/445: 2 files\n",
      "Dataset Processing batch 416/445: 2 files\n",
      "Dataset Processing batch 417/445: 2 files\n",
      "Dataset Processing batch 418/445: 2 files\n",
      "Dataset Processing batch 419/445: 2 files\n",
      "Dataset Processing batch 420/445: 2 files\n",
      "Dataset Processing batch 421/445: 2 files\n",
      "Dataset Processing batch 422/445: 2 files\n",
      "Dataset Processing batch 423/445: 2 files\n",
      "Dataset Processing batch 424/445: 2 files\n",
      "Dataset Processing batch 425/445: 2 files\n",
      "Dataset Processing batch 426/445: 2 files\n",
      "Dataset Processing batch 427/445: 2 files\n",
      "Dataset Processing batch 428/445: 2 files\n",
      "Dataset Processing batch 429/445: 2 files\n",
      "Dataset Processing batch 430/445: 2 files\n",
      "Dataset Processing batch 431/445: 2 files\n",
      "Dataset Processing batch 432/445: 2 files\n",
      "Dataset Processing batch 433/445: 2 files\n",
      "Dataset Processing batch 434/445: 2 files\n",
      "Dataset Processing batch 435/445: 2 files\n",
      "Dataset Processing batch 436/445: 2 files\n",
      "Dataset Processing batch 437/445: 2 files\n",
      "Dataset Processing batch 438/445: 2 files\n",
      "Dataset Processing batch 439/445: 2 files\n",
      "Dataset Processing batch 440/445: 2 files\n",
      "Dataset Processing batch 441/445: 2 files\n",
      "Dataset Processing batch 442/445: 2 files\n",
      "Dataset Processing batch 443/445: 2 files\n",
      "Dataset Processing batch 444/445: 2 files\n",
      "There are 99 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "Dataset Processing batch 1/50: 2 files\n",
      "Dataset Processing batch 2/50: 2 files\n",
      "Dataset Processing batch 3/50: 2 files\n",
      "Dataset Processing batch 4/50: 2 files\n",
      "Dataset Processing batch 5/50: 2 files\n",
      "Dataset Processing batch 6/50: 2 files\n",
      "Dataset Processing batch 7/50: 2 files\n",
      "Dataset Processing batch 8/50: 2 files\n",
      "Dataset Processing batch 9/50: 2 files\n",
      "Dataset Processing batch 10/50: 2 files\n",
      "Dataset Processing batch 11/50: 2 files\n",
      "Dataset Processing batch 12/50: 2 files\n",
      "Dataset Processing batch 13/50: 2 files\n",
      "Dataset Processing batch 14/50: 2 files\n",
      "Dataset Processing batch 15/50: 2 files\n",
      "Dataset Processing batch 16/50: 2 files\n",
      "Dataset Processing batch 17/50: 2 files\n",
      "Dataset Processing batch 18/50: 2 files\n",
      "Dataset Processing batch 19/50: 2 files\n",
      "Dataset Processing batch 20/50: 2 files\n",
      "Dataset Processing batch 21/50: 2 files\n",
      "Dataset Processing batch 22/50: 2 files\n",
      "Dataset Processing batch 23/50: 2 files\n",
      "Dataset Processing batch 24/50: 2 files\n",
      "Dataset Processing batch 25/50: 2 files\n",
      "Dataset Processing batch 26/50: 2 files\n",
      "Dataset Processing batch 27/50: 2 files\n",
      "Dataset Processing batch 28/50: 2 files\n",
      "Dataset Processing batch 29/50: 2 files\n",
      "Dataset Processing batch 30/50: 2 files\n",
      "Dataset Processing batch 31/50: 2 files\n",
      "Dataset Processing batch 32/50: 2 files\n",
      "Dataset Processing batch 33/50: 2 files\n",
      "Dataset Processing batch 34/50: 2 files\n",
      "Dataset Processing batch 35/50: 2 files\n",
      "Dataset Processing batch 36/50: 2 files\n",
      "Dataset Processing batch 37/50: 2 files\n",
      "Dataset Processing batch 38/50: 2 files\n",
      "Dataset Processing batch 39/50: 2 files\n",
      "Dataset Processing batch 40/50: 2 files\n",
      "Dataset Processing batch 41/50: 2 files\n",
      "Dataset Processing batch 42/50: 2 files\n",
      "Dataset Processing batch 43/50: 2 files\n",
      "Dataset Processing batch 44/50: 2 files\n",
      "Dataset Processing batch 45/50: 2 files\n",
      "Dataset Processing batch 46/50: 2 files\n",
      "Dataset Processing batch 47/50: 2 files\n",
      "Dataset Processing batch 48/50: 2 files\n",
      "Dataset Processing batch 49/50: 2 files\n",
      "Dataset Processing batch 50/50: 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** F1 score: 2.96 \n",
      "\n",
      "========== batch: rep_5000 ========== \n",
      "Number of files with error: 0\n",
      "There are 0 files for train\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n",
      "There are 0 files for validation\n",
      "**** Tokenization Processing\n",
      "**** Dataset Processing & Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bd1debda6348>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mall_pred_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_true_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_files_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_files_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_files_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mf1_macro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**** F1 score: {f1_macro*100:.2f} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1f18f8dbc956>\u001b[0m in \u001b[0;36mrun_files_in_batches\u001b[0;34m(full_file_paths, batch_files_size, mode)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# save all labels as tensor to return if mode= 'validation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mall_true_label_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mall_pred_label_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Main init and training sequence\n",
    "###############################################################\n",
    "# Batch params\n",
    "batch_data_size = 1\n",
    "batch_files_size = 2\n",
    "num_feature_dim = 15\n",
    "\n",
    "# Initialize model\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "for i in range(0, 22501, 500):\n",
    "  batch_curr = f'rep_{i}'\n",
    "  print(f\"========== batch: {batch_curr} ========== \")\n",
    "  list_train, list_val, list_err = process_graph_directory(training_path_dir, val_size=0.1, rep_batch=batch_curr)\n",
    "\n",
    "  # train\n",
    "  hash_t = run_files_in_batches(list_train, batch_files_size=batch_files_size, mode='train')\n",
    "\n",
    "  # validate\n",
    "  all_pred_label_tensor, all_true_label_tensor, hash_v = run_files_in_batches(list_val, batch_files_size=batch_files_size, mode='validation')\n",
    "  f1_macro = f1_score(all_true_label_tensor, all_pred_label_tensor, average='macro', zero_division=1)\n",
    "  print(f\"**** F1 score: {f1_macro*100:.2f} \\n\")\n",
    "\n",
    "  if not os.path.exists(filename_trained):\n",
    "      with open(filename_trained, 'w', newline='') as csvfile:\n",
    "          writer = csv.writer(csvfile)\n",
    "          writer.writerow([\"files_trained\"])\n",
    "          for item in hash_t:\n",
    "              for value in item:\n",
    "                  writer.writerow([value])\n",
    "          csvfile.close()\n",
    "  else:\n",
    "      with open(filename_trained, 'a', newline='') as csvfile:\n",
    "          writer = csv.writer(csvfile)\n",
    "          for item in hash_t:\n",
    "              for value in item:\n",
    "                  writer.writerow([value])\n",
    "          csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_irax4h1OKj1",
    "outputId": "7334cdcd-2bc7-4732-d1d6-54efa8d86275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN Mask:\n",
      " [[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False]]\n",
      "\n",
      "Total NaN values: 0\n",
      "\n",
      "Total NaN values using sum: 0\n",
      "\n",
      "Nan counts per column: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "Nan counts per row: [0]\n"
     ]
    }
   ],
   "source": [
    "# Check pred value, for F1 score warning\n",
    "\n",
    "nan_mask = np.isnan(all_pred_label_tensor)\n",
    "#print(\"NaN Mask:\\n\", nan_mask)\n",
    "\n",
    "# Count total NaN values\n",
    "total_nan = np.count_nonzero(nan_mask)\n",
    "print(\"\\nTotal NaN values:\", total_nan)\n",
    "\n",
    "#Alternative way to count total nan values.\n",
    "#total_nan_sum = np.sum(nan_mask)\n",
    "#print(\"\\nTotal NaN values using sum:\", total_nan_sum)\n",
    "\n",
    "#Count nan values per axis.\n",
    "#nan_count_axis0 = np.sum(np.isnan(all_pred_label_tensor), axis=0)\n",
    "#nan_count_axis1 = np.sum(np.isnan(all_pred_label_tensor), axis=1)\n",
    "\n",
    "#print(\"\\nNan counts per column:\", nan_count_axis0)\n",
    "#print(\"Nan counts per row:\", nan_count_axis1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_init_file = r\"test_set_to_predict-JC.csv\"\n",
    "#filename_test_results = r\"test_prediction.csv\"\n",
    "df_test_init = pd.read_csv(test_init_file, sep=\",\")\n",
    "col_to_predict = df_test_init.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 454)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  95ff2b3fd399984950293194a717d404bc4e9e3aa0296f...   \n",
       "1  4216c07609bb5b89f32d9b559494848a0f5411d1e2d3cf...   \n",
       "2  2518f84c015a5795bdb2597d580ab7df8e0bfa4b6543c6...   \n",
       "3  243268c658456d9b8ec968c088c4f3c7cb976df92a4b99...   \n",
       "4  533c09cb9b49c10494337d3eb7d2919c2c656b37f554fb...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                NaN     NaN         NaN   \n",
       "1                                NaN     NaN         NaN   \n",
       "2                                NaN     NaN         NaN   \n",
       "3                                NaN     NaN         NaN   \n",
       "4                                NaN     NaN         NaN   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                            NaN                           NaN   \n",
       "1                            NaN                           NaN   \n",
       "2                            NaN                           NaN   \n",
       "3                            NaN                           NaN   \n",
       "4                            NaN                           NaN   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                NaN                 NaN                  NaN   \n",
       "1                NaN                 NaN                  NaN   \n",
       "2                NaN                 NaN                  NaN   \n",
       "3                NaN                 NaN                  NaN   \n",
       "4                NaN                 NaN                  NaN   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "1              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "2              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "3              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "4              NaN  ...     NaN   NaN   NaN                       NaN   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                   NaN                  NaN                    NaN   \n",
       "1                   NaN                  NaN                    NaN   \n",
       "2                   NaN                  NaN                    NaN   \n",
       "3                   NaN                  NaN                    NaN   \n",
       "4                   NaN                  NaN                    NaN   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0         NaN       NaN   NaN  \n",
       "1         NaN       NaN   NaN  \n",
       "2         NaN       NaN   NaN  \n",
       "3         NaN       NaN   NaN  \n",
       "4         NaN       NaN   NaN  \n",
       "\n",
       "[5 rows x 454 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_test_init.shape)\n",
    "df_test_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mts_g2jUtSu-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test to be done later, when full training is completed\n",
      "**** Dataset Processing for Test\n"
     ]
    }
   ],
   "source": [
    "print(\"test to be done later, when full training is completed\")\n",
    "\n",
    "batch_data_size = 2\n",
    "batch_files_size = 2\n",
    "num_feature_dim = 15\n",
    "\n",
    "tokenizer = HierarchicalAssemblyTokenizer()\n",
    "model = AssemblyGAT(node_feature_dim=num_feature_dim, hidden_dim=64, output_dim=453)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_file, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "#init test\n",
    "df_all_labels = df_test_init\n",
    "list_test_hash = []\n",
    "#all_pred_labels = pd.DataFrame(columns=col_to_predict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pour tous les hash du fichiers test\n",
    "    print(f\"**** Dataset Processing for Test\")\n",
    "    count_batch = 0  \n",
    "    for count_test in range(0, len(df_test_init), batch_files_size):\n",
    "      \n",
    "        hash_names = df_test_init.iloc[count_test:count_test + batch_files_size]['name'].to_list()\n",
    "        #print(\"1 :\",count_test,count_test+ batch_files_size)\n",
    "        full_file_test_name = [os.path.join(test_path_dir, file+'.json') for file in hash_names]\n",
    "        #print(full_file_test_name)\n",
    "\n",
    "        batch_graph_list = process_batch(full_file_test_name)\n",
    "        #print(\"2 :\",len(batch_graph_list))\n",
    "        tokenizer.fit(batch_graph_list)\n",
    "\n",
    "        test_dataset = AssemblyGraphDataset(batch_graph_list, tokenizer,  node_feature_dim=num_feature_dim)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_data_size, shuffle=False)\n",
    "        \n",
    "        # loop on the batch from the dataloader\n",
    "        for batch in test_dataloader:\n",
    "            batch_hash_list = hash_names\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            hash_encoded = batch.hash_encoded.to(device)\n",
    "\n",
    "            # Create batch vector for global pooling\n",
    "            num_graphs = batch.num_graphs #obtain the number of graphs in the batch\n",
    "            #print(\"3:\", num_graphs)\n",
    "            num_nodes_per_graph = torch.bincount(batch.batch).tolist() # get number of nodes per graph.\n",
    "            if any(num_nodes_per_graph): #checks if any values are not zero.\n",
    "                batch_p = torch.cat([torch.full((num_nodes_graph_i,), i, device=device) for i, num_nodes_graph_i in enumerate(torch.bincount(batch.batch).tolist())])\n",
    "                batch_p = batch_p.to(device)\n",
    "            else:\n",
    "                # Handle the case where the list is empty. For example, create an empty tensor or raise an error.\n",
    "                print(\"Warning: bincount_list is empty. Creating an empty tensor.\")\n",
    "                batch_p = torch.empty(0, dtype=torch.long, device=device) # create empty tensor.\n",
    "\n",
    "            output = model(x, edge_index, hash_encoded, batch_p)\n",
    "            predicted_labels_prob = torch.sigmoid(output)\n",
    "            prediction_labels = (predicted_labels_prob > 0.5).int()\n",
    "\n",
    "            if len(prediction_labels) != len(batch_hash_list):\n",
    "                print(f\"*** error: len(prediction_labels)={len(prediction_labels)}, len(hash_names)={len(batch_hash_list)}\")\n",
    "\n",
    "            for i, hash_val in enumerate(batch_hash_list):\n",
    "                row_index = df_all_labels[df_all_labels['name'] == hash_val].index\n",
    "                #print(row_index,hash_val )\n",
    "                if not row_index.empty:\n",
    "                    label_list = prediction_labels[i].tolist()  # Convert tensor row to list\n",
    "                    int_label_list = [int(val) for val in label_list]  # Convert to int\n",
    "\n",
    "                    # Update the DataFrame\n",
    "                    for j in range(len(int_label_list)):\n",
    "                        df_all_labels.iloc[row_index, j+1] = int_label_list[j]\n",
    "                else:\n",
    "                    print(f\"Warning: hash_val '{hash_val}' not found in df_all_labels.\")\n",
    "\n",
    "        count_batch += 1\n",
    "\n",
    "    print(f\"Test end: nb pred labels: {len(all_pred_labels)} \")\n",
    "    df_all_labels.to_csv(filename_test_results, header=True ,index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 454)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>64-bit execution via heavens gate</th>\n",
       "      <th>64bits</th>\n",
       "      <th>PEB access</th>\n",
       "      <th>accept command line arguments</th>\n",
       "      <th>access the Windows event log</th>\n",
       "      <th>act as TCP client</th>\n",
       "      <th>allocate RW memory</th>\n",
       "      <th>allocate RWX memory</th>\n",
       "      <th>allocate memory</th>\n",
       "      <th>...</th>\n",
       "      <th>winzip</th>\n",
       "      <th>wise</th>\n",
       "      <th>worm</th>\n",
       "      <th>write and execute a file</th>\n",
       "      <th>write clipboard data</th>\n",
       "      <th>write file on Linux</th>\n",
       "      <th>write file on Windows</th>\n",
       "      <th>write pipe</th>\n",
       "      <th>xorcrypt</th>\n",
       "      <th>yoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000fd5f03bb251ae0d81d8fd0d6aa86485639a17fe4a04...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a4d467d954416807c8493f2fa73b0196e3ad8dc85ee5f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0a4b7259c5a459697122a2c4e85d5499919fa6a9c91d66...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a0c976cce2e9ce258a73bcb1d45a5c1a790fafc12c1ab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00da1ec8b4b2832056b13e77af850e2bbb22f4d36dd2ae...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0b709b99660397d1b4367ee14f404d7968811ddfea8f90...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0b678fb57ca1d75961a48b58942a0cf9fccffae49225f6...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0b99c9d49626e2bbdf1e3a87d55a6b454c994497534572...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0b58a2bed7d9d44e29ad6988250456c49863a23adeb9b5...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0b45ddc2ff31d966c0e1823e8d0c6b90fef9400d4a0ea2...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 454 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  000fd5f03bb251ae0d81d8fd0d6aa86485639a17fe4a04...   \n",
       "1  0a4d467d954416807c8493f2fa73b0196e3ad8dc85ee5f...   \n",
       "2  0a4b7259c5a459697122a2c4e85d5499919fa6a9c91d66...   \n",
       "3  0a0c976cce2e9ce258a73bcb1d45a5c1a790fafc12c1ab...   \n",
       "4  00da1ec8b4b2832056b13e77af850e2bbb22f4d36dd2ae...   \n",
       "5  0b709b99660397d1b4367ee14f404d7968811ddfea8f90...   \n",
       "6  0b678fb57ca1d75961a48b58942a0cf9fccffae49225f6...   \n",
       "7  0b99c9d49626e2bbdf1e3a87d55a6b454c994497534572...   \n",
       "8  0b58a2bed7d9d44e29ad6988250456c49863a23adeb9b5...   \n",
       "9  0b45ddc2ff31d966c0e1823e8d0c6b90fef9400d4a0ea2...   \n",
       "\n",
       "   64-bit execution via heavens gate  64bits  PEB access  \\\n",
       "0                                0.0     1.0         0.0   \n",
       "1                                0.0     1.0         0.0   \n",
       "2                                0.0     1.0         0.0   \n",
       "3                                0.0     1.0         0.0   \n",
       "4                                0.0     1.0         0.0   \n",
       "5                                0.0     1.0         0.0   \n",
       "6                                0.0     0.0         0.0   \n",
       "7                                0.0     0.0         0.0   \n",
       "8                                0.0     0.0         0.0   \n",
       "9                                0.0     0.0         0.0   \n",
       "\n",
       "   accept command line arguments  access the Windows event log  \\\n",
       "0                            0.0                           0.0   \n",
       "1                            0.0                           0.0   \n",
       "2                            0.0                           0.0   \n",
       "3                            0.0                           0.0   \n",
       "4                            0.0                           0.0   \n",
       "5                            0.0                           0.0   \n",
       "6                            0.0                           0.0   \n",
       "7                            0.0                           0.0   \n",
       "8                            0.0                           0.0   \n",
       "9                            0.0                           0.0   \n",
       "\n",
       "   act as TCP client  allocate RW memory  allocate RWX memory  \\\n",
       "0                0.0                 0.0                  0.0   \n",
       "1                0.0                 0.0                  0.0   \n",
       "2                0.0                 0.0                  0.0   \n",
       "3                0.0                 0.0                  0.0   \n",
       "4                0.0                 0.0                  0.0   \n",
       "5                0.0                 0.0                  0.0   \n",
       "6                0.0                 0.0                  0.0   \n",
       "7                0.0                 0.0                  0.0   \n",
       "8                0.0                 0.0                  0.0   \n",
       "9                0.0                 0.0                  0.0   \n",
       "\n",
       "   allocate memory  ...  winzip  wise  worm  write and execute a file  \\\n",
       "0              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "1              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "2              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "3              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "4              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "5              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "6              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "7              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "8              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "9              0.0  ...     0.0   0.0   0.0                       0.0   \n",
       "\n",
       "   write clipboard data  write file on Linux  write file on Windows  \\\n",
       "0                   0.0                  0.0                    1.0   \n",
       "1                   0.0                  0.0                    1.0   \n",
       "2                   0.0                  0.0                    1.0   \n",
       "3                   0.0                  0.0                    1.0   \n",
       "4                   0.0                  0.0                    0.0   \n",
       "5                   0.0                  0.0                    1.0   \n",
       "6                   1.0                  0.0                    1.0   \n",
       "7                   0.0                  0.0                    1.0   \n",
       "8                   0.0                  0.0                    1.0   \n",
       "9                   0.0                  0.0                    1.0   \n",
       "\n",
       "   write pipe  xorcrypt  yoda  \n",
       "0         0.0       0.0   0.0  \n",
       "1         0.0       0.0   0.0  \n",
       "2         0.0       0.0   0.0  \n",
       "3         0.0       0.0   0.0  \n",
       "4         0.0       0.0   0.0  \n",
       "5         0.0       0.0   0.0  \n",
       "6         0.0       0.0   0.0  \n",
       "7         0.0       0.0   0.0  \n",
       "8         0.0       0.0   0.0  \n",
       "9         0.0       0.0   0.0  \n",
       "\n",
       "[10 rows x 454 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_all_labels.shape)\n",
    "df_all_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
